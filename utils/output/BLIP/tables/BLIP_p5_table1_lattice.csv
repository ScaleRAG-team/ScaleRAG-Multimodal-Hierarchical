"Table 1. Evaluation of the effect of the captioner (C) and ï¬lter (F) for dataset bootstrapping. Downstream tasks include image-text retrieval
and image captioning with ï¬netuning (FT) and zero-shot (ZS) settings. TR / IR@1: recall@1 for text retrieval / image retrieval. (cid:51)B/L:
captioner or ï¬lter uses ViT-B / ViT-L as vision backbone.
Figure 4. Examples of the web text Tw and the synthetic text Ts. Green texts are accepted by the ï¬lter, whereas red texts are rejected.
Generation
Noise
method
ratio
None
N.A.
78.4
60.7
93.9
82.1
38.0
127.8
102.2
Beam
19%
79.6
61.9
94.1
83.1
38.4
128.9
103.5","","ğ‘‡!: â€œfrom bridge 
near my houseâ€
ğ‘‡"": â€œa flock of birds 
flying over a lake at 
sunsetâ€","Retrieval-FT (COCO)
TR@1
IR@1","","","ğ‘‡!: â€œin front of a house 
door in Reichenfels, 
Austriaâ€ 
ğ‘‡"": â€œa potted plant sitting 
on top of a pile of rocksâ€","Caption-FT (COCO)
B@4
CIDEr","","",""
"","","","","","","","","","","ğ‘‡!: â€œthe current castle was 
built in 1180, replacing a 9th 
century wooden castleâ€
ğ‘‡"": â€œa large building with a lot 
of windows on itâ€ 
Caption-ZS (NoCaps)
SPICE
13.9
14.2"
"","","","","","Retrieval-ZS (Flickr)
TR@1
IR@1","","","","CIDEr",""
"","","","","","","","","","",""
"","","","","","","","","","",""
