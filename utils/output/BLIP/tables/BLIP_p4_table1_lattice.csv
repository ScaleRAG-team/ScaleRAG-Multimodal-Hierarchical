"BLIP: Bootstrapping Language-Image Pre-training for Uniﬁed Vision-Language Understanding and Generation
Model Pretraining
Dataset Bootstrapping
Figure 3. Learning framework of BLIP. We introduce a captioner to produce synthetic captions for web images, and a ﬁlter to remove
noisy image-text pairs. The captioner and ﬁlter are initialized from the same pre-trained model and ﬁnetuned individually on a small-scale
human-annotated dataset. The bootstrapped dataset is used to pre-train a new model.
We propose Captioning and Filtering (CapFilt),
method to improve the quality of the text corpus. Figure 3
gives an illustration of CapFilt. It introduces two modules:
a captioner to generate captions given web images, and a
ﬁlter to remove noisy image-text pairs. Both the captioner
and the ﬁlter are initialized from the same pre-trained MED
model, and ﬁnetuned individually on the COCO dataset.
The ﬁnetuning is a lightweight procedure.
Speciﬁcally,
coder. It is ﬁnetuned with the LM objective to decode texts
given images. Given the web images Iw, the captioner gen-
erates synthetic captions Ts with one caption per
The ﬁlter is an image-grounded text encoder. It is ﬁnetuned
with the ITC and ITM objectives to learn whether a text
matches an image. The ﬁlter removes noisy texts in both
the original web texts Tw and the synthetic texts Ts, where
a text is considered to be noisy if the ITM head predicts it
as unmatched to the image. Finally, we combine the ﬁltered
image-text pairs with the human-annotated pairs to form a
new dataset, which we use to pre-train a new model.
4. Experiments and Discussions
In this section, we ﬁrst introduce pre-training details. Then
we provide a detailed experimental analysis on our method.
4.1. Pre-training Details
Our models are implemented in PyTorch (Paszke et al.,
2019) and pre-trained on two 16-GPU nodes.
age transformer is initialized from ViT pre-trained on Ima-
geNet (Touvron et al., 2020; Dosovitskiy et al., 2021), and
the text
et al., 2019). We explore two variants of ViTs: ViT-B/16
and ViT-L/16. Unless otherwise speciﬁed, all
ported in this paper as “BLIP” uses ViT-B. We pre-train the
model for 20 epochs using a batch size of 2880 (ViT-B) /","","","","","","","","","","","","","","","","","","","","To model
To data
𝐼!: web images
𝐼"": human-annotated
images
𝑇!: web texts
𝑇!: filtered web texts
𝑇#: synthetic texts"
"","","Pre-train","","","","Filter 
(Image-grounded 
Text Encoder)
ITC&ITM finetune
LM finetune","","","Filtering","","","","","","","","","","",""
"","","","𝐷 =
+
𝐼!, 𝑇!
𝐼"", 𝑇""","","","","","","","","","","","","","+
𝐼!, 𝑇!
𝐼!, 𝑇#","","","",""
"","","","Multimodal Mixture of 
Encoder-Decoder","","","","","","","","","","","","","","","","",""
"","","","","","","","","","𝐼!, 𝑇!","","","","","","","","","","",""
"","","","","","","","","","","","","","","","","𝐷 =
+
𝐼!, 𝑇!
𝐼!, 𝑇#
+
𝐼"", 𝑇""","","","",""
"","","","","","","","𝐼"", 𝑇""","","","","","","","","","","","","",""
"","","","","","","","","","","{𝐼!}","","","","","","","","","",""
"","","","","","","","","","","","","","","","","","","","","𝑇#: filtered synthetic"
"","","","Downstream Tasks","","","Captioner 
(Image-grounded 
Text Decoder)","","","","","","","𝐼!, 𝑇#","","","","","","","texts
𝑇"": human-annotated
texts"
"","","","","","","","","","","","","","","","","","","","",""
"","","","","","","","","","Captioning","","","","","","","","","","",""
"","","","","","","","","","","","","","","","","","","","",""
"","","","","","","","","","","","","","","","","","","","",""
"","","","","the captioner is an image-grounded text de-
transformer is initialized from BERTbase (Devlin","","a new
image.
The im-
results re-","","","2400 (ViT-L). We use AdamW (Loshchilov & Hutter, 2017)
optimizer with a weight decay of 0.05. The learning rate
is warmed-up to 3e-4 (ViT-B) / 2e-4 (ViT-L) and decayed
linearly with a rate of 0.85. We take random image crops of
resolution 224 × 224 during pre-training, and increase the
image resolution to 384 × 384 during ﬁnetuning. We use
the same pre-training dataset as Li et al. (2021a) with 14M
images in total,
including two human-annotated datasets
(COCO and Visual Genome (Krishna et al., 2017)), and
three web datasets (Conceptual Captions (Changpinyo et al.,
2021), Conceptual 12M (Changpinyo et al., 2021), SBU cap-
tions (Ordonez et al., 2011)). We also experimented with an
additional web dataset, LAION (Schuhmann et al., 2021),
which contains 115M images with more noisy texts1. More
details about the datasets can be found in the appendix.
4.2. Effect of CapFilt
In Table 1, we compare models pre-trained on different
datasets to demonstrate the efﬁcacy of CapFilt on down-
stream tasks, including image-text retrieval and image cap-
tioning with ﬁnetuned and zero-shot settings.
When only the captioner or the ﬁlter is applied to the dataset
with 14M images, performance improvement can be ob-
served. When applied together,
their effects compliment
each other, leading to substantial improvements compared
to using the original noisy web texts.
CapFilt can further boost performance with a larger dataset
and a larger vision backbone, which veriﬁes its scalability
in both the data size and the model size. Furthermore, by
using a large captioner and ﬁlter with ViT-L, performance
of the base model can also be improved.
1We only download images whose shorter edge is larger than
256 pixels from the original LAION400M. Due to the large size of
LAION, we only use 1/5 of it each epoch during pre-training.","","","","","","","","","","",""
