"BLIP: Bootstrapping Language-Image Pre-training for Uniﬁed Vision-Language Understanding and Generation",""
"et al., 2020; Puri et al., 2020; Yang et al., 2020). Differ-","former and the text
transformer by encouraging positive"
"ent from these methods which focus on the low-resource","image-text pairs to have similar representations in contrast"
"language-only tasks, our method demonstrates the advan-","to the negative pairs.
It has been shown to be an effective"
"tage of synthetic captions in large-scale vision-language","objective for
improving vision and language understand-"
"pre-training.","ing (Radford et al., 2021; Li et al., 2021a). We follow the"
"","ITC loss by Li et al. (2021a), where a momentum encoder"
"3. Method","is introduced to produce features, and soft labels are created"
"We propose BLIP, a uniﬁed VLP framework to learn from","from the momentum encoder as training targets to account"
"noisy image-text pairs. This section ﬁrst introduces our new","for the potential positives in the negative pairs."
"model architecture MED and its pre-training objectives, and","Image-Text Matching Loss
(ITM) activates
the image-"
"then delineates CapFilt for dataset bootstrapping.","grounded text encoder.
It aims to learn image-text mul-"
"3.1. Model Architecture","timodal representation that captures the ﬁne-grained align-"
"","ment between vision and language.
ITM is a binary clas-"
"We employ a visual transformer (Dosovitskiy et al., 2021)","siﬁcation task, where the model uses an ITM head (a lin-"
"as our image encoder, which divides an input
image into","ear layer) to predict whether an image-text pair is positive"
"patches and encodes them as a sequence of embeddings,","(matched) or negative (unmatched) given their multimodal"
"with an additional [CLS] token to represent the global im-","feature.
In order to ﬁnd more informative negatives, we"
"age feature. Compared to using pre-trained object detectors","adopt the hard negative mining strategy by Li et al. (2021a),"
"for visual feature extraction (Chen et al., 2020), using a ViT","where negatives pairs with higher contrastive similarity in a"
"is more computation-friendly and has been adopted by the","batch are more likely to be selected to compute the loss."
"more recent methods (Li et al., 2021a; Kim et al., 2021).","Language Modeling Loss
(LM)
activates
the
image-"
"In order to pre-train a uniﬁed model with both understanding","grounded text decoder, which aims to generate textual de-"
"and generation capabilities, we propose multimodal mixture","scriptions given an image. It optimizes a cross entropy loss"
"of encoder-decoder (MED), a multi-task model which can","which trains the model
to maximize the likelihood of the"
"operate in one of the three functionalities:","text in an autoregressive manner. We apply a label smooth-"
"(1) Unimodal encoder, which separately encodes image","ing of 0.1 when computing the loss. Compared to the MLM"
"and text. The text encoder is the same as BERT (Devlin et al.,","loss that has been widely-used for VLP, LM enables the"
"2019), where a [CLS] token is appended to the beginning","model with the generalization capability to convert visual"
"of the text input to summarize the sentence.","information into coherent captions."
"(2)
Image-grounded text encoder, which injects visual","In order to perform efﬁcient pre-training while leveraging"
"information by inserting one additional cross-attention (CA)","multi-task learning, the text encoder and text decoder share"
"layer between the self-attention (SA)
layer and the feed","all parameters except for the SA layers. The reason is that"
"forward network (FFN) for each transformer block of the","the differences between the encoding and decoding tasks are"
"text encoder. A task-speciﬁc [Encode] token is appended","best captured by the SA layers.
In particular,
the encoder"
"to the text, and the output embedding of [Encode] is used","employs bi-directional self-attention to build representations"
"as the multimodal representation of the image-text pair.","for
the current
input
tokens, while the decoder employs"
"(3) Image-grounded text decoder, which replaces the bi-","causal self-attention to predict next
tokens. On the other"
"directional self-attention layers in the image-grounded text","hand, the embedding layers, CA layers and FFN function"
"encoder with causal self-attention layers. A [Decode]","similarly between encoding and decoding tasks, therefore"
"token is used to signal the beginning of a sequence, and an","sharing these layers can improve training efﬁciency while"
"end-of-sequence token is used to signal its end.","beneﬁting from multi-task learning,"
"3.2. Pre-training Objectives","3.3. CapFilt"
"We jointly optimize three objectives during pre-training,","Due to the prohibitive annotation cost,
there exist a lim-"
"with two understanding-based objectives and one generation-","ited number of high-quality human-annotated image-text"
"based objective. Each image-text pair only requires one for-","pairs {(Ih, Th)} (e.g., COCO (Lin et al., 2014)). Recent"
"ward pass through the computational-heavier visual trans-","work (Li et al., 2021a; Wang et al., 2021) utilizes a much"
"former, and three forward passes through the text
trans-","larger number of image and alt-text pairs {(Iw, Tw)} that"
"former, where different functionalities are activated to com-","are automatically collected from the web. However,
the"
"pute the three losses as delineated below.","alt-texts often do not accurately describe the visual content"
"Image-Text Contrastive Loss (ITC) activates the unimodal","of the images, making them a noisy signal that is suboptimal"
"encoder. It aims to align the feature space of the visual trans-","for learning vision-language alignment."
