"","","","","SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","",""
"","","","per-tensor quant.","","per-tensor quant.",""
"","","","","","","Table 1: Among different activation quantization schemes,"
"","","","","","[1]",""
"","","","","","","only per-channel quantization (Bondarenko et al., 2021) pre-"
"","","","Ci","","ΔW",""
"","","","[1]","","",""
"","","","ΔX","","","serves the accuracy, but it is not compatible (marked in gray)"
"","","[1×C0]","","","Co",""
"","","sCo","T","","",""
"","","","","","",""
"","","","","","","with INT8 GEMM kernels. We report the average accuracy"
"","Ci","","X","*","Ci",""
"","","","","","W","on WinoGrande, HellaSwag, PIQA, and LAMBADA."
"T","","","","","",""
"","","Co","","","",""
