"","","SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",""
"A","Discussion on Weight-Only Quantization","","4. Finally, we think the two settings are somewhat orthog-"
"","","","onal. We believe we can integrate GPTQ’s method for"
"In this work, we study W8A8 quantization so that we can","","",""
"","","","a better weight quantization and potentially achieve"
"utilize INT8 GEMM kernels to increase the throughput and","","",""
"","","W4A4 quantization, which will","lead to even better"
"accelerate inference.","There is another
line of work that","",""
"","","","hardware efficiency (INT4 instructions are supported"
"only quantizes the weight of LLMs (e.g., GPTQ (Frantar","","",""
"","","","on NVIDIA’s Hopper GPU architecture). We leave this"
"et al., 2022)).","It converts the quantized weights to FP16","",""
"","","exploration to future work.",""
"on the fly for matmul during inference and can also lead to","","",""
"speed up due to the reduced data loading, especially for the","","",""
"generation stage with batch size 1.","","",""
"We mainly compare our method with existing work on","","",""
"weight-activation quantization (i.e., W8A8) like (Dettmers","","",""
"et al., 2022; Yao et al., 2022; Wei et al., 2022) since they are","","",""
"under the same setting. Here we would like to give a short","","",""
"discussion about the weight-only quantization methods in","","",""
"LLM settings:","","",""
