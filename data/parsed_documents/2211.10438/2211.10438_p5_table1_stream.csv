"SmoothQuant. All weight and activations use INT8 repre-","","","serving of a >500B model within a single node. Note that"
"sentations unless specified. For SmoothQuant, the efficiency","","","we focus on the relative performance change before and"
"improves from O1 to O3 (i.e., lower latency).","","","after quantization but not the absolute value."
"Method","Weight","Activation","Activation smoothing.
The migration strength α = 0.5 is"
"","","","a general sweet spot for all the OPT and BLOOM models,"
"W8A8","per-tensor","per-tensor dynamic",""
"","","","and α = 0.75 for GLM-130B since its activations are more"
"ZeroQuant","group-wise","per-token dynamic",""
"LLM.int8()","per-channel","per-token dynamic+FP16","difficult to quantize (Zeng et al., 2022). We get a suitable α"
"Outlier Suppression","per-tensor","per-tensor static","by running a quick grid search on a subset of the Pile (Gao"
"","","","et al., 2020) validation set. To get the statistics of activations,"
"SmoothQuant-O1","per-tensor","per-token dynamic",""
"SmoothQuant-O2","per-tensor","per-tensor dynamic","we calibrate the smoothing factors and the static quantiza-"
"SmoothQuant-O3","per-tensor","per-tensor static","tion step sizes once with 512 random sentences from the"
"","","","pre-training dataset Pile, and apply the same smoothed and"
