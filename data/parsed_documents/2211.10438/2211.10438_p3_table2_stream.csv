"","","","","","Ci","","","ΔW",""
"","","","","[1]","","","","",""
"","","","","ΔX","","","","","serves the accuracy, but it is not compatible (marked in gray)"
"","","","[1×C0]","","","","","Co",""
"","","sCo","","T","","","","",""
"","","","","","","","","",""
"","","","","","","","","","with INT8 GEMM kernels. We report the average accuracy"
"","","","","","X","*","Ci","",""
"","","","","","","","","W","on WinoGrande, HellaSwag, PIQA, and LAMBADA."
"","","Co","","","","","","",""
"","","","","","","(a) per-tensor quantization","","",""
"","","","","","","","","","Model size (OPT-)
6.7B
13B
30B
66B
175B"
"*","Ci","","","[T×1]","","","","",""
"","","","","","","","","",""
"","","","","","","","","[1×C0]",""
"","","","","ΔX","","","","ΔW",""
"","","","","","","","","",""
"","","","W","","","","","","FP16
64.9% 65.6% 67.9% 69.5% 71.6%"
"","","","","","Ci","","","",""
"","","","","","","","","","INT8 per-tensor
39.9% 33.0% 32.8% 33.1% 32.3%"
"","","","","","","","","Co",""
"","","","","T","","","","",""
"","","","","","","","","",""
"","","","","","X","*","Ci","","INT8 per-token
42.5% 33.0% 33.1% 32.9% 31.7%"
"","","","","","","","","",""
"","","","","","","","","W","INT8 per-channel
64.8% 65.6% 68.0% 69.4% 71.4%"
