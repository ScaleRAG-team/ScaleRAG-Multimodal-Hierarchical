"discussion about the weight-only quantization methods in",""
"LLM settings:",""
"","1. Firstly, we were trying to compare our method with"
"","GPTQ (Frantar et al., 2022) but found it difficult due to"
"","different implementations. GPTQ’s low-bit kenerl **"
"","only supports the generation stage with batch size 1"
"","(i.e., only processing a single token at a time), and can-"
"","not support the context stage (widely used in different"
"","downstream tasks and chatbot) or batch-based setting."
"","Furthermore, its low-bit kernel optimization only tar-"
"","gets the OPT-175B model (as stated in the README)."
"","At the same time, our work utilizes FasterTransformer"
"","for serving large models, which may lead to an unfair"
"","advantage if we make a direct comparison."
"","2. GPTQ may perform better at handling a small number"
"","of input tokens (1 in its experiments) since the process"
"","is highly memory-bounded. In contrast, SmoothQuant"
"","may serve better with a batching setting or for the con-"
"","text stage (i.e., when the number of processed tokens"
"","is more significant). Nonetheless, some work shows"
"","that in production, we can improve the throughput of"
"","serving GPT models by 37× at similar latency with"
"","advanced batching (Yu et al., 2022). We believe in"
"","production, batching will be the future standard, and"
"","SmoothQuant will bring further improvement, even for"
"the generation stage.",""
"","3. Applications like chatbots need to handle a long con-"
"text","length and potentially run under a batch setting."
"Due to the two factors,","the memory size of
the KV"
"","cache can no longer be ignored (as shown in (Pope"
"","et al., 2022), the KV cache totals 3TB given batch size"
"","512 and context length 2048, which is 3× larger than"
"","the model weights). In this case, quantization of activa-"
"","tion can also help reduce the memory cost from storing"
"the KV cache.",""
"**https://github.com/IST-DASLab/gptq",""
