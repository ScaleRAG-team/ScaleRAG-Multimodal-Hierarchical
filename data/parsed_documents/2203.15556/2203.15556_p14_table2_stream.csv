"","0-shot","55.4%","43.5%","-",""
"TriviaQA (ﬁltered, dev)","5-shot","64.1%","57.0%","-","72.5%"
"","64-shot","64.6%","57.2%","-",""
"","Table 9 | Closed-book question answering. For Natural Questions (Kwiatkowski et al., 2019) and","","","",""
"","TriviaQA (Joshi et al., 2017), Chinchilla outperforms Gopher in all cases. On Natural Questions,","","","",""
"","Chinchilla outperforms GPT-3. On TriviaQA we show results on two diﬀerent evaluation sets to allow","","","",""
"","for comparison to GPT-3 and to open book SOTA (FiD + Distillation (Izacard and Grave, 2020)).","","","",""
"","albeit with slightly diﬀerent relative weights, and because it has a similar architecture. Here, we","","","",""
"","examine gender bias (particularly gender and occupation bias) and generation of toxic language. We","","","",""
"","select a few common evaluations to highlight potential issues, but stress that our evaluations are not","","","",""
"","comprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs.","","","",""
"","As discussed in Rae et al. (2021), large language models reﬂect contemporary and","","","",""
"Gender bias.","","","","",""
"","historical discourse about diﬀerent groups (such as gender groups) from their training dataset, and","","","",""
"","we expect the same to be true for Chinchilla. Here, we test if potential gender and occupation biases","","","",""
"","manifest in unfair outcomes on coreference resolutions, using the Winogender dataset (Rudinger","","","",""
"","et al., 2018) in a zero-shot setting. Winogender tests whether a model can correctly determine if","","","",""
"","a pronoun refers to diﬀerent occupation words. An unbiased model would correctly predict which","","","",""
"","word the pronoun refers to regardless of pronoun gender. We follow the same setup as in Rae et al.","","","",""
"(2021) (described further in Section H.3).","","","","",""
"","As shown in Table 10, Chinchilla correctly resolves pronouns more frequently than Gopher across","","","",""
"","all groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase","","","",""
"","of 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also","","","",""
"","consider gotcha examples, in which the correct pronoun resolution contradicts gender stereotypes","","","",""
"(determined by labor statistics). Again, we see that Chinchilla resolves pronouns more accurately","","","","",""
"","than Gopher. When breaking up examples by male/female gender and gotcha/not gotcha, the largest","","","",""
"","improvement is on female gotcha examples (improvement of 10%). Thus, though Chinchilla uniformly","","","",""
"","overcomes gender stereotypes for more coreference examples than Gopher, the rate of improvement","","","",""
"","is higher for some pronouns than others, suggesting that the improvements conferred by using a more","","","",""
"compute-optimal model can be uneven.","","","","",""
"","Language models are capable of generating toxic language—including insults,","","","",""
"Sample toxicity.","","","","",""
"","hate speech, profanities and threats (Gehman et al., 2020; Rae et al., 2021). While toxicity is an","","","",""
"","umbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021),","","","",""
"","automatic classiﬁer scores can provide an indication for the levels of harmful text that a LM generates.","","","",""
"","Rae et al. (2021) found that improving language modelling loss by increasing the number of model","","","",""
"","parameters has only a negligible eﬀect on toxic text generation (unprompted); here we analyze","","","",""
