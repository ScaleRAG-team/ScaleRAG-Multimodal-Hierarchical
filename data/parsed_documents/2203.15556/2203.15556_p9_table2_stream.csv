"The full set of hyperparameters used to train Chinchilla are given in Table 4. Chinchilla uses the same"
"model architecture and training setup as Gopher with the exception of the diﬀerences listed below."
"• We train Chinchilla on MassiveText (the same dataset as Gopher) but use a slightly diﬀerent"
"subset distribution (shown in Table A1) to account for the increased number of training tokens."
"• We use AdamW (Loshchilov and Hutter, 2019) for Chinchilla rather than Adam (Kingma and"
"Ba, 2014) as this improves the language modelling loss and the downstream task performance"
"after ﬁnetuning.8"
"• We train Chinchilla with a slightly modiﬁed SentencePiece (Kudo and Richardson, 2018)"
"tokenizer that does not apply NFKC normalisation. The vocabulary is very similar– 94.15% of"
"tokens are the same as those used for training Gopher. We ﬁnd that this particularly helps with"
"the representation of mathematics and chemistry, for example."
"• Whilst the forward and backward pass are computed in bfloat16, we store a float32 copy"
"of the weights in the distributed optimiser state (Rajbhandari et al., 2020). See Lessons Learned"
"from Rae et al. (2021) for additional details."
