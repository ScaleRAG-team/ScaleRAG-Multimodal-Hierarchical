"language models, showing that the scaling with number of experts diminishes as the model size"
"increases—their approach models the loss as a function of two variables:
the model size and the"
"number of experts. However, the analysis is done with a ﬁxed number of training tokens, as in Kaplan"
"et al. (2020), potentially underestimating the improvements of branching."
"The model size and the number of training tokens"
"Estimating hyperparameters for large models."
"are not the only two parameters to chose when selecting a language model and a procedure to train"
"it. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and"
"width-to-depth ratio.
In this work, we focus on model size and the number of training steps, and"
"we rely on existing work and provided experimental heuristics to determine the other necessary"
"hyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for"
"training an autoregressive transformer, including the learning rate and batch size. McCandlish et al."
"(2018) ﬁnds only a weak dependence between optimal batch size and model size. Shallue et al."
"(2018); Zhang et al. (2019) suggest that using larger batch-sizes than those we use is possible. Levine"
"et al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model sizes. We"
"use slightly less deep models than proposed as this translates to better wall-clock performance on our"
"hardware."
"Recently, various promising alternatives to traditional dense trans-"
"Improved model architectures."
"formers have been proposed. For example, through the use of conditional computation large MoE"
"models like the 1.7 trillion parameter Switch transformer (Fedus et al., 2021), the 1.2 Trillion pa-"
"rameter GLaM model (Du et al., 2021), and others (Artetxe et al., 2021; Zoph et al., 2022) are able"
"to provide a large eﬀective model size despite using relatively fewer training and inference FLOPs."
"However, for very large models the computational beneﬁts of routed models seems to diminish (Clark"
"et al., 2022). An orthogonal approach to improving language models is to augment transformers"
"with explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al."
"(2020). This approach eﬀectively increases the number of data tokens seen during training (by a"
"factor of ∼ 10 in Borgeaud et al. (2021)). This suggests that the performance of language models"
"may be more dependant on the size of the training data than previously thought."
"3. Estimating the optimal parameter/training tokens allocation"
"We present three diﬀerent approaches to answer the question driving our research: Given a ﬁxed"
"FLOPs budget, how should one trade-oﬀ model size and the number of training tokens? In all three"
"cases we start by training a range of models varying both model size and the number of training"
"tokens and use the resulting training curves to ﬁt an empirical estimator of how they should scale."
"We assume a power-law relationship between compute and model size as done in Clark et al. (2022);"
"Kaplan et al. (2020), though future work may want to include potential curvature in this relationship"
"for large model sizes. The resulting predictions are similar for all three methods and suggest that"
"parameter count and number of training tokens should be increased equally with more compute3—"
"with proportions reported in Table 2. This is in clear contrast to previous work on this topic and"
"warrants further investigation."
