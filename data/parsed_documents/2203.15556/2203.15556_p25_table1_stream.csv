"datapoints that we can only see once, and when we constrain the size of the hypothesis space to be"
"ğ‘-dimensional ? We are making steps toward minimizing the empirical risk within a ï¬nite-dimensional"
"functional space Hğ‘:"
"Ë†"
"setting
(8)
ğ‘“ğ‘,ğ· (cid:44) argmin
ğ¿ğ· ( ğ‘“ ).
ğ¿ğ· ( ğ‘“ ) (cid:44) Ë†ğ”¼ğ· [log ğ‘“ (ğ‘¥) ğ‘¦],"
"ğ‘“ âˆˆHğ‘"
"We are never able to obtain Ë†ğ‘“ğ‘,ğ· as we typically perform a single epoch over the dataset of size ğ·."
"Instead, be obtain Â¯ğ‘“ğ‘,ğ·, which is the result of applying a certain number of gradient steps based on"
"the ğ· datapointsâ€”the number of steps to perform depends on the gradient batch size, for which we"
"use well-tested heuristics."
"Using the Bayes-classiï¬er
ğ‘“ â˜…, the expected-risk minimizer
ğ‘“ğ‘ and the â€œsingle-epoch empirical-risk"
"minimizerâ€ Â¯ğ‘“ğ‘,ğ·, we can ï¬nally decompose the loss ğ¿(ğ‘, ğ·) into"
"(9)
ğ¿(ğ‘, ğ·) (cid:44) ğ¿( Â¯ğ‘“ğ‘,ğ·) = ğ¿( ğ‘“ â˜…) + (cid:0)ğ¿( ğ‘“ğ‘) âˆ’ ğ¿( ğ‘“ â˜…)(cid:1) + (cid:0)ğ¿( Â¯ğ‘“ğ‘,ğ·) âˆ’ ğ¿( ğ‘“ğ‘)(cid:1) ."
"The loss comprises three terms:
the Bayes risk,
i.e.
the minimal
loss achievable for next-token"
"prediction on the full distribution ğ‘ƒ, a.k.a the â€œentropy of natural text.â€; a functional approximation"
"term that depends on the size of the hypothesis space; ï¬nally, a stochastic approximation term that"
"captures the suboptimality of minimizing Ë†ğ¿ğ· instead of ğ¿, and of making a single epoch on the provided"
"dataset."
"In the decomposition (9), the second term depends entirely on"
"Expected forms of the loss terms."
"the number of parameters ğ‘ that deï¬nes the size of the functional approximation space. On the set"
"1"
""
"of two-layer neural networks, it is expected to be proportional to"
"ğ‘1/2"
"given that it corresponds to early stopping in stochastic ï¬rst order methods, the third term should"
"1"
"scale as the convergence rate of these methods, which is lower-bounded by
(Robbins and Monro,"
"ğ·1/2"
"1951) (and may attain the bound). This convergence rate is expected to be dimension free (see e.g."
"Bubeck, 2015, for a review) and depends only on the loss smoothness; hence we assume that the"
"second term only depends on ğ· in (2). Empirically, we ï¬nd after ï¬tting (2) that"
"ğµ
ğ´"
"(10)
,
ğ¿(ğ‘, ğ·) = ğ¸ +"
"ğ‘ 0.34 +
ğ·0.28"
"with ğ¸ = 1.69, ğ´ = 406.4, ğµ = 410.7. We note that the parameter/data coeï¬ƒcients are both lower"
"than 1"
"2 ; this is expected for the data-eï¬ƒciency coeï¬ƒcient (but far from the known lower-bound)."
