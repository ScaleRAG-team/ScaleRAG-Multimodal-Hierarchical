"Average human expert performance"
"89.8%"
"June 2022 Forecast
57.1%"
"June 2023 Forecast
63.4%"
""
"Table 6 | Massive Multitask Language Understanding (MMLU). We report"
"accuracy over 57 tasks with model and human accuracy comparisons taken from Hendrycks et al."
"(2020). We also include the average prediction for state of the art accuracy in June 2022/2023 made"
"by 73 competitive human forecasters in Steinhardt (2021)."
"tasks for which leakage is less of a concern, such as MMLU (Hendrycks et al., 2020) and BIG-bench"
"(BIG-bench collaboration, 2021) along with various closed-book question answering and common"
"sense analyses."
"4.2.2. MMLU"
"The Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020) consists"
""
"In Table 6, we report Chinchilla’s average"
"5-shot performance on MMLU (the full breakdown of results is shown in Table A6). On this benchmark,"
"Chinchilla signiﬁcantly outperforms Gopher despite being much smaller, with an average accuracy of"
"67.6% (improving upon Gopher by 7.6%). Remarkably, Chinchilla even outperforms the expert forecast"
"for June 2023 of 63.4% accuracy (see Table 6) (Steinhardt, 2021). Furthermore, Chinchilla achieves"
"greater than 90% accuracy on 4 diﬀerent individual tasks– high_school_gov_and_politics,"
"international_law,
sociology, and us_foreign_policy. To our knowledge, no other model"
"has achieved greater than 90% accuracy on a subset."
"In Figure 6, we show a comparison to Gopher broken down by task. Overall, we ﬁnd that Chin-"
"chilla improves performance on the vast majority of tasks. On four tasks (college_mathematics,"
"econometrics,
moral_scenarios, and formal_logic) Chinchilla underperforms Gopher, and"
"there is no change in performance on two tasks."
"4.2.3. Reading comprehension"
"On the ﬁnal word prediction dataset LAMBADA (Paperno et al., 2016), Chinchilla achieves 77.4%"
"accuracy, compared to 74.5% accuracy from Gopher and 76.6% from MT-NLG 530B (see Table 7). On"
"RACE-h and RACE-m (Lai et al., 2017), Chinchilla greatly outperforms Gopher, improving accuracy"
"by more than 10% in both cases—see Table 7."
"4.2.4. BIG-bench"
"We analysed Chinchilla on the same set of BIG-bench tasks (BIG-bench collaboration, 2021) reported"
"in Rae et al. (2021). Similar to what we observed in MMLU, Chinchilla outperforms Gopher on the"
""
"tasks (see Figure 7). We ﬁnd that Chinchilla improves the average performance"
"by 10.7%, reaching an accuracy of 65.1% versus 54.4% for Gopher. Of the 62 tasks we consider,"
"Chinchilla performs worse than Gopher on only four—crash_blossom,
dark_humor_detection,"
