"4. Chinchilla"
"Based on our analysis in Section 3, the optimal model size for the Gopher compute budget is somewhere"
"between 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end"
"of this range—70B parameters—for 1.4T tokens, due to both dataset and computational eﬃciency"
""
"In this section we compare this model, which we call Chinchilla, to Gopher and other"
"LLMs. Both Chinchilla and Gopher have been trained for the same number of FLOPs but diﬀer in the"
"size of the model and the number of training tokens."
"While pre-training a large language model has a considerable compute cost, downstream ﬁne-"
"tuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being 4×"
"smaller than Gopher, both the memory footprint and inference cost of Chinchilla are also smaller."
"4.1. Model and training details"
"The full set of hyperparameters used to train Chinchilla are given in Table 4. Chinchilla uses the same"
"model architecture and training setup as Gopher with the exception of the diﬀerences listed below."
