"A. Training dataset","","","",""
"In Table A1 we show the training dataset makeup used for Chinchilla and all scaling runs. Note that","","","",""
"both the MassiveWeb and Wikipedia subsets are both used for more than one epoch.","","","",""
"","Disk Size","Documents","Sampling proportion","Epochs in 1.4T tokens"
"","1.9 TB","604M","45% (48%)","1.24"
"MassiveWeb","","","",""
"Books","2.1 TB","4M","30% (27%)","0.75"
"C4","0.75 TB","361M","10% (10%)","0.77"
"News","2.7 TB","1.1B","10% (10%)","0.21"
"GitHub","3.1 TB","142M","4% (3%)","0.13"
"Wikipedia","0.001 TB","6M","1% (2%)","3.40"
