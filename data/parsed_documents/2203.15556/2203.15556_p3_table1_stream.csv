"and the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are","",""
"trained for approximately 300 billion tokens. We introduce Chinchilla, a substantially smaller model,","",""
"trained for much longer than 300B tokens.","",""
"Model","Size (# Parameters)","Training Tokens"
"LaMDA (Thoppilan et al., 2022)","137 Billion","168 Billion"
"GPT-3 (Brown et al., 2020)","175 Billion","300 Billion"
"Jurassic (Lieber et al., 2021)","178 Billion","300 Billion"
"","",""
"Gopher (Rae et al., 2021)","",""
"MT-NLG 530B (Smith et al., 2022)","530 Billion","270 Billion"
"","70 Billion","1.4 Trillion"
"Chinchilla","",""
