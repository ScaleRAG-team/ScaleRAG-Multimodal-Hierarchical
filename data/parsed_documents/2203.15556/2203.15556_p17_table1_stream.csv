"J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-"
"derPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy"
"programs. 2018. URL http://github.com/google/jax."
"T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,"
"G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,"
"D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,"
"C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot"
"learners.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances"
"in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.,"
"2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb49674"
"18bfb8ac142f64a-Paper.pdf."
"S. Bubeck. Convex Optimization: Algorithms and Complexity. Foundations and Trends in Machine"
"Learning, 8(3-4):231–357, 2015. URL http://www.nowpublishers.com/article/Detail"
"s/MAL-050."
"A. Clark, D. d.
l. Casas, A. Guy, A. Mensch, M. Paganini, J. Hoﬀmann, B. Damoc, B. Hechtman,"
"T. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican,"
"A. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen,"
"K. Kavukcuoglu, and K. Simonyan. Uniﬁed scaling laws for routed language models, 2022. URL"
"https://arxiv.org/abs/2202.01169."
"C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring"
"the surprising diﬃculty of natural yes/no questions.
In Proceedings of the 2019 Conference of"
"the North American Chapter of the Association for Computational Linguistics: Human Language"
"Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, 2019."
"N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph,"
"L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-"
"Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Eﬃcient scaling of"
"language models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905."
"W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with"
"simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
"L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,"
"S. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv"
"preprint arXiv:2101.00027, 2020."
"S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating"
"neural toxic degeneration in language models.
In Findings of the Association for Computational"
"Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational"
"Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2"
"020.findings-emnlp.301."
"K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model"
"pre-training, 2020."
"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive"
"multitask language understanding. arXiv preprint arXiv:2009.03300, 2020."
