"","lightweight) model at the same compute budget.",""
"","Though there has been signiﬁcant recent work allowing larger and larger models to be trained,",""
"","our analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that",""
"","scaling to larger and larger datasets is only beneﬁcial when the data is high-quality. This calls for",""
"","responsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require",""
"","extra care to ensure train-test set overlap is properly accounted for, both in the language modelling",""
"","loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical",""
"","and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and",""
"","private information. With even larger datasets being used, the quantity (if not the frequency) of such",""
"","information increases, which makes dataset introspection all the more important. Chinchilla does",""
"","suﬀer from bias and toxicity but interestingly it seems less aﬀected than Gopher. Better understanding",""
"","how performance of large language models and toxicity interact is an important future research",""
"question.","",""
"","While we have applied our methodology towards the training of auto-regressive language models,",""
"we expect","that","there is a similar trade-oﬀ between model size and the amount of data in other"
"","modalities. As training large models is very expensive, choosing the optimal model size and training",""
"","steps beforehand is essential. The methods we propose are easy to reproduce in new settings.",""
"","6. Acknowledgements",""
"We’d like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis,","",""
"","Geoﬀrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on",""
"","the manuscript. We’d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and",""
"","other colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA",""
"","team for their support and assistance.",""
"References","",""
"","M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru,",""
"","G. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O’Horo,",""
