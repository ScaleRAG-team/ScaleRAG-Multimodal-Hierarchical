{
  "title": null,
  "authors": [],
  "source_path": "../data/pdf/2203.15556.pdf",
  "page_count": 36,
  "processed_pages": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36
  ],
  "counts": {
    "texts": 886,
    "pictures": 4,
    "tables": 57
  },
  "stats_per_page": [
    {
      "page": 1,
      "text_blocks": 11,
      "layout_blocks": 1,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 1,
      "tables_found": 1
    },
    {
      "page": 2,
      "text_blocks": 16,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 3,
      "text_blocks": 9,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 4,
      "text_blocks": 8,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 5,
      "text_blocks": 40,
      "layout_blocks": 1,
      "xobjects_found": 1,
      "xobjects_exported": 1,
      "reused_exported": 0,
      "rasterized": 1,
      "tables_found": 2
    },
    {
      "page": 6,
      "text_blocks": 41,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 7,
      "text_blocks": 34,
      "layout_blocks": 1,
      "xobjects_found": 1,
      "xobjects_exported": 1,
      "reused_exported": 0,
      "rasterized": 1,
      "tables_found": 1
    },
    {
      "page": 8,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 9,
      "text_blocks": 13,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 10,
      "text_blocks": 36,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 11,
      "text_blocks": 12,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 12,
      "text_blocks": 73,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 13,
      "text_blocks": 81,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 14,
      "text_blocks": 10,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 15,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 16,
      "text_blocks": 12,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 17,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 18,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 19,
      "text_blocks": 18,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 20,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 21,
      "text_blocks": 7,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 22,
      "text_blocks": 13,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 23,
      "text_blocks": 120,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 6
    },
    {
      "page": 24,
      "text_blocks": 19,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 25,
      "text_blocks": 21,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 26,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 27,
      "text_blocks": 30,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 28,
      "text_blocks": 33,
      "layout_blocks": 1,
      "xobjects_found": 1,
      "xobjects_exported": 1,
      "reused_exported": 0,
      "rasterized": 1,
      "tables_found": 1
    },
    {
      "page": 29,
      "text_blocks": 71,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 30,
      "text_blocks": 12,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 31,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 32,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 33,
      "text_blocks": 10,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 34,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 35,
      "text_blocks": 4,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 36,
      "text_blocks": 4,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    }
  ],
  "texts": [
    {
      "page_no": 1,
      "bbox": [
        72.7760009765625,
        84.64623260498047,
        498.5903625488281,
        102.57903289794922
      ],
      "text": "Training Compute-Optimal Large Language Models"
    },
    {
      "page_no": 1,
      "bbox": [
        63.10600280761719,
        119.33602142333984,
        509.1408386230469,
        170.08651733398438
      ],
      "text": "Jordan Hoﬀmann★, Sebastian Borgeaud★, Arthur Mensch★, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre★"
    },
    {
      "page_no": 1,
      "bbox": [
        249.89398193359375,
        171.6800537109375,
        321.2296447753906,
        181.78103637695312
      ],
      "text": "★Equal contributions"
    },
    {
      "page_no": 1,
      "bbox": [
        61.783966064453125,
        209.39466857910156,
        534.6393432617188,
        391.5113220214844
      ],
      "text": "We investigate the optimal model size and number of tokens for training a transformer language model\nunder a given compute budget. We ﬁnd that current large language models are signiﬁcantly under-\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\ntraining data constant. By training over 400 language models ranging from 70 million to over 16 billion\nparameters on 5 to 500 billion tokens, we ﬁnd that for compute-optimal training, the model size and\nthe number of training tokens should be scaled equally: for every doubling of model size the number\nof training tokens should also be doubled. We test this hypothesis by training a predicted compute-\noptimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and\n4× more more data. Chinchilla uniformly and signiﬁcantly outperforms Gopher (280B), GPT-3 (175B),\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\nThis also means that Chinchilla uses substantially less compute for ﬁne-tuning and inference, greatly\nfacilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher."
    },
    {
      "page_no": 1,
      "bbox": [
        62.3619384765625,
        416.6720886230469,
        154.67953491210938,
        429.6235046386719
      ],
      "text": "1. Introduction"
    },
    {
      "page_no": 1,
      "bbox": [
        61.457000732421875,
        443.2667236328125,
        533.11767578125,
        508.371826171875
      ],
      "text": "Recently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020; Lieber\net al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense\nlanguage models now having over 500 billion parameters. These large autoregressive transformers\n(Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols such as zero-shot, few-shot, and ﬁne-tuning."
    },
    {
      "page_no": 1,
      "bbox": [
        61.9370002746582,
        517.7867431640625,
        534.7332763671875,
        582.8928833007812
      ],
      "text": "The compute and energy cost for training large language models is substantial (Rae et al., 2021;\nThoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training\ncompute budget is often known in advance: how many accelerators are available and for how long\nwe want to use them. Since it is typically only feasible to train these large models once, accurately\nestimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021)."
    },
    {
      "page_no": 1,
      "bbox": [
        62.36199951171875,
        592.3077392578125,
        532.9164428710938,
        711.6098022460938
      ],
      "text": "Kaplan et al. (2020) showed that there is a power law relationship between the number of\nparameters in an autoregressive language model (LM) and its performance. As a result, the ﬁeld has\nbeen training larger and larger models, expecting performance improvements. One notable conclusion\nin Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be\ncompute optimal. Whilst we reach the same conclusion, we estimate that large models should be\ntrained for many more training tokens than recommended by the authors. Speciﬁcally, given a 10×\nincrease computational budget, they suggests that the size of the model should increase 5.5× while\nthe number of training tokens should only increase 1.8×. Instead, we ﬁnd that model size and the\nnumber of training tokens should be scaled in equal proportions."
    },
    {
      "page_no": 1,
      "bbox": [
        62.36199951171875,
        721.0247192382812,
        532.9183959960938,
        759.0328369140625
      ],
      "text": "Following Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the\nrecently trained large models have been trained for approximately 300 billion tokens (Table 1), in\nline with the approach of predominantly increasing model size when increasing compute."
    },
    {
      "page_no": 1,
      "bbox": [
        61.64500045776367,
        786.6557006835938,
        352.8165588378906,
        804.3370361328125
      ],
      "text": "Corresponding authors: {jordanhoﬀmann|sborgeaud|amensch|sifre}@deepmind.com\n© 2023 DeepMind. All rights reserved"
    },
    {
      "page_no": 1,
      "bbox": [
        10.940000534057617,
        259.92999267578125,
        37.619998931884766,
        609.8900146484375
      ],
      "text": "arXiv:2203.15556v1  [cs.CL]  29 Mar 2022"
    },
    {
      "page_no": 2,
      "bbox": [
        159.1620635986328,
        261.1795349121094,
        355.364013671875,
        284.268798828125
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 2,
      "bbox": [
        148.18594360351562,
        255.4027557373047,
        164.78915405273438,
        268.59197998046875
      ],
      "text": "10M"
    },
    {
      "page_no": 2,
      "bbox": [
        143.24017333984375,
        222.44955444335938,
        164.78933715820312,
        235.6387939453125
      ],
      "text": "100M"
    },
    {
      "page_no": 2,
      "bbox": [
        147.0913543701172,
        189.49635314941406,
        164.79104614257812,
        202.6855926513672
      ],
      "text": "1.0B"
    },
    {
      "page_no": 2,
      "bbox": [
        149.56173706054688,
        156.5431671142578,
        164.7884521484375,
        169.73240661621094
      ],
      "text": "10B"
    },
    {
      "page_no": 2,
      "bbox": [
        144.61595153808594,
        123.58998107910156,
        164.78863525390625,
        136.7792205810547
      ],
      "text": "100B"
    },
    {
      "page_no": 2,
      "bbox": [
        155.08995056152344,
        90.63677978515625,
        164.78746032714844,
        103.82601928710938
      ],
      "text": "1T"
    },
    {
      "page_no": 2,
      "bbox": [
        129.73204040527344,
        150.03939819335938,
        144.12030029296875,
        198.97303771972656
      ],
      "text": "Parameters"
    },
    {
      "page_no": 2,
      "bbox": [
        393.8568115234375,
        116.87117004394531,
        506.0663757324219,
        221.3398895263672
      ],
      "text": "Approach 1\nApproach 2\nApproach 3\nKaplan et al (2020)\n \nChinchilla (70B)\nGopher (280B)\nGPT-3 (175B)\nMegatron-Turing NLG (530B)"
    },
    {
      "page_no": 2,
      "bbox": [
        62.36199951171875,
        294.8852844238281,
        534.426025390625,
        374.0816955566406
      ],
      "text": "Figure 1 | Overlaid predictions. We overlay the predictions from our three diﬀerent approaches,\nalong with projections from Kaplan et al. (2020). We ﬁnd that all three methods predict that current\nlarge models should be substantially smaller and therefore trained much longer than is currently\ndone. In Figure A3, we show the results with the predicted optimal tokens plotted against the optimal\nnumber of parameters for ﬁxed FLOP budgets. Chinchilla outperforms Gopher and the other large\nmodels (see Section 4.2)."
    },
    {
      "page_no": 2,
      "bbox": [
        62.36199951171875,
        394.2247619628906,
        532.9129638671875,
        422.61541748046875
      ],
      "text": "In this work, we revisit the question: Given a ﬁxed FLOPs budget,1 how should one trade-oﬀmodel\nsize and the number of training tokens? To answer this question, we model the ﬁnal pre-training loss2"
    },
    {
      "page_no": 2,
      "bbox": [
        62.36199951171875,
        423.7420349121094,
        534.7353515625,
        499.3711242675781
      ],
      "text": "𝐿(𝑁, 𝐷) as a function of the number of model parameters 𝑁, and the number of training tokens, 𝐷.\nSince the computational budget 𝐶is a deterministic function FLOPs(𝑁, 𝐷) of the number of seen\ntraining tokens and model parameters, we are interested in minimizing 𝐿under the constraint\nFLOPs(𝑁, 𝐷) = 𝐶:\n𝑁𝑜𝑝𝑡(𝐶), 𝐷𝑜𝑝𝑡(𝐶) =\nargmin\n𝑁,𝐷s.t. FLOPs(𝑁,𝐷)=𝐶\n𝐿(𝑁, 𝐷).\n(1)"
    },
    {
      "page_no": 2,
      "bbox": [
        62.03499984741211,
        506.0450439453125,
        533.1226196289062,
        585.8878784179688
      ],
      "text": "The functions 𝑁𝑜𝑝𝑡(𝐶), and 𝐷𝑜𝑝𝑡(𝐶) describe the optimal allocation of a computational budget 𝐶. We\nempirically estimate these functions based on the losses of over 400 models, ranging from under 70M\nto over 16B parameters, and trained on 5B to over 400B tokens – with each model conﬁguration\ntrained for several diﬀerent training horizons. Our approach leads to considerably diﬀerent results\nthan that of Kaplan et al. (2020). We highlight our results in Figure 1 and how our approaches diﬀer\nin Section 2."
    },
    {
      "page_no": 2,
      "bbox": [
        62.36199951171875,
        595.302734375,
        533.1260986328125,
        701.0558471679688
      ],
      "text": "Based on our estimated compute-optimal frontier, we predict that for the compute budget used\nto train Gopher, an optimal model should be 4 times smaller, while being training on 4 times more\ntokens. We verify this by training a more compute-optimal 70B model, called Chinchilla, on 1.4 trillion\ntokens. Not only does Chinchilla outperform its much larger counterpart, Gopher, but its reduced\nmodel size reduces inference cost considerably and greatly facilitates downstream uses on smaller\nhardware. The energy cost of a large language model is amortized through its usage for inference an\nﬁne-tuning. The beneﬁts of a more optimally trained smaller model, therefore, extend beyond the\nimmediate beneﬁts of its improved performance."
    },
    {
      "page_no": 2,
      "bbox": [
        62.012001037597656,
        725.3308715820312,
        532.9097290039062,
        758.644287109375
      ],
      "text": "1For example, knowing the number of accelerators and a target training duration.\n2For simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as\nwe are in the inﬁnite data regime (the number of training tokens is less than the number of tokens in the entire corpus)."
    },
    {
      "page_no": 2,
      "bbox": [
        294.6050109863281,
        778.01171875,
        300.67047119140625,
        788.9208374023438
      ],
      "text": "2"
    },
    {
      "page_no": 3,
      "bbox": [
        62.03499984741211,
        85.54524993896484,
        534.427490234375,
        137.35382080078125
      ],
      "text": "Table 1 | Current LLMs. We show ﬁve of the current largest dense transformer models, their size,\nand the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are\ntrained for approximately 300 billion tokens. We introduce Chinchilla, a substantially smaller model,\ntrained for much longer than 300B tokens."
    },
    {
      "page_no": 3,
      "bbox": [
        117.43199920654297,
        153.87574768066406,
        477.8453369140625,
        164.78485107421875
      ],
      "text": "Model\nSize (# Parameters)\nTraining Tokens"
    },
    {
      "page_no": 3,
      "bbox": [
        117.43199157714844,
        172.8987579345703,
        465.5413818359375,
        238.00390625
      ],
      "text": "LaMDA (Thoppilan et al., 2022)\n137 Billion\n168 Billion\nGPT-3 (Brown et al., 2020)\n175 Billion\n300 Billion\nJurassic (Lieber et al., 2021)\n178 Billion\n300 Billion\nGopher (Rae et al., 2021)\n280 Billion\n300 Billion\nMT-NLG 530B (Smith et al., 2022)\n530 Billion\n270 Billion"
    },
    {
      "page_no": 3,
      "bbox": [
        117.43199920654297,
        246.11671447753906,
        465.8561706542969,
        257.37042236328125
      ],
      "text": "Chinchilla\n70 Billion\n1.4 Trillion"
    },
    {
      "page_no": 3,
      "bbox": [
        62.36199951171875,
        282.5220031738281,
        160.59840393066406,
        295.4734191894531
      ],
      "text": "2. Related Work"
    },
    {
      "page_no": 3,
      "bbox": [
        62.36199951171875,
        309.11669921875,
        533.3364868164062,
        455.5178527832031
      ],
      "text": "Large language models.\nA variety of large language models have been introduced in the last few\nyears. These include both dense transformer models (Brown et al., 2020; Lieber et al., 2021; Rae\net al., 2021; Smith et al., 2022; Thoppilan et al., 2022) and mixture-of-expert (MoE) models (Du\net al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear—so far\nincreasing the size of language models has been responsible for improving the state-of-the-art in many\nlanguage modelling tasks. Nonetheless, large language models face several challenges, including\ntheir overwhelming computational requirements (the cost of training and inference increase with\nmodel size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality\ntraining data. In fact, in this work we ﬁnd that larger, high quality datasets will play a key role in any\nfurther scaling of language models."
    },
    {
      "page_no": 3,
      "bbox": [
        61.457000732421875,
        480.19476318359375,
        534.7373046875,
        734.9888305664062
      ],
      "text": "Modelling the scaling behavior.\nUnderstanding the scaling behaviour of language models and\ntheir transfer properties has been important in the development of recent large models (Hernandez\net al., 2021; Kaplan et al., 2020). Kaplan et al. (2020) ﬁrst showed a predictable relationship between\nmodel size and loss over many orders of magnitude. The authors investigate the question of choosing\nthe optimal model size to train for a given compute budget. Similar to us, they address this question\nby training various models. Our work diﬀers from Kaplan et al. (2020) in several important ways.\nFirst, the authors use a ﬁxed number of training tokens and learning rate schedule for all models; this\nprevents them from modelling the impact of these hyperparameters on the loss. In contrast, we ﬁnd\nthat setting the learning rate schedule to approximately match the number of training tokens results\nin the best ﬁnal loss regardless of model size—see Figure A1. For a ﬁxed learning rate cosine schedule\nto 130B tokens, the intermediate loss estimates (for 𝐷′ << 130B) are therefore overestimates of the\nloss of a model trained with a schedule length matching 𝐷′. Using these intermediate losses results in\nunderestimating the eﬀectiveness of training models on less data than 130B tokens, and eventually\ncontributes to the conclusion that model size should increase faster than training data size as compute\nbudget increases. In contrast, our analysis predicts that both quantities should scale at roughly the\nsame rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight\ncurvature in the FLOP-loss frontier (see Appendix E)—in fact, the majority of the models used in\nour analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al.\n(2020) are signiﬁcantly smaller—many being less than 100M parameters."
    },
    {
      "page_no": 3,
      "bbox": [
        79.29900360107422,
        744.4036865234375,
        533.0974731445312,
        755.3128051757812
      ],
      "text": "Recently, Clark et al. (2022) speciﬁcally looked in to the scaling properties of Mixture of Expert"
    },
    {
      "page_no": 3,
      "bbox": [
        294.6050109863281,
        778.01171875,
        300.67047119140625,
        788.9208374023438
      ],
      "text": "3"
    },
    {
      "page_no": 4,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        532.9126586914062,
        138.82781982421875
      ],
      "text": "language models, showing that the scaling with number of experts diminishes as the model size\nincreases—their approach models the loss as a function of two variables: the model size and the\nnumber of experts. However, the analysis is done with a ﬁxed number of training tokens, as in Kaplan\net al. (2020), potentially underestimating the improvements of branching."
    },
    {
      "page_no": 4,
      "bbox": [
        61.457000732421875,
        163.5037384033203,
        534.7393188476562,
        323.453857421875
      ],
      "text": "Estimating hyperparameters for large models.\nThe model size and the number of training tokens\nare not the only two parameters to chose when selecting a language model and a procedure to train\nit. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and\nwidth-to-depth ratio. In this work, we focus on model size and the number of training steps, and\nwe rely on existing work and provided experimental heuristics to determine the other necessary\nhyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for\ntraining an autoregressive transformer, including the learning rate and batch size. McCandlish et al.\n(2018) ﬁnds only a weak dependence between optimal batch size and model size. Shallue et al.\n(2018); Zhang et al. (2019) suggest that using larger batch-sizes than those we use is possible. Levine\net al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model sizes. We\nuse slightly less deep models than proposed as this translates to better wall-clock performance on our\nhardware."
    },
    {
      "page_no": 4,
      "bbox": [
        61.457000732421875,
        348.1307373046875,
        534.7402954101562,
        494.5318298339844
      ],
      "text": "Improved model architectures.\nRecently, various promising alternatives to traditional dense trans-\nformers have been proposed. For example, through the use of conditional computation large MoE\nmodels like the 1.7 trillion parameter Switch transformer (Fedus et al., 2021), the 1.2 Trillion pa-\nrameter GLaM model (Du et al., 2021), and others (Artetxe et al., 2021; Zoph et al., 2022) are able\nto provide a large eﬀective model size despite using relatively fewer training and inference FLOPs.\nHowever, for very large models the computational beneﬁts of routed models seems to diminish (Clark\net al., 2022). An orthogonal approach to improving language models is to augment transformers\nwith explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al.\n(2020). This approach eﬀectively increases the number of data tokens seen during training (by a\nfactor of ∼10 in Borgeaud et al. (2021)). This suggests that the performance of language models\nmay be more dependant on the size of the training data than previously thought."
    },
    {
      "page_no": 4,
      "bbox": [
        62.36199951171875,
        518.7969970703125,
        443.301513671875,
        531.7483520507812
      ],
      "text": "3. Estimating the optimal parameter/training tokens allocation"
    },
    {
      "page_no": 4,
      "bbox": [
        61.86000061035156,
        545.3917236328125,
        534.7330322265625,
        610.496826171875
      ],
      "text": "We present three diﬀerent approaches to answer the question driving our research: Given a ﬁxed\nFLOPs budget, how should one trade-oﬀmodel size and the number of training tokens? In all three\ncases we start by training a range of models varying both model size and the number of training\ntokens and use the resulting training curves to ﬁt an empirical estimator of how they should scale.\nWe assume a power-law relationship between compute and model size as done in Clark et al. (2022);"
    },
    {
      "page_no": 4,
      "bbox": [
        61.9370002746582,
        613.1376953125,
        535.6407470703125,
        678.2428588867188
      ],
      "text": "Kaplan et al. (2020), though future work may want to include potential curvature in this relationship\nfor large model sizes. The resulting predictions are similar for all three methods and suggest that\nparameter count and number of training tokens should be increased equally with more compute3—\nwith proportions reported in Table 2. This is in clear contrast to previous work on this topic and\nwarrants further investigation."
    },
    {
      "page_no": 4,
      "bbox": [
        77.34500122070312,
        747.77783203125,
        266.6216735839844,
        758.644287109375
      ],
      "text": "3We compute FLOPs as described in Appendix F."
    },
    {
      "page_no": 4,
      "bbox": [
        294.6050109863281,
        778.01171875,
        300.67047119140625,
        788.9208374023438
      ],
      "text": "4"
    },
    {
      "page_no": 5,
      "bbox": [
        92.81494903564453,
        185.92364501953125,
        207.11355590820312,
        200.96685791015625
      ],
      "text": "1017\n1018\n1019\n1020\n1021\n1022\nFLOPS"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        175.802001953125,
        90.02289581298828,
        184.34645080566406
      ],
      "text": "2.0"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        158.1177520751953,
        90.02289581298828,
        166.66220092773438
      ],
      "text": "2.5"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        143.66867065429688,
        90.02289581298828,
        152.21311950683594
      ],
      "text": "3.0"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        131.4521484375,
        90.02289581298828,
        139.99659729003906
      ],
      "text": "3.5"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        120.86972045898438,
        90.02289581298828,
        129.41416931152344
      ],
      "text": "4.0"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        111.53535461425781,
        90.02289581298828,
        120.07980346679688
      ],
      "text": "4.5"
    },
    {
      "page_no": 5,
      "bbox": [
        82.0124740600586,
        88.73639678955078,
        90.02289581298828,
        111.72992706298828
      ],
      "text": "5.0\n5.5\n6.0"
    },
    {
      "page_no": 5,
      "bbox": [
        73.1749038696289,
        118.64775085449219,
        82.4961166381836,
        153.51437377929688
      ],
      "text": "Training loss"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        154.4118194580078,
        203.77198791503906,
        162.95626831054688
      ],
      "text": "75M"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        138.2939453125,
        206.9761505126953,
        146.83839416503906
      ],
      "text": "250M"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        129.01461791992188,
        206.9761505126953,
        137.55906677246094
      ],
      "text": "500M"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        119.73528289794922,
        199.67608642578125,
        128.27972412109375
      ],
      "text": "1B"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        107.46867370605469,
        204.4823455810547,
        116.01312255859375
      ],
      "text": "2.5B"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        98.18934631347656,
        199.67608642578125,
        106.73379516601562
      ],
      "text": "5B"
    },
    {
      "page_no": 5,
      "bbox": [
        193.01583862304688,
        90.0262680053711,
        202.88026428222656,
        98.57071685791016
      ],
      "text": "10B"
    },
    {
      "page_no": 5,
      "bbox": [
        384.24932861328125,
        185.92364501953125,
        519.2185668945312,
        200.96685791015625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 5,
      "bbox": [
        378.75335693359375,
        165.69725036621094,
        387.45513916015625,
        174.30674743652344
      ],
      "text": "109"
    },
    {
      "page_no": 5,
      "bbox": [
        376.46337890625,
        142.5429229736328,
        387.4089050292969,
        151.1524200439453
      ],
      "text": "1010"
    },
    {
      "page_no": 5,
      "bbox": [
        376.46337890625,
        119.34098052978516,
        387.4089050292969,
        127.95048522949219
      ],
      "text": "1011"
    },
    {
      "page_no": 5,
      "bbox": [
        376.46337890625,
        96.2342300415039,
        387.4089050292969,
        104.84373474121094
      ],
      "text": "1012"
    },
    {
      "page_no": 5,
      "bbox": [
        367.6257629394531,
        126.09666442871094,
        376.9469909667969,
        145.72837829589844
      ],
      "text": "Tokens"
    },
    {
      "page_no": 5,
      "bbox": [
        389.85986328125,
        91.28656005859375,
        397.4203186035156,
        97.11231994628906
      ],
      "text": "1.5T"
    },
    {
      "page_no": 5,
      "bbox": [
        235.42092895507812,
        185.92364501953125,
        370.3901672363281,
        200.96685791015625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 5,
      "bbox": [
        224.8905029296875,
        168.05874633789062,
        238.85079956054688,
        176.6031951904297
      ],
      "text": "100M"
    },
    {
      "page_no": 5,
      "bbox": [
        227.38446044921875,
        148.14125061035156,
        238.85093688964844,
        156.68569946289062
      ],
      "text": "1.0B"
    },
    {
      "page_no": 5,
      "bbox": [
        228.98745727539062,
        128.2237548828125,
        238.85186767578125,
        136.76820373535156
      ],
      "text": "10B"
    },
    {
      "page_no": 5,
      "bbox": [
        225.7814483642578,
        108.30624389648438,
        238.85003662109375,
        116.85069274902344
      ],
      "text": "100B"
    },
    {
      "page_no": 5,
      "bbox": [
        232.57273864746094,
        88.38873291015625,
        238.85511779785156,
        96.93318176269531
      ],
      "text": "1T"
    },
    {
      "page_no": 5,
      "bbox": [
        216.0529327392578,
        120.50563049316406,
        225.3741455078125,
        152.20657348632812
      ],
      "text": "Parameters"
    },
    {
      "page_no": 5,
      "bbox": [
        241.0314178466797,
        110.8918228149414,
        247.75714111328125,
        116.71758270263672
      ],
      "text": "67B"
    },
    {
      "page_no": 5,
      "bbox": [
        61.457000732421875,
        211.68429565429688,
        532.9150390625,
        290.9354248046875
      ],
      "text": "Figure 2 | Training curve envelope. On the left we show all of our diﬀerent runs. We launched a\nrange of model sizes going from 70M to 10B, each for four diﬀerent cosine cycle lengths. From these\ncurves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the\noptimal model size (center) for a given compute budget and the optimal number of training tokens\n(right). In green, we show projections of optimal model size and training token count based on the\nnumber of FLOPs used to train Gopher (5.76 × 1023)."
    },
    {
      "page_no": 5,
      "bbox": [
        62.36198425292969,
        314.9205627441406,
        411.7805480957031,
        325.82965087890625
      ],
      "text": "3.1. Approach 1: Fix model sizes and vary number of training tokens"
    },
    {
      "page_no": 5,
      "bbox": [
        62.05699920654297,
        336.94671630859375,
        534.73291015625,
        388.5028381347656
      ],
      "text": "In our ﬁrst approach we vary the number of training steps for a ﬁxed family of models (ranging from\n70M to over 10B parameters), training each model for 4 diﬀerent number of training sequences.\nFrom these runs, we are able to directly extract an estimate of the minimum loss achieved for a given\nnumber of training FLOPs. Training details for this approach can be found in Appendix D."
    },
    {
      "page_no": 5,
      "bbox": [
        61.457000732421875,
        396.73004150390625,
        533.12109375,
        571.4178466796875
      ],
      "text": "For each parameter count 𝑁we train 4 diﬀerent models, decaying the learning rate by a factor of\n10× over a horizon (measured in number of training tokens) that ranges by a factor of 16×. Then, for\neach run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous\nmapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine\nwhich run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP\ncount 𝐶, to the most eﬃcient choice of model size 𝑁and number of training tokens 𝐷such that\nFLOPs(𝑁, 𝐷) = 𝐶.4 At 1500 logarithmically spaced FLOP values, we ﬁnd which model size achieves the\nlowest loss of all models along with the required number of training tokens. Finally, we ﬁt power laws\nto estimate the optimal model size and number of training tokens for any given amount of compute\n(see the center and right panels of Figure 2), obtaining a relationship 𝑁𝑜𝑝𝑡∝𝐶𝑎and 𝐷𝑜𝑝𝑡∝𝐶𝑏. We\nﬁnd that 𝑎= 0.50 and 𝑏= 0.50—as summarized in Table 2. In Section D.4, we show a head-to-head\ncomparison at 1021 FLOPs, using the model size recommended by our analysis and by the analysis of\nKaplan et al. (2020)—using the model size we predict has a clear advantage."
    },
    {
      "page_no": 5,
      "bbox": [
        62.36199951171875,
        594.2406005859375,
        234.1803436279297,
        605.1497192382812
      ],
      "text": "3.2. Approach 2: IsoFLOP proﬁles"
    },
    {
      "page_no": 5,
      "bbox": [
        62.36199951171875,
        613.4136962890625,
        532.4154052734375,
        628.2828369140625
      ],
      "text": "In our second approach we vary the model size5 for a ﬁxed set of 9 diﬀerent training FLOP counts6"
    },
    {
      "page_no": 5,
      "bbox": [
        61.457000732421875,
        626.9627075195312,
        532.9176635742188,
        668.9298706054688
      ],
      "text": "(ranging from 6 × 1018 to 3 × 1021 FLOPs), and consider the ﬁnal training loss for each point7. in\ncontrast with Approach 1 that considered points (𝑁, 𝐷, 𝐿) along the entire training runs. This allows\nus to directly answer the question: For a given FLOP budget, what is the optimal parameter count?"
    },
    {
      "page_no": 5,
      "bbox": [
        62.012001037597656,
        691.1998291015625,
        534.1609497070312,
        758.644287109375
      ],
      "text": "4Note that all selected points are within the last 15% of training. This suggests that when training a model over 𝐷tokens,\nwe should pick a cosine cycle length that decays 10× over approximately 𝐷tokens—see further details in Appendix B.\n5In approach 2, model size varies up to 16B as opposed to approach 1 where we only used models up to 10B.\n6The number of training tokens is determined by the model size and training FLOPs.\n7We set the cosine schedule length to match the number of tokens, which is optimal according to the analysis presented\nin Appendix B."
    },
    {
      "page_no": 5,
      "bbox": [
        294.60498046875,
        778.01171875,
        300.6704406738281,
        788.9208374023438
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        112.34905242919922,
        197.939208984375,
        218.36331176757812,
        213.50299072265625
      ],
      "text": "100M 300M\n1B\n3B\n6B\n30B\nParameters"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        193.56065368652344,
        90.81959533691406,
        202.4612579345703
      ],
      "text": "2.0"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        176.0889129638672,
        90.81959533691406,
        184.98951721191406
      ],
      "text": "2.2"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        158.61717224121094,
        90.81959533691406,
        167.5177764892578
      ],
      "text": "2.4"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        141.1454315185547,
        90.81959533691406,
        150.04603576660156
      ],
      "text": "2.6"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        123.67371368408203,
        90.81959533691406,
        132.57431030273438
      ],
      "text": "2.8"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        106.20196533203125,
        90.81959533691406,
        115.10255432128906
      ],
      "text": "3.0"
    },
    {
      "page_no": 6,
      "bbox": [
        82.47528839111328,
        88.730224609375,
        90.81959533691406,
        97.63081359863281
      ],
      "text": "3.2"
    },
    {
      "page_no": 6,
      "bbox": [
        73.22306823730469,
        124.0032958984375,
        82.93280029296875,
        161.92050170898438
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 6,
      "bbox": [
        112.62345886230469,
        124.1462631225586,
        125.8641357421875,
        194.6542510986328
      ],
      "text": "6e18\n1e19\n3e19\n6e19\n1e20\n3e20\n6e20\n1e21\n3e21"
    },
    {
      "page_no": 6,
      "bbox": [
        235.786376953125,
        197.78729248046875,
        370.8138427734375,
        213.50299072265625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 6,
      "bbox": [
        224.6320343017578,
        178.29537963867188,
        239.1742401123047,
        187.19598388671875
      ],
      "text": "100M"
    },
    {
      "page_no": 6,
      "bbox": [
        232.23959350585938,
        156.45570373535156,
        239.1774444580078,
        165.35630798339844
      ],
      "text": "1B"
    },
    {
      "page_no": 6,
      "bbox": [
        228.90074157714844,
        134.6160430908203,
        239.1763153076172,
        143.5166473388672
      ],
      "text": "10B"
    },
    {
      "page_no": 6,
      "bbox": [
        225.5618896484375,
        112.7763671875,
        239.17520141601562,
        121.67695617675781
      ],
      "text": "100B"
    },
    {
      "page_no": 6,
      "bbox": [
        232.63192749023438,
        90.93669128417969,
        239.17617797851562,
        99.8372802734375
      ],
      "text": "1T"
    },
    {
      "page_no": 6,
      "bbox": [
        215.37982177734375,
        126.8031234741211,
        225.0895538330078,
        159.82537841796875
      ],
      "text": "Parameters"
    },
    {
      "page_no": 6,
      "bbox": [
        241.56167602539062,
        115.98650360107422,
        248.93646240234375,
        122.3744888305664
      ],
      "text": "63B"
    },
    {
      "page_no": 6,
      "bbox": [
        384.14215087890625,
        197.78729248046875,
        519.169677734375,
        213.50299072265625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 6,
      "bbox": [
        372.98785400390625,
        193.56065368652344,
        387.5299987792969,
        202.4612579345703
      ],
      "text": "100M"
    },
    {
      "page_no": 6,
      "bbox": [
        380.59539794921875,
        171.7209930419922,
        387.5332336425781,
        180.62159729003906
      ],
      "text": "1B"
    },
    {
      "page_no": 6,
      "bbox": [
        377.25653076171875,
        149.8813018798828,
        387.5320739746094,
        158.7819061279297
      ],
      "text": "10B"
    },
    {
      "page_no": 6,
      "bbox": [
        373.91766357421875,
        128.04164123535156,
        387.5309753417969,
        136.94224548339844
      ],
      "text": "100B"
    },
    {
      "page_no": 6,
      "bbox": [
        380.98773193359375,
        106.20196533203125,
        387.5319519042969,
        115.10255432128906
      ],
      "text": "1T"
    },
    {
      "page_no": 6,
      "bbox": [
        377.64886474609375,
        84.36229705810547,
        387.5307922363281,
        93.26288604736328
      ],
      "text": "10T"
    },
    {
      "page_no": 6,
      "bbox": [
        363.735595703125,
        132.63294982910156,
        373.4453430175781,
        153.08294677734375
      ],
      "text": "Tokens"
    },
    {
      "page_no": 6,
      "bbox": [
        389.91748046875,
        102.15676879882812,
        398.20758056640625,
        108.54475402832031
      ],
      "text": "1.4T"
    },
    {
      "page_no": 6,
      "bbox": [
        61.457000732421875,
        224.31729125976562,
        533.1231689453125,
        303.56842041015625
      ],
      "text": "Figure 3 | IsoFLOP curves. For various model sizes, we choose the number of training tokens such\nthat the ﬁnal FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We\nﬁnd a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train\n(left). Using the location of these valleys, we project optimal model size and number of tokens for\nlarger models (center and right). In green, we show the estimated number of parameters and tokens\nfor an optimal model trained with the compute budget of Gopher."
    },
    {
      "page_no": 6,
      "bbox": [
        62.36199951171875,
        327.26373291015625,
        533.1043090820312,
        419.53912353515625
      ],
      "text": "For each FLOP budget, we plot the ﬁnal loss (after smoothing) against the parameter count in\nFigure 3 (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see\na clear minimum in the loss. We ﬁt a parabola to each IsoFLOPs curve to directly estimate at what\nmodel size the minimum loss is achieved (Figure 3 (left)). As with the previous approach, we then ﬁt\na power law between FLOPs and loss-optimal model size and number of training tokens, shown in\nFigure 3 (center, right). Again, we ﬁt exponents of the form 𝑁𝑜𝑝𝑡∝𝐶𝑎and 𝐷𝑜𝑝𝑡∝𝐶𝑏and we ﬁnd that\n𝑎= 0.49 and 𝑏= 0.51—as summarized in Table 2."
    },
    {
      "page_no": 6,
      "bbox": [
        62.36199188232422,
        442.2906188964844,
        320.3294982910156,
        453.19970703125
      ],
      "text": "3.3. Approach 3: Fitting a parametric loss function"
    },
    {
      "page_no": 6,
      "bbox": [
        62.36199951171875,
        464.31671142578125,
        532.9155883789062,
        502.3238525390625
      ],
      "text": "Lastly, we model all ﬁnal losses from experiments in Approach 1 & 2 as a parametric function of\nmodel parameter count and the number of seen tokens. Following a classical risk decomposition (see\nSection D.2), we propose the following functional form"
    },
    {
      "page_no": 6,
      "bbox": [
        243.1060028076172,
        511.36810302734375,
        319.0670471191406,
        530.5994262695312
      ],
      "text": "ˆ𝐿(𝑁, 𝐷) ≜𝐸+ 𝐴"
    },
    {
      "page_no": 6,
      "bbox": [
        309.5660095214844,
        511.3680419921875,
        345.1304016113281,
        536.2733764648438
      ],
      "text": "𝑁𝛼+ 𝐵"
    },
    {
      "page_no": 6,
      "bbox": [
        336.21099853515625,
        518.7540283203125,
        532.91357421875,
        536.6654052734375
      ],
      "text": "𝐷𝛽.\n(2)"
    },
    {
      "page_no": 6,
      "bbox": [
        62.03499984741211,
        547.876708984375,
        532.918212890625,
        612.9828491210938
      ],
      "text": "The ﬁrst term captures the loss for an ideal generative process on the data distribution, and should\ncorrespond to the entropy of natural text. The second term captures the fact that a perfectly trained\ntransformer with 𝑁parameters underperforms the ideal generative process. The ﬁnal term captures\nthe fact that the transformer is not trained to convergence, as we only make a ﬁnite number of\noptimisation steps, on a sample of the dataset distribution."
    },
    {
      "page_no": 6,
      "bbox": [
        62.36199951171875,
        636.4710083007812,
        532.9179077148438,
        662.1168212890625
      ],
      "text": "Model ﬁtting.\nTo estimate (𝐴, 𝐵, 𝐸, 𝛼, 𝛽), we minimize the Huber loss (Huber, 1964) between the\npredicted and observed log loss using the L-BFGS algorithm (Nocedal, 1980):"
    },
    {
      "page_no": 6,
      "bbox": [
        195.0120086669922,
        678.1957397460938,
        226.25576782226562,
        694.2217407226562
      ],
      "text": "min\n𝐴,𝐵,𝐸,𝛼,𝛽"
    },
    {
      "page_no": 6,
      "bbox": [
        242.92601013183594,
        674.845458984375,
        257.3587646484375,
        685.7545776367188
      ],
      "text": "∑︁"
    },
    {
      "page_no": 6,
      "bbox": [
        239.31300354003906,
        673.9544677734375,
        532.91357421875,
        701.3140258789062
      ],
      "text": "Runs 𝑖\nHuber𝛿\n\u0010\nlog ˆ𝐿(𝑁𝑖, 𝐷𝑖) −log 𝐿𝑖\n\u0011\n(3)"
    },
    {
      "page_no": 6,
      "bbox": [
        61.86000061035156,
        712.2077026367188,
        533.1264038085938,
        750.2158203125
      ],
      "text": "We account for possible local minima by selecting the best ﬁt from a grid of initialisations. The Huber\nloss (𝛿= 10−3) is robust to outliers, which we ﬁnd important for good predictive performance over\nheld-out data points. Section D.2 details the ﬁtting procedure and the loss decomposition."
    },
    {
      "page_no": 6,
      "bbox": [
        294.60498046875,
        778.01171875,
        300.6704406738281,
        788.9208374023438
      ],
      "text": "6"
    },
    {
      "page_no": 7,
      "bbox": [
        92.61997985839844,
        253.1540985107422,
        302.4764709472656,
        269.0699768066406
      ],
      "text": "1018\n1019\n1020\n1021\n1022\n1023 Gopher"
    },
    {
      "page_no": 7,
      "bbox": [
        168.4612579345703,
        264.6676025390625,
        301.9584045410156,
        287.7178955078125
      ],
      "text": "budget\nTraining FLOPs"
    },
    {
      "page_no": 7,
      "bbox": [
        75.61306762695312,
        226.26446533203125,
        96.78075408935547,
        239.22023010253906
      ],
      "text": "100M"
    },
    {
      "page_no": 7,
      "bbox": [
        86.68094635009766,
        181.16297912597656,
        96.77970886230469,
        194.11874389648438
      ],
      "text": "1B"
    },
    {
      "page_no": 7,
      "bbox": [
        81.81976318359375,
        136.06150817871094,
        96.77693176269531,
        149.01727294921875
      ],
      "text": "10B"
    },
    {
      "page_no": 7,
      "bbox": [
        81.81976318359375,
        109.12767791748047,
        96.77693176269531,
        122.08342742919922
      ],
      "text": "40B"
    },
    {
      "page_no": 7,
      "bbox": [
        76.95857238769531,
        90.96002960205078,
        96.77415466308594,
        103.91577911376953
      ],
      "text": "100B"
    },
    {
      "page_no": 7,
      "bbox": [
        61.749900817871094,
        154.48683166503906,
        75.8834457397461,
        198.32083129882812
      ],
      "text": "Model size"
    },
    {
      "page_no": 7,
      "bbox": [
        164.0955810546875,
        84.4251480102539,
        233.4549560546875,
        98.5586929321289
      ],
      "text": "IsoLoss contours"
    },
    {
      "page_no": 7,
      "bbox": [
        237.9265594482422,
        221.13742065429688,
        290.4662780761719,
        251.15536499023438
      ],
      "text": "Efficient frontier\nEmpirical data\nIsoFLOPs slice"
    },
    {
      "page_no": 7,
      "bbox": [
        318.28045654296875,
        232.5359649658203,
        335.2848815917969,
        245.49172973632812
      ],
      "text": "2.00"
    },
    {
      "page_no": 7,
      "bbox": [
        318.28045654296875,
        169.8876190185547,
        335.2848815917969,
        182.8433837890625
      ],
      "text": "3.00"
    },
    {
      "page_no": 7,
      "bbox": [
        318.28045654296875,
        125.43791961669922,
        335.2848815917969,
        138.3936767578125
      ],
      "text": "4.00"
    },
    {
      "page_no": 7,
      "bbox": [
        318.28045654296875,
        90.96002960205078,
        335.2848815917969,
        103.91577911376953
      ],
      "text": "5.00"
    },
    {
      "page_no": 7,
      "bbox": [
        304.41729736328125,
        167.1060028076172,
        318.55084228515625,
        185.5312957763672
      ],
      "text": "Loss"
    },
    {
      "page_no": 7,
      "bbox": [
        353.3523864746094,
        256.1672058105469,
        471.1819152832031,
        269.1229553222656
      ],
      "text": "100M\n1B\n10B\n40B"
    },
    {
      "page_no": 7,
      "bbox": [
        393.8636169433594,
        272.4914855957031,
        437.6976623535156,
        286.6250305175781
      ],
      "text": "Model size"
    },
    {
      "page_no": 7,
      "bbox": [
        384.5427551269531,
        84.3721694946289,
        447.47705078125,
        98.5057144165039
      ],
      "text": "IsoFLOPs slices"
    },
    {
      "page_no": 7,
      "bbox": [
        486.48333740234375,
        122.64138793945312,
        526.8377075195312,
        133.6341552734375
      ],
      "text": "Train. FLOPs"
    },
    {
      "page_no": 7,
      "bbox": [
        503.16400146484375,
        132.1539764404297,
        527.0291748046875,
        228.76002502441406
      ],
      "text": "6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\n6e+20\n1e+21\n3e+21\nGopher"
    },
    {
      "page_no": 7,
      "bbox": [
        62.36199951171875,
        298.23773193359375,
        534.4301147460938,
        365.2594299316406
      ],
      "text": "Figure 4 | Parametric ﬁt. We ﬁt a parametric modelling of the loss ˆ𝐿(𝑁, 𝐷) and display contour (left)\nand isoFLOP slices (right). For each isoFLOP slice, we include a corresponding dashed line in the left\nplot. In the left plot, we show the eﬃcient frontier in blue, which is a line in log-log space. Speciﬁcally,\nthe curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the\noptimal model size given the Gopher FLOP budget to be 40B parameters."
    },
    {
      "page_no": 7,
      "bbox": [
        62.36199951171875,
        387.29803466796875,
        533.1829223632812,
        440.0428466796875
      ],
      "text": "Eﬃcient frontier.\nWe can approximate the functions 𝑁𝑜𝑝𝑡and 𝐷𝑜𝑝𝑡by minimizing the parametric\nloss ˆ𝐿under the constraint FLOPs(𝑁, 𝐷) ≈6𝑁𝐷(Kaplan et al., 2020). The resulting 𝑁𝑜𝑝𝑡and 𝐷𝑜𝑝𝑡\nbalance the two terms in Equation (3) that depend on model size and data. By construction, they\nhave a power-law form:"
    },
    {
      "page_no": 7,
      "bbox": [
        72.72000122070312,
        454.4190673828125,
        137.98997497558594,
        473.9721374511719
      ],
      "text": "𝑁𝑜𝑝𝑡(𝐶) = 𝐺\n\u0012𝐶"
    },
    {
      "page_no": 7,
      "bbox": [
        132.24200439453125,
        470.4757080078125,
        138.30746459960938,
        481.38482666015625
      ],
      "text": "6"
    },
    {
      "page_no": 7,
      "bbox": [
        139.92601013183594,
        451.5451965332031,
        240.64898681640625,
        473.97210693359375
      ],
      "text": "\u0013𝑎\n,\n𝐷𝑜𝑝𝑡(𝐶) = 𝐺−1\n\u0012𝐶"
    },
    {
      "page_no": 7,
      "bbox": [
        234.89999389648438,
        470.4757080078125,
        240.9654541015625,
        481.38482666015625
      ],
      "text": "6"
    },
    {
      "page_no": 7,
      "bbox": [
        242.58399963378906,
        451.5451965332031,
        352.7104187011719,
        473.97210693359375
      ],
      "text": "\u0013𝑏\n,\nwhere\n𝐺=\n\u0012𝛼𝐴"
    },
    {
      "page_no": 7,
      "bbox": [
        340.37799072265625,
        469.28802490234375,
        352.62237548828125,
        479.32440185546875
      ],
      "text": "𝛽𝐵"
    },
    {
      "page_no": 7,
      "bbox": [
        354.2359924316406,
        450.7872619628906,
        532.91357421875,
        481.13336181640625
      ],
      "text": "\u0013\n1\n𝛼+𝛽\n,\n𝑎=\n𝛽\n𝛼+ 𝛽, and 𝑏=\n𝛼\n𝛼+ 𝛽.\n(4)"
    },
    {
      "page_no": 7,
      "bbox": [
        61.86000061035156,
        492.71575927734375,
        534.5431518554688,
        518.817138671875
      ],
      "text": "We show contours of the ﬁtted function ˆ𝐿in Figure 4 (left), and the closed-form eﬃcient computational\nfrontier in blue. From this approach, we ﬁnd that 𝑎= 0.46 and 𝑏= 0.54—as summarized in Table 2."
    },
    {
      "page_no": 7,
      "bbox": [
        62.36199951171875,
        541.3335571289062,
        200.14393615722656,
        552.24267578125
      ],
      "text": "3.4. Optimal model scaling"
    },
    {
      "page_no": 7,
      "bbox": [
        61.457000732421875,
        563.3607177734375,
        534.739990234375,
        725.1598510742188
      ],
      "text": "We ﬁnd that the three approaches, despite using diﬀerent ﬁtting methodologies and diﬀerent trained\nmodels, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs\n(shown in Table 2). All three approaches suggest that as compute budget increases, model size and\nthe amount of training data should be increased in approximately equal proportions. The ﬁrst and\nsecond approaches yield very similar predictions for optimal model sizes, as shown in Figure 1 and\nFigure A3. The third approach predicts even smaller models being optimal at larger compute budgets.\nWe note that the observed points (𝐿, 𝑁, 𝐷) for low training FLOPs (𝐶⩽1𝑒21) have larger residuals\n∥𝐿−ˆ𝐿(𝑁, 𝐷)∥\n2\n2 than points with higher computational budgets. The ﬁtted model places increased\nweight on the points with more FLOPs—automatically considering the low-computational budget\npoints as outliers due to the Huber loss. As a consequence of the empirically observed negative\ncurvature in the frontier 𝐶→𝑁𝑜𝑝𝑡(see Appendix E), this results in predicting a lower 𝑁𝑜𝑝𝑡than the\ntwo other approaches."
    },
    {
      "page_no": 7,
      "bbox": [
        62.36199951171875,
        734.57373046875,
        532.9144287109375,
        759.0328369140625
      ],
      "text": "In Table 3 we show the estimated number of FLOPs and tokens that would ensure that a model of\na given size lies on the compute-optimal frontier. Our ﬁndings suggests that the current generation of"
    },
    {
      "page_no": 7,
      "bbox": [
        294.6050109863281,
        778.01171875,
        300.67047119140625,
        788.9208374023438
      ],
      "text": "7"
    },
    {
      "page_no": 8,
      "bbox": [
        62.03499984741211,
        85.54524993896484,
        533.1016235351562,
        150.90283203125
      ],
      "text": "Table 2 | Estimated parameter and data scaling with increased training compute. The listed\nvalues are the exponents, 𝑎and 𝑏, on the relationship 𝑁𝑜𝑝𝑡∝𝐶𝑎and 𝐷𝑜𝑝𝑡∝𝐶𝑏. Our analysis suggests\na near equal scaling in parameters and data with increasing compute which is in clear contrast\nto previous work on the scaling of large models. The 10th and 90th percentiles are estimated via\nbootstrapping data (80% of the dataset is sampled 100 times) and are shown in parenthesis."
    },
    {
      "page_no": 8,
      "bbox": [
        86.96800231933594,
        164.94918823242188,
        504.78057861328125,
        178.3348388671875
      ],
      "text": "Approach\nCoeﬀ. 𝑎where 𝑁𝑜𝑝𝑡∝𝐶𝑎\nCoeﬀ. 𝑏where 𝐷𝑜𝑝𝑡∝𝐶𝑏"
    },
    {
      "page_no": 8,
      "bbox": [
        86.9677734375,
        185.25999450683594,
        493.83660888671875,
        224.454833984375
      ],
      "text": "1. Minimum over training curves\n0.50 (0.488, 0.502)\n0.50 (0.501, 0.512)\n2. IsoFLOP proﬁles\n0.49 (0.462, 0.534)\n0.51 (0.483, 0.529)\n3. Parametric modelling of the loss\n0.46 (0.454, 0.455)\n0.54 (0.542, 0.543)"
    },
    {
      "page_no": 8,
      "bbox": [
        86.96800231933594,
        232.56776428222656,
        458.6409912109375,
        243.47686767578125
      ],
      "text": "Kaplan et al. (2020)\n0.73\n0.27"
    },
    {
      "page_no": 8,
      "bbox": [
        62.03499984741211,
        260.5672912597656,
        533.117431640625,
        312.37481689453125
      ],
      "text": "Table 3 | Estimated optimal training FLOPs and training tokens for various model sizes. For\nvarious model sizes, we show the projections from Approach 1 of how many FLOPs and training\ntokens would be needed to train compute-optimal models. The estimates for Approach 2 & 3 are\nsimilar (shown in Section D.3)"
    },
    {
      "page_no": 8,
      "bbox": [
        132.4639892578125,
        392.11273193359375,
        135.49671936035156,
        403.0218200683594
      ],
      "text": "."
    },
    {
      "page_no": 8,
      "bbox": [
        146.72500610351562,
        328.897705078125,
        456.8353271484375,
        340.15142822265625
      ],
      "text": "Parameters\nFLOPs\nFLOPs (in Gopher unit)\nTokens"
    },
    {
      "page_no": 8,
      "bbox": [
        145.01199340820312,
        346.7320251464844,
        456.83404541015625,
        467.2228698730469
      ],
      "text": "400 Million\n1.92e+19\n1/29, 968\n8.0 Billion\n1 Billion\n1.21e+20\n1/4, 761\n20.2 Billion\n10 Billion\n1.23e+22\n1/46\n205.1 Billion\n67 Billion\n5.76e+23\n1\n1.5 Trillion\n175 Billion\n3.85e+24\n6.7\n3.7 Trillion\n280 Billion\n9.90e+24\n17.2\n5.9 Trillion\n520 Billion\n3.43e+25\n59.5\n11.0 Trillion\n1 Trillion\n1.27e+26\n221.3\n21.2 Trillion\n10 Trillion\n1.30e+28\n22515.9\n216.2 Trillion"
    },
    {
      "page_no": 8,
      "bbox": [
        62.05699920654297,
        494.00872802734375,
        534.4290771484375,
        653.9588012695312
      ],
      "text": "large language models are considerably over-sized, given their respective compute budgets, as shown\nin Figure 1. For example, we ﬁnd that a 175 billion parameter model should be trained with a compute\nbudget of 4.41 × 1024 FLOPs and on over 4.2 trillion tokens. A 280 billion Gopher-like model is the\noptimal model to train given a compute budget of approximately 1025 FLOPs and should be trained on\n6.8 trillion tokens. Unless one has a compute budget of 1026 FLOPs (over 250× the compute used to\ntrain Gopher), a 1 trillion parameter model is unlikely to be the optimal model to train. Furthermore,\nthe amount of training data that is projected to be needed is far beyond what is currently used to\ntrain large models, and underscores the importance of dataset collection in addition to engineering\nimprovements that allow for model scale. While there is signiﬁcant uncertainty extrapolating out\nmany orders of magnitude, our analysis clearly suggests that given the training compute budget for\nmany current LLMs, smaller models should have been trained on more tokens to achieve the most\nperformant model."
    },
    {
      "page_no": 8,
      "bbox": [
        62.05699920654297,
        663.3737182617188,
        534.4268188476562,
        701.380859375
      ],
      "text": "In Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C4 (Raﬀel et al.,\n2020a) and GitHub code (Rae et al., 2021). In both cases we reach the similar conclusion that model\nsize and number of training tokens should be scaled in equal proportions."
    },
    {
      "page_no": 8,
      "bbox": [
        294.60498046875,
        778.01171875,
        300.6704406738281,
        788.9208374023438
      ],
      "text": "8"
    },
    {
      "page_no": 9,
      "bbox": [
        62.36199951171875,
        85.93616485595703,
        140.65380859375,
        98.93342590332031
      ],
      "text": "4. Chinchilla"
    },
    {
      "page_no": 9,
      "bbox": [
        62.36199951171875,
        112.57673645019531,
        533.1854248046875,
        191.2318115234375
      ],
      "text": "Based on our analysis in Section 3, the optimal model size for the Gopher compute budget is somewhere\nbetween 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end\nof this range—70B parameters—for 1.4T tokens, due to both dataset and computational eﬃciency\nconsiderations. In this section we compare this model, which we call Chinchilla, to Gopher and other\nLLMs. Both Chinchilla and Gopher have been trained for the same number of FLOPs but diﬀer in the\nsize of the model and the number of training tokens."
    },
    {
      "page_no": 9,
      "bbox": [
        62.361968994140625,
        200.6457061767578,
        534.6498413085938,
        238.9984893798828
      ],
      "text": "While pre-training a large language model has a considerable compute cost, downstream ﬁne-\ntuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being 4×\nsmaller than Gopher, both the memory footprint and inference cost of Chinchilla are also smaller."
    },
    {
      "page_no": 9,
      "bbox": [
        62.361968994140625,
        261.4766540527344,
        220.91481018066406,
        272.3857421875
      ],
      "text": "4.1. Model and training details"
    },
    {
      "page_no": 9,
      "bbox": [
        62.03499984741211,
        283.50274658203125,
        532.9124755859375,
        308.305419921875
      ],
      "text": "The full set of hyperparameters used to train Chinchilla are given in Table 4. Chinchilla uses the same\nmodel architecture and training setup as Gopher with the exception of the diﬀerences listed below."
    },
    {
      "page_no": 9,
      "bbox": [
        77.74400329589844,
        326.34173583984375,
        534.4757690429688,
        391.4478454589844
      ],
      "text": "• We train Chinchilla on MassiveText (the same dataset as Gopher) but use a slightly diﬀerent\nsubset distribution (shown in Table A1) to account for the increased number of training tokens.\n• We use AdamW (Loshchilov and Hutter, 2019) for Chinchilla rather than Adam (Kingma and\nBa, 2014) as this improves the language modelling loss and the downstream task performance\nafter ﬁnetuning.8"
    },
    {
      "page_no": 9,
      "bbox": [
        77.7439956665039,
        394.08770751953125,
        533.8145751953125,
        486.2928466796875
      ],
      "text": "• We train Chinchilla with a slightly modiﬁed SentencePiece (Kudo and Richardson, 2018)\ntokenizer that does not apply NFKC normalisation. The vocabulary is very similar– 94.15% of\ntokens are the same as those used for training Gopher. We ﬁnd that this particularly helps with\nthe representation of mathematics and chemistry, for example.\n• Whilst the forward and backward pass are computed in bfloat16, we store a float32 copy\nof the weights in the distributed optimiser state (Rajbhandari et al., 2020). See Lessons Learned\nfrom Rae et al. (2021) for additional details."
    },
    {
      "page_no": 9,
      "bbox": [
        61.457000732421875,
        504.6737060546875,
        532.91796875,
        556.2297973632812
      ],
      "text": "In Appendix G we show the impact of the various optimiser related changes between Chinchilla\nand Gopher. All models in this analysis have been trained on TPUv3/TPUv4 (Jouppi et al., 2017) with\nJAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020). We include a Chinchilla model card\n(Mitchell et al., 2019) in Table A8."
    },
    {
      "page_no": 9,
      "bbox": [
        89.49600219726562,
        580.756591796875,
        521.93115234375,
        593.1408081054688
      ],
      "text": "Model\nLayers\nNumber Heads\nKey/Value Size\ndmodel\nMax LR\nBatch Size"
    },
    {
      "page_no": 9,
      "bbox": [
        70.864013671875,
        597.6961669921875,
        524.4117431640625,
        624.2924194335938
      ],
      "text": "Gopher 280B\n80\n128\n128\n16,384\n4 × 10−5\n3M →6M\nChinchilla 70B\n80\n64\n128\n8,192\n1 × 10−4\n1.5M →3M"
    },
    {
      "page_no": 9,
      "bbox": [
        62.03499984741211,
        639.0452880859375,
        534.7313842773438,
        691.1974487304688
      ],
      "text": "Table 4 | Chinchilla architecture details. We list the number of layers, the key/value size, the\nbottleneck activation size dmodel, the maximum learning rate, and the training batch size (# tokens).\nThe feed-forward size is always set to 4 × dmodel. Note that we double the batch size midway through\ntraining for both Chinchilla and Gopher."
    },
    {
      "page_no": 9,
      "bbox": [
        62.36199951171875,
        736.81884765625,
        532.909912109375,
        758.644287109375
      ],
      "text": "8Interestingly, a model trained with AdamW only passes the training performance of a model trained with Adam around\n80% of the way through the cosine cycle, though the ending performance is notably better– see Figure A7"
    },
    {
      "page_no": 9,
      "bbox": [
        294.60498046875,
        778.01171875,
        300.6704406738281,
        788.9208374023438
      ],
      "text": "9"
    },
    {
      "page_no": 10,
      "bbox": [
        196.2689971923828,
        89.71675109863281,
        289.7926940917969,
        100.6258544921875
      ],
      "text": "# Tasks\nExamples"
    },
    {
      "page_no": 10,
      "bbox": [
        68.33999633789062,
        108.4112777709961,
        529.9891967773438,
        187.39385986328125
      ],
      "text": "Language Modelling\n20\nWikiText-103, The Pile: PG-19, arXiv, FreeLaw, . . .\nReading Comprehension\n3\nRACE-m, RACE-h, LAMBADA\nQuestion Answering\n3\nNatural Questions, TriviaQA, TruthfulQA\nCommon Sense\n5\nHellaSwag, Winogrande, PIQA, SIQA, BoolQ\nMMLU\n57\nHigh School Chemistry, Astronomy, Clinical Knowledge, . . .\nBIG-bench\n62\nCausal Judgement, Epistemic Reasoning, Temporal Sequences, . . ."
    },
    {
      "page_no": 10,
      "bbox": [
        61.9370002746582,
        202.49130249023438,
        533.1156616210938,
        240.74981689453125
      ],
      "text": "Table 5 | All evaluation tasks. We evaluate Chinchilla on a collection of language modelling along\nwith downstream tasks. We evaluate on largely the same tasks as in Rae et al. (2021), to allow for\ndirect comparison."
    },
    {
      "page_no": 10,
      "bbox": [
        62.36199951171875,
        265.0795593261719,
        122.91842651367188,
        275.9886474609375
      ],
      "text": "4.2. Results"
    },
    {
      "page_no": 10,
      "bbox": [
        61.86000061035156,
        287.106689453125,
        534.7311401367188,
        352.21185302734375
      ],
      "text": "We perform an extensive evaluation of Chinchilla, comparing against various large language models.\nWe evaluate on a large subset of the tasks presented in Rae et al. (2021), shown in Table 5. As\nthe focus of this work is on optimal model scaling, we included a large representative subset, and\nintroduce a few new evaluations to allow for better comparison to other existing large models. The\nevaluation details for all tasks are the same as described in Rae et al. (2021)."
    },
    {
      "page_no": 10,
      "bbox": [
        62.36199951171875,
        372.37298583984375,
        196.7948455810547,
        383.2820739746094
      ],
      "text": "4.2.1. Language modelling"
    },
    {
      "page_no": 10,
      "bbox": [
        165.3863525390625,
        518.8541259765625,
        178.44480895996094,
        590.2752685546875
      ],
      "text": "pubmed_abstracts"
    },
    {
      "page_no": 10,
      "bbox": [
        181.44007873535156,
        518.7140502929688,
        194.49853515625,
        567.444580078125
      ],
      "text": "nih_exporter"
    },
    {
      "page_no": 10,
      "bbox": [
        197.49380493164062,
        518.6703491210938,
        210.55226135253906,
        593.76416015625
      ],
      "text": "uspto_backgrounds"
    },
    {
      "page_no": 10,
      "bbox": [
        213.5475311279297,
        518.844482421875,
        226.60598754882812,
        581.3726806640625
      ],
      "text": "pubmed_central"
    },
    {
      "page_no": 10,
      "bbox": [
        229.60125732421875,
        518.843017578125,
        242.6597137451172,
        545.0676879882812
      ],
      "text": "pile_cc"
    },
    {
      "page_no": 10,
      "bbox": [
        245.76162719726562,
        518.8558959960938,
        258.8200988769531,
        568.4179077148438
      ],
      "text": "bookcorpus2"
    },
    {
      "page_no": 10,
      "bbox": [
        261.8153381347656,
        518.3131103515625,
        274.8738098144531,
        576.3909301757812
      ],
      "text": "stackexchange"
    },
    {
      "page_no": 10,
      "bbox": [
        277.8690490722656,
        518.8471069335938,
        290.9275207519531,
        570.9115600585938
      ],
      "text": "opensubtitles"
    },
    {
      "page_no": 10,
      "bbox": [
        293.92279052734375,
        518.72119140625,
        306.98126220703125,
        574.0886840820312
      ],
      "text": "openwebtext2"
    },
    {
      "page_no": 10,
      "bbox": [
        309.97650146484375,
        518.4532470703125,
        323.03497314453125,
        564.5736083984375
      ],
      "text": "hackernews"
    },
    {
      "page_no": 10,
      "bbox": [
        325.923583984375,
        518.855712890625,
        338.9820556640625,
        585.5724487304688
      ],
      "text": "dm_mathematics"
    },
    {
      "page_no": 10,
      "bbox": [
        342.0839538574219,
        518.626220703125,
        355.1424255371094,
        537.767333984375
      ],
      "text": "arxiv"
    },
    {
      "page_no": 10,
      "bbox": [
        358.1376953125,
        518.6802368164062,
        371.1961669921875,
        547.1839599609375
      ],
      "text": "freelaw"
    },
    {
      "page_no": 10,
      "bbox": [
        374.19140625,
        518.8588256835938,
        387.2498779296875,
        546.5386962890625
      ],
      "text": "books3"
    },
    {
      "page_no": 10,
      "bbox": [
        390.2451171875,
        518.8463134765625,
        403.3035888671875,
        559.3074951171875
      ],
      "text": "philpapers"
    },
    {
      "page_no": 10,
      "bbox": [
        406.2988586425781,
        518.841796875,
        419.3573303222656,
        543.5419921875
      ],
      "text": "github"
    },
    {
      "page_no": 10,
      "bbox": [
        422.2459411621094,
        518.6594848632812,
        435.3044128417969,
        559.4824829101562
      ],
      "text": "ubuntu_irc"
    },
    {
      "page_no": 10,
      "bbox": [
        438.4062805175781,
        518.675537109375,
        451.4647521972656,
        551.0830078125
      ],
      "text": "europarl"
    },
    {
      "page_no": 10,
      "bbox": [
        454.3533935546875,
        518.7197875976562,
        467.411865234375,
        586.0755004882812
      ],
      "text": "gutenberg_pg_19"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        508.9592590332031,
        148.01547241210938,
        522.0177001953125
      ],
      "text": "0.00"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        488.72686767578125,
        148.01547241210938,
        501.78533935546875
      ],
      "text": "0.02"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        468.4944763183594,
        148.01547241210938,
        481.5529479980469
      ],
      "text": "0.04"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        448.2620544433594,
        148.01547241210938,
        461.3205261230469
      ],
      "text": "0.06"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        428.0296630859375,
        148.01547241210938,
        441.088134765625
      ],
      "text": "0.08"
    },
    {
      "page_no": 10,
      "bbox": [
        130.87623596191406,
        407.7972717285156,
        148.01547241210938,
        420.8557434082031
      ],
      "text": "0.10"
    },
    {
      "page_no": 10,
      "bbox": [
        107.9619140625,
        422.74456787109375,
        122.2074966430664,
        493.98089599609375
      ],
      "text": "Decrease in bpb"
    },
    {
      "page_no": 10,
      "bbox": [
        117.36981964111328,
        413.48736572265625,
        131.6154022216797,
        503.2362365722656
      ],
      "text": "compared to Gopher"
    },
    {
      "page_no": 10,
      "bbox": [
        62.03499984741211,
        606.7522583007812,
        534.4246215820312,
        645.3564453125
      ],
      "text": "Figure 5 | Pile Evaluation. For the diﬀerent evaluation sets in The Pile (Gao et al., 2020), we show\nthe bits-per-byte (bpb) improvement (decrease) of Chinchilla compared to Gopher. On all subsets,\nChinchilla outperforms Gopher."
    },
    {
      "page_no": 10,
      "bbox": [
        62.05699920654297,
        659.606689453125,
        534.4276733398438,
        751.8108520507812
      ],
      "text": "Chinchilla signiﬁcantly outperforms Gopher on all evaluation subsets of The Pile (Gao et al.,\n2020), as shown in Figure 5. Compared to Jurassic-1 (178B) Lieber et al. (2021), Chinchilla is more\nperformant on all but two subsets– dm_mathematics and ubuntu_irc– see Table A5 for a raw\nbits-per-byte comparison. On Wikitext103 (Merity et al., 2017), Chinchilla achieves a perplexity of\n7.16 compared to 7.75 for Gopher. Some caution is needed when comparing Chinchilla with Gopher\non these language modelling benchmarks as Chinchilla is trained on 4× more data than Gopher and\nthus train/test set leakage may artiﬁcially enhance the results. We thus place more emphasis on other"
    },
    {
      "page_no": 10,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "10"
    },
    {
      "page_no": 11,
      "bbox": [
        190.71299743652344,
        89.71675109863281,
        404.56451416015625,
        168.7165069580078
      ],
      "text": "Random\n25.0%\nAverage human rater\n34.5%\nGPT-3 5-shot\n43.9%\nGopher 5-shot\n60.0%\nChinchilla 5-shot\n67.6%\nAverage human expert performance\n89.8%"
    },
    {
      "page_no": 11,
      "bbox": [
        190.71299743652344,
        176.48475646972656,
        403.89862060546875,
        200.94287109375
      ],
      "text": "June 2022 Forecast\n57.1%\nJune 2023 Forecast\n63.4%"
    },
    {
      "page_no": 11,
      "bbox": [
        61.457000732421875,
        216.04031372070312,
        534.7373657226562,
        267.84881591796875
      ],
      "text": "Table 6 | Massive Multitask Language Understanding (MMLU). We report the average 5-shot\naccuracy over 57 tasks with model and human accuracy comparisons taken from Hendrycks et al.\n(2020). We also include the average prediction for state of the art accuracy in June 2022/2023 made\nby 73 competitive human forecasters in Steinhardt (2021)."
    },
    {
      "page_no": 11,
      "bbox": [
        61.457000732421875,
        291.88873291015625,
        532.9153442382812,
        329.89581298828125
      ],
      "text": "tasks for which leakage is less of a concern, such as MMLU (Hendrycks et al., 2020) and BIG-bench\n(BIG-bench collaboration, 2021) along with various closed-book question answering and common\nsense analyses."
    },
    {
      "page_no": 11,
      "bbox": [
        62.36199951171875,
        350.0559387207031,
        129.18023681640625,
        360.96502685546875
      ],
      "text": "4.2.2. MMLU"
    },
    {
      "page_no": 11,
      "bbox": [
        62.03499984741211,
        372.12176513671875,
        536.064453125,
        491.423828125
      ],
      "text": "The Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020) consists\nof a range of exam-like questions on academic subjects. In Table 6, we report Chinchilla’s average\n5-shot performance on MMLU (the full breakdown of results is shown in Table A6). On this benchmark,\nChinchilla signiﬁcantly outperforms Gopher despite being much smaller, with an average accuracy of\n67.6% (improving upon Gopher by 7.6%). Remarkably, Chinchilla even outperforms the expert forecast\nfor June 2023 of 63.4% accuracy (see Table 6) (Steinhardt, 2021). Furthermore, Chinchilla achieves\ngreater than 90% accuracy on 4 diﬀerent individual tasks– high_school_gov_and_politics,\ninternational_law, sociology, and us_foreign_policy. To our knowledge, no other model\nhas achieved greater than 90% accuracy on a subset."
    },
    {
      "page_no": 11,
      "bbox": [
        62.36199951171875,
        500.8387451171875,
        536.06396484375,
        552.3958129882812
      ],
      "text": "In Figure 6, we show a comparison to Gopher broken down by task. Overall, we ﬁnd that Chin-\nchilla improves performance on the vast majority of tasks. On four tasks (college_mathematics,\neconometrics, moral_scenarios, and formal_logic) Chinchilla underperforms Gopher, and\nthere is no change in performance on two tasks."
    },
    {
      "page_no": 11,
      "bbox": [
        62.36199951171875,
        572.555908203125,
        214.2602996826172,
        583.4650268554688
      ],
      "text": "4.2.3. Reading comprehension"
    },
    {
      "page_no": 11,
      "bbox": [
        61.95800018310547,
        594.6207275390625,
        533.3817749023438,
        646.1778564453125
      ],
      "text": "On the ﬁnal word prediction dataset LAMBADA (Paperno et al., 2016), Chinchilla achieves 77.4%\naccuracy, compared to 74.5% accuracy from Gopher and 76.6% from MT-NLG 530B (see Table 7). On\nRACE-h and RACE-m (Lai et al., 2017), Chinchilla greatly outperforms Gopher, improving accuracy\nby more than 10% in both cases—see Table 7."
    },
    {
      "page_no": 11,
      "bbox": [
        62.36199951171875,
        666.3379516601562,
        147.23480224609375,
        677.2470703125
      ],
      "text": "4.2.4. BIG-bench"
    },
    {
      "page_no": 11,
      "bbox": [
        61.86000061035156,
        688.4036865234375,
        536.0647583007812,
        754.5718994140625
      ],
      "text": "We analysed Chinchilla on the same set of BIG-bench tasks (BIG-bench collaboration, 2021) reported\nin Rae et al. (2021). Similar to what we observed in MMLU, Chinchilla outperforms Gopher on the\nvast majority of tasks (see Figure 7). We ﬁnd that Chinchilla improves the average performance\nby 10.7%, reaching an accuracy of 65.1% versus 54.4% for Gopher. Of the 62 tasks we consider,\nChinchilla performs worse than Gopher on only four—crash_blossom, dark_humor_detection,"
    },
    {
      "page_no": 11,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "11"
    },
    {
      "page_no": 12,
      "bbox": [
        130.3051300048828,
        198.40773010253906,
        139.67230224609375,
        257.1679992675781
      ],
      "text": "college_mathematics"
    },
    {
      "page_no": 12,
      "bbox": [
        136.65948486328125,
        198.41165161132812,
        146.0266571044922,
        235.7698974609375
      ],
      "text": "econometrics"
    },
    {
      "page_no": 12,
      "bbox": [
        142.86083984375,
        198.41311645507812,
        152.22801208496094,
        243.3876953125
      ],
      "text": "moral_scenarios"
    },
    {
      "page_no": 12,
      "bbox": [
        149.13868713378906,
        198.3166046142578,
        158.505859375,
        231.96884155273438
      ],
      "text": "formal_logic"
    },
    {
      "page_no": 12,
      "bbox": [
        155.4165496826172,
        198.40399169921875,
        164.78372192382812,
        246.36106872558594
      ],
      "text": "medical_genetics"
    },
    {
      "page_no": 12,
      "bbox": [
        161.6944122314453,
        198.30239868164062,
        171.06158447265625,
        247.43588256835938
      ],
      "text": "machine_learning"
    },
    {
      "page_no": 12,
      "bbox": [
        167.97227478027344,
        198.27516174316406,
        177.33944702148438,
        241.70880126953125
      ],
      "text": "public_relations"
    },
    {
      "page_no": 12,
      "bbox": [
        174.25013732910156,
        198.40884399414062,
        183.6173095703125,
        231.43142700195312
      ],
      "text": "global_facts"
    },
    {
      "page_no": 12,
      "bbox": [
        180.52798461914062,
        198.40505981445312,
        189.89515686035156,
        241.75587463378906
      ],
      "text": "business_ethics"
    },
    {
      "page_no": 12,
      "bbox": [
        186.8058319091797,
        198.39439392089844,
        196.17300415039062,
        259.501953125
      ],
      "text": "electrical_engineering"
    },
    {
      "page_no": 12,
      "bbox": [
        193.0836944580078,
        198.40284729003906,
        202.45086669921875,
        271.1365661621094
      ],
      "text": "college_computer_science"
    },
    {
      "page_no": 12,
      "bbox": [
        199.36155700683594,
        198.28292846679688,
        208.72872924804688,
        239.79454040527344
      ],
      "text": "world_religions"
    },
    {
      "page_no": 12,
      "bbox": [
        205.63941955566406,
        198.3985137939453,
        215.006591796875,
        261.82025146484375
      ],
      "text": "high_school_us_history"
    },
    {
      "page_no": 12,
      "bbox": [
        211.9172821044922,
        198.39488220214844,
        221.28445434570312,
        264.94268798828125
      ],
      "text": "high_school_psychology"
    },
    {
      "page_no": 12,
      "bbox": [
        218.27162170410156,
        198.40977478027344,
        227.6387939453125,
        235.4090118408203
      ],
      "text": "management"
    },
    {
      "page_no": 12,
      "bbox": [
        224.4729766845703,
        198.39720153808594,
        233.84014892578125,
        283.8616638183594
      ],
      "text": "high_school_computer_science"
    },
    {
      "page_no": 12,
      "bbox": [
        230.8273162841797,
        198.22047424316406,
        240.19448852539062,
        226.55950927734375
      ],
      "text": "marketing"
    },
    {
      "page_no": 12,
      "bbox": [
        237.02870178222656,
        198.39659118652344,
        246.3958740234375,
        254.2848358154297
      ],
      "text": "high_school_physics"
    },
    {
      "page_no": 12,
      "bbox": [
        243.3065643310547,
        198.27813720703125,
        252.67373657226562,
        280.5391845703125
      ],
      "text": "high_school_macroeconomics"
    },
    {
      "page_no": 12,
      "bbox": [
        249.66090393066406,
        198.4084930419922,
        259.028076171875,
        224.3118133544922
      ],
      "text": "sociology"
    },
    {
      "page_no": 12,
      "bbox": [
        255.86228942871094,
        198.29824829101562,
        265.2294616699219,
        302.8394775390625
      ],
      "text": "high_school_government_and_politics"
    },
    {
      "page_no": 12,
      "bbox": [
        262.14013671875,
        198.27098083496094,
        271.5072937011719,
        281.6531982421875
      ],
      "text": "high_school_european_history"
    },
    {
      "page_no": 12,
      "bbox": [
        268.4945068359375,
        198.40220642089844,
        277.8616638183594,
        221.95822143554688
      ],
      "text": "nutrition"
    },
    {
      "page_no": 12,
      "bbox": [
        274.69586181640625,
        198.4022979736328,
        284.0630187988281,
        246.24339294433594
      ],
      "text": "college_medicine"
    },
    {
      "page_no": 12,
      "bbox": [
        281.0501708984375,
        198.2877960205078,
        290.4173583984375,
        227.89712524414062
      ],
      "text": "astronomy"
    },
    {
      "page_no": 12,
      "bbox": [
        287.25152587890625,
        198.40057373046875,
        296.61871337890625,
        241.7127227783203
      ],
      "text": "logical_fallacies"
    },
    {
      "page_no": 12,
      "bbox": [
        293.5294189453125,
        198.280517578125,
        302.8965759277344,
        265.8998107910156
      ],
      "text": "professional_psychology"
    },
    {
      "page_no": 12,
      "bbox": [
        299.8837890625,
        198.4058837890625,
        309.2509460449219,
        237.74691772460938
      ],
      "text": "miscellaneous"
    },
    {
      "page_no": 12,
      "bbox": [
        306.16162109375,
        198.40185546875,
        315.52880859375,
        236.24061584472656
      ],
      "text": "jurisprudence"
    },
    {
      "page_no": 12,
      "bbox": [
        312.36297607421875,
        198.39466857910156,
        321.73016357421875,
        250.20135498046875
      ],
      "text": "clinical_knowledge"
    },
    {
      "page_no": 12,
      "bbox": [
        318.640869140625,
        198.3981475830078,
        328.0080261230469,
        263.4049987792969
      ],
      "text": "high_school_geography"
    },
    {
      "page_no": 12,
      "bbox": [
        324.918701171875,
        198.3978271484375,
        334.285888671875,
        253.7945098876953
      ],
      "text": "high_school_biology"
    },
    {
      "page_no": 12,
      "bbox": [
        331.196533203125,
        198.4034881591797,
        340.563720703125,
        241.0694122314453
      ],
      "text": "college_biology"
    },
    {
      "page_no": 12,
      "bbox": [
        337.47442626953125,
        198.40504455566406,
        346.8415832519531,
        248.3890838623047
      ],
      "text": "college_chemistry"
    },
    {
      "page_no": 12,
      "bbox": [
        343.75225830078125,
        198.40090942382812,
        353.11944580078125,
        270.6540832519531
      ],
      "text": "high_school_world_history"
    },
    {
      "page_no": 12,
      "bbox": [
        350.03009033203125,
        198.27757263183594,
        359.39727783203125,
        245.98057556152344
      ],
      "text": "us_foreign_policy"
    },
    {
      "page_no": 12,
      "bbox": [
        356.38446044921875,
        198.28077697753906,
        365.75164794921875,
        220.4283905029297
      ],
      "text": "virology"
    },
    {
      "page_no": 12,
      "bbox": [
        362.662353515625,
        198.4032745361328,
        372.0295104980469,
        228.39923095703125
      ],
      "text": "philosophy"
    },
    {
      "page_no": 12,
      "bbox": [
        368.86370849609375,
        198.41229248046875,
        378.2308654785156,
        240.49276733398438
      ],
      "text": "moral_disputes"
    },
    {
      "page_no": 12,
      "bbox": [
        375.14154052734375,
        198.39926147460938,
        384.50872802734375,
        235.86795043945312
      ],
      "text": "human_aging"
    },
    {
      "page_no": 12,
      "bbox": [
        381.41937255859375,
        198.40689086914062,
        390.78656005859375,
        249.86007690429688
      ],
      "text": "computer_security"
    },
    {
      "page_no": 12,
      "bbox": [
        387.697265625,
        198.4053955078125,
        397.064453125,
        243.08172607421875
      ],
      "text": "security_studies"
    },
    {
      "page_no": 12,
      "bbox": [
        393.97509765625,
        198.30484008789062,
        403.34228515625,
        245.76483154296875
      ],
      "text": "international_law"
    },
    {
      "page_no": 12,
      "bbox": [
        400.25299072265625,
        198.27688598632812,
        409.6201477050781,
        278.68768310546875
      ],
      "text": "high_school_microeconomics"
    },
    {
      "page_no": 12,
      "bbox": [
        406.53082275390625,
        198.40203857421875,
        415.89801025390625,
        258.3055419921875
      ],
      "text": "high_school_statistics"
    },
    {
      "page_no": 12,
      "bbox": [
        412.8087158203125,
        198.275146484375,
        422.1758728027344,
        265.1819763183594
      ],
      "text": "professional_accounting"
    },
    {
      "page_no": 12,
      "bbox": [
        419.0865478515625,
        198.28228759765625,
        428.4537353515625,
        259.9256286621094
      ],
      "text": "professional_medicine"
    },
    {
      "page_no": 12,
      "bbox": [
        425.44085693359375,
        198.28419494628906,
        434.80804443359375,
        226.45751953125
      ],
      "text": "prehistory"
    },
    {
      "page_no": 12,
      "bbox": [
        431.64227294921875,
        198.39939880371094,
        441.0094299316406,
        261.11419677734375
      ],
      "text": "high_school_chemistry"
    },
    {
      "page_no": 12,
      "bbox": [
        437.92010498046875,
        198.40911865234375,
        447.28729248046875,
        269.0771789550781
      ],
      "text": "elementary_mathematics"
    },
    {
      "page_no": 12,
      "bbox": [
        444.197998046875,
        198.40968322753906,
        453.5651550292969,
        244.9528350830078
      ],
      "text": "abstract_algebra"
    },
    {
      "page_no": 12,
      "bbox": [
        450.55230712890625,
        198.40879821777344,
        459.91949462890625,
        222.87612915039062
      ],
      "text": "anatomy"
    },
    {
      "page_no": 12,
      "bbox": [
        456.75372314453125,
        198.28607177734375,
        466.1208801269531,
        244.0780792236328
      ],
      "text": "professional_law"
    },
    {
      "page_no": 12,
      "bbox": [
        463.03155517578125,
        198.30213928222656,
        472.3987121582031,
        245.27056884765625
      ],
      "text": "human_sexuality"
    },
    {
      "page_no": 12,
      "bbox": [
        469.3094482421875,
        198.40225219726562,
        478.6766052246094,
        241.5597381591797
      ],
      "text": "college_physics"
    },
    {
      "page_no": 12,
      "bbox": [
        475.5872802734375,
        198.40208435058594,
        484.9544372558594,
        269.8930969238281
      ],
      "text": "high_school_mathematics"
    },
    {
      "page_no": 12,
      "bbox": [
        481.8651123046875,
        198.39659118652344,
        491.2322692871094,
        252.2136688232422
      ],
      "text": "conceptual_physics"
    },
    {
      "page_no": 12,
      "bbox": [
        105.91204071044922,
        174.9449005126953,
        112.93742370605469,
        184.31207275390625
      ],
      "text": "10"
    },
    {
      "page_no": 12,
      "bbox": [
        109.42771911621094,
        155.9337158203125,
        112.94041442871094,
        165.30088806152344
      ],
      "text": "0"
    },
    {
      "page_no": 12,
      "bbox": [
        105.91302490234375,
        136.92254638671875,
        112.93840789794922,
        146.2897186279297
      ],
      "text": "10"
    },
    {
      "page_no": 12,
      "bbox": [
        105.91302490234375,
        117.91136169433594,
        112.93840789794922,
        127.2785415649414
      ],
      "text": "20"
    },
    {
      "page_no": 12,
      "bbox": [
        105.91302490234375,
        98.90019226074219,
        112.93840789794922,
        108.26737213134766
      ],
      "text": "30"
    },
    {
      "page_no": 12,
      "bbox": [
        84.84722900390625,
        106.697509765625,
        95.06596374511719,
        175.76438903808594
      ],
      "text": "Relative Improvement"
    },
    {
      "page_no": 12,
      "bbox": [
        91.59576416015625,
        121.71125030517578,
        101.81449890136719,
        161.16427612304688
      ],
      "text": "over Gopher"
    },
    {
      "page_no": 12,
      "bbox": [
        62.05699920654297,
        315.1152648925781,
        533.3853759765625,
        353.3748474121094
      ],
      "text": "Figure 6 | MMLU results compared to Gopher We ﬁnd that Chinchilla outperforms Gopher by 7.6%\non average (see Table 6) in addition to performing better on 51/57 individual tasks, the same on\n2/57, and worse on only 4/57 tasks."
    },
    {
      "page_no": 12,
      "bbox": [
        247.61000061035156,
        371.88970947265625,
        459.1676940917969,
        383.1434326171875
      ],
      "text": "Chinchilla\nGopher\nGPT-3\nMT-NLG 530B"
    },
    {
      "page_no": 12,
      "bbox": [
        136.11000061035156,
        390.9117431640625,
        436.292236328125,
        429.2087097167969
      ],
      "text": "LAMBADA Zero-Shot\n77.4\n74.5\n76.2\n76.6\nRACE-m Few-Shot\n86.8\n75.1\n58.1\n-\nRACE-h Few-Shot\n82.3\n71.6\n46.8\n47.9"
    },
    {
      "page_no": 12,
      "bbox": [
        61.457000732421875,
        444.01629638671875,
        533.2645263671875,
        496.16943359375
      ],
      "text": "Table 7 | Reading comprehension. On RACE-h and RACE-m (Lai et al., 2017), Chinchilla considerably\nimproves performance over Gopher. Note that GPT-3 and MT-NLG 530B use a diﬀerent prompt format\nthan we do on RACE-h/m, so results are not comparable to Gopher and Chinchilla. On LAMBADA\n(Paperno et al., 2016), Chinchilla outperforms both Gopher and MT-NLG 530B."
    },
    {
      "page_no": 12,
      "bbox": [
        62.36199951171875,
        519.8367919921875,
        532.9090576171875,
        544.3228149414062
      ],
      "text": "mathematical_induction and logical_args. Full accuracy results for Chinchilla can be found\nin Table A7."
    },
    {
      "page_no": 12,
      "bbox": [
        62.36199951171875,
        564.48291015625,
        169.42388916015625,
        575.3920288085938
      ],
      "text": "4.2.5. Common sense"
    },
    {
      "page_no": 12,
      "bbox": [
        61.457000732421875,
        586.5487060546875,
        533.4688720703125,
        638.1048583984375
      ],
      "text": "We evaluate Chinchilla on various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap\net al., 2019), Winogrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ\n(Clark et al., 2019). We ﬁnd that Chinchilla outperforms both Gopher and GPT-3 on all tasks and\noutperforms MT-NLG 530B on all but one task—see Table 8."
    },
    {
      "page_no": 12,
      "bbox": [
        61.457000732421875,
        647.5197143554688,
        533.3795776367188,
        712.6258544921875
      ],
      "text": "On TruthfulQA (Lin et al., 2021), Chinchilla reaches 43.6%, 58.5%, and 66.7% accuracy with\n0-shot, 5-shot, and 10-shot respectively. In comparison, Gopher achieved only 29.5% 0-shot and 43.7%\n10-shot accuracy. In stark contrast with the ﬁndings of Lin et al. (2021), the large improvements\n(14.1% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training\ndata alone can lead to substantial improvements on this benchmark."
    },
    {
      "page_no": 12,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "12"
    },
    {
      "page_no": 13,
      "bbox": [
        130.10195922851562,
        198.4152374267578,
        139.46913146972656,
        239.186767578125
      ],
      "text": "crash_blossom"
    },
    {
      "page_no": 13,
      "bbox": [
        135.8719024658203,
        198.40602111816406,
        145.23907470703125,
        260.4083251953125
      ],
      "text": "dark_humor_detection"
    },
    {
      "page_no": 13,
      "bbox": [
        141.641845703125,
        198.39590454101562,
        151.00901794433594,
        265.0762939453125
      ],
      "text": "mathematical_induction"
    },
    {
      "page_no": 13,
      "bbox": [
        147.4117889404297,
        198.31272888183594,
        156.77896118164062,
        231.03155517578125
      ],
      "text": "logical_args"
    },
    {
      "page_no": 13,
      "bbox": [
        153.1817169189453,
        198.4031524658203,
        162.54888916015625,
        266.155029296875
      ],
      "text": "general_knowledge_json"
    },
    {
      "page_no": 13,
      "bbox": [
        158.95166015625,
        198.3069305419922,
        168.31883239746094,
        307.5704345703125
      ],
      "text": "Human_organs_senses_multiple_choice"
    },
    {
      "page_no": 13,
      "bbox": [
        164.7216033935547,
        198.31234741210938,
        174.08877563476562,
        300.2742919921875
      ],
      "text": "formal_fallacies_syllogisms_negation"
    },
    {
      "page_no": 13,
      "bbox": [
        170.4915313720703,
        198.3992156982422,
        179.85870361328125,
        247.23997497558594
      ],
      "text": "known_unknowns"
    },
    {
      "page_no": 13,
      "bbox": [
        176.3379669189453,
        198.40379333496094,
        185.70513916015625,
        222.55078125
      ],
      "text": "navigate"
    },
    {
      "page_no": 13,
      "bbox": [
        182.0314178466797,
        198.4003448486328,
        191.39859008789062,
        254.22230529785156
      ],
      "text": "sentence_ambiguity"
    },
    {
      "page_no": 13,
      "bbox": [
        187.80136108398438,
        198.3158721923828,
        197.1685333251953,
        253.95556640625
      ],
      "text": "moral_permissibility"
    },
    {
      "page_no": 13,
      "bbox": [
        193.57130432128906,
        198.27281188964844,
        202.9384765625,
        248.51092529296875
      ],
      "text": "intent_recognition"
    },
    {
      "page_no": 13,
      "bbox": [
        199.3412322998047,
        198.27056884765625,
        208.70840454101562,
        251.11557006835938
      ],
      "text": "irony_identification"
    },
    {
      "page_no": 13,
      "bbox": [
        205.11117553710938,
        198.40147399902344,
        214.4783477783203,
        244.6353302001953
      ],
      "text": "entailed_polarity"
    },
    {
      "page_no": 13,
      "bbox": [
        210.9575958251953,
        198.40533447265625,
        220.32476806640625,
        230.29017639160156
      ],
      "text": "hyperbaton"
    },
    {
      "page_no": 13,
      "bbox": [
        216.72755432128906,
        198.40744018554688,
        226.0947265625,
        241.520751953125
      ],
      "text": "misconceptions"
    },
    {
      "page_no": 13,
      "bbox": [
        222.42100524902344,
        198.29849243164062,
        231.78817749023438,
        296.626220703125
      ],
      "text": "evaluating_information_essentiality"
    },
    {
      "page_no": 13,
      "bbox": [
        228.19093322753906,
        198.4072265625,
        237.55810546875,
        262.40887451171875
      ],
      "text": "similarities_abstraction"
    },
    {
      "page_no": 13,
      "bbox": [
        233.9608612060547,
        198.28175354003906,
        243.32803344726562,
        255.23043823242188
      ],
      "text": "epistemic_reasoning"
    },
    {
      "page_no": 13,
      "bbox": [
        239.73081970214844,
        198.2799072265625,
        249.09799194335938,
        248.92672729492188
      ],
      "text": "fantasy_reasoning"
    },
    {
      "page_no": 13,
      "bbox": [
        245.50074768066406,
        198.2891082763672,
        254.867919921875,
        287.4981994628906
      ],
      "text": "movie_dialog_same_or_different"
    },
    {
      "page_no": 13,
      "bbox": [
        251.34718322753906,
        198.40492248535156,
        260.71435546875,
        222.62924194335938
      ],
      "text": "winowhy"
    },
    {
      "page_no": 13,
      "bbox": [
        257.0406494140625,
        198.40322875976562,
        266.4078063964844,
        241.15200805664062
      ],
      "text": "novel_concepts"
    },
    {
      "page_no": 13,
      "bbox": [
        262.81060791015625,
        198.0955352783203,
        272.1777648925781,
        277.7386474609375
      ],
      "text": "discourse_marker_prediction"
    },
    {
      "page_no": 13,
      "bbox": [
        268.65704345703125,
        198.4109344482422,
        278.0242004394531,
        228.3406219482422
      ],
      "text": "strategyqa"
    },
    {
      "page_no": 13,
      "bbox": [
        274.3504638671875,
        198.4011688232422,
        283.7176208496094,
        245.38064575195312
      ],
      "text": "causal_judgment"
    },
    {
      "page_no": 13,
      "bbox": [
        280.12042236328125,
        198.39784240722656,
        289.4875793457031,
        246.6476593017578
      ],
      "text": "hindu_knowledge"
    },
    {
      "page_no": 13,
      "bbox": [
        285.89031982421875,
        198.285888671875,
        295.25750732421875,
        252.29627990722656
      ],
      "text": "phrase_relatedness"
    },
    {
      "page_no": 13,
      "bbox": [
        291.6602783203125,
        198.2718963623047,
        301.0274658203125,
        266.89642333984375
      ],
      "text": "alignment_questionnaire"
    },
    {
      "page_no": 13,
      "bbox": [
        297.43023681640625,
        198.15484619140625,
        306.7973937988281,
        290.1420593261719
      ],
      "text": "reasoning_about_colored_objects"
    },
    {
      "page_no": 13,
      "bbox": [
        303.20013427734375,
        198.40052795410156,
        312.56732177734375,
        253.77511596679688
      ],
      "text": "date_understanding"
    },
    {
      "page_no": 13,
      "bbox": [
        308.9700927734375,
        198.3960418701172,
        318.3372802734375,
        254.42236328125
      ],
      "text": "penguins_in_a_table"
    },
    {
      "page_no": 13,
      "bbox": [
        314.74005126953125,
        198.28030395507812,
        324.1072082519531,
        273.84735107421875
      ],
      "text": "figure_of_speech_detection"
    },
    {
      "page_no": 13,
      "bbox": [
        320.50994873046875,
        198.40396118164062,
        329.87713623046875,
        247.3772735595703
      ],
      "text": "disambiguation_q"
    },
    {
      "page_no": 13,
      "bbox": [
        326.35638427734375,
        198.28118896484375,
        335.72357177734375,
        232.40841674804688
      ],
      "text": "implicatures"
    },
    {
      "page_no": 13,
      "bbox": [
        332.1263427734375,
        198.4043731689453,
        341.4935302734375,
        220.7895050048828
      ],
      "text": "SNARKS"
    },
    {
      "page_no": 13,
      "bbox": [
        337.81976318359375,
        198.40640258789062,
        347.18695068359375,
        230.5177001953125
      ],
      "text": "ruin_names"
    },
    {
      "page_no": 13,
      "bbox": [
        343.5897216796875,
        198.39450073242188,
        352.9569091796875,
        266.0138244628906
      ],
      "text": "logical_fallacy_detection"
    },
    {
      "page_no": 13,
      "bbox": [
        349.4361572265625,
        198.28146362304688,
        358.8033447265625,
        236.91554260253906
      ],
      "text": "anachronisms"
    },
    {
      "page_no": 13,
      "bbox": [
        355.12957763671875,
        198.39979553222656,
        364.49676513671875,
        245.47869873046875
      ],
      "text": "logic_grid_puzzle"
    },
    {
      "page_no": 13,
      "bbox": [
        360.8995361328125,
        198.4097137451172,
        370.2667236328125,
        232.9732666015625
      ],
      "text": "riddle_sense"
    },
    {
      "page_no": 13,
      "bbox": [
        366.66949462890625,
        198.3950653076172,
        376.0366516113281,
        252.9356689453125
      ],
      "text": "analytic_entailment"
    },
    {
      "page_no": 13,
      "bbox": [
        372.439453125,
        198.4031982421875,
        381.8066101074219,
        249.85638427734375
      ],
      "text": "question_selection"
    },
    {
      "page_no": 13,
      "bbox": [
        378.2093505859375,
        198.32479858398438,
        387.5765380859375,
        272.4116516113281
      ],
      "text": "nonsense_words_grammar"
    },
    {
      "page_no": 13,
      "bbox": [
        383.97930908203125,
        198.4066619873047,
        393.3464660644531,
        230.19210815429688
      ],
      "text": "physics_mc"
    },
    {
      "page_no": 13,
      "bbox": [
        389.749267578125,
        198.40447998046875,
        399.1164245605469,
        256.1208801269531
      ],
      "text": "empirical_judgments"
    },
    {
      "page_no": 13,
      "bbox": [
        395.51922607421875,
        198.40643310546875,
        404.8863830566406,
        258.40386962890625
      ],
      "text": "sports_understanding"
    },
    {
      "page_no": 13,
      "bbox": [
        401.28912353515625,
        198.4113311767578,
        410.6562805175781,
        220.54238891601562
      ],
      "text": "crass_ai"
    },
    {
      "page_no": 13,
      "bbox": [
        407.05908203125,
        198.39089965820312,
        416.4262390136719,
        246.62411499023438
      ],
      "text": "physical_intuition"
    },
    {
      "page_no": 13,
      "bbox": [
        412.90545654296875,
        198.4080047607422,
        422.27264404296875,
        220.8483428955078
      ],
      "text": "timedial"
    },
    {
      "page_no": 13,
      "bbox": [
        418.59893798828125,
        198.27764892578125,
        427.96612548828125,
        245.31787109375
      ],
      "text": "implicit_relations"
    },
    {
      "page_no": 13,
      "bbox": [
        424.368896484375,
        198.28125,
        433.7360534667969,
        245.376708984375
      ],
      "text": "english_proverbs"
    },
    {
      "page_no": 13,
      "bbox": [
        430.1387939453125,
        198.28028869628906,
        439.5059814453125,
        260.4593200683594
      ],
      "text": "presuppositions_as_nli"
    },
    {
      "page_no": 13,
      "bbox": [
        435.90875244140625,
        198.285400390625,
        445.27593994140625,
        265.225341796875
      ],
      "text": "movie_recommendation"
    },
    {
      "page_no": 13,
      "bbox": [
        441.6787109375,
        198.40182495117188,
        451.0458984375,
        257.96844482421875
      ],
      "text": "understanding_fables"
    },
    {
      "page_no": 13,
      "bbox": [
        447.44866943359375,
        198.41046142578125,
        456.8158264160156,
        250.24473571777344
      ],
      "text": "metaphor_boolean"
    },
    {
      "page_no": 13,
      "bbox": [
        453.21856689453125,
        198.41091918945312,
        462.58575439453125,
        255.68545532226562
      ],
      "text": "temporal_sequences"
    },
    {
      "page_no": 13,
      "bbox": [
        458.988525390625,
        198.40013122558594,
        468.3556823730469,
        245.6944580078125
      ],
      "text": "logical_sequence"
    },
    {
      "page_no": 13,
      "bbox": [
        464.75848388671875,
        198.31289672851562,
        474.1256408691406,
        262.071533203125
      ],
      "text": "identify_odd_metaphor"
    },
    {
      "page_no": 13,
      "bbox": [
        470.5284423828125,
        198.02847290039062,
        479.8955993652344,
        276.4951477050781
      ],
      "text": "gre_reading_comprehension"
    },
    {
      "page_no": 13,
      "bbox": [
        476.29833984375,
        198.4090118408203,
        485.6654968261719,
        233.65188598632812
      ],
      "text": "odd_one_out"
    },
    {
      "page_no": 13,
      "bbox": [
        482.06829833984375,
        198.3976287841797,
        491.4354553222656,
        254.83816528320312
      ],
      "text": "analogical_similarity"
    },
    {
      "page_no": 13,
      "bbox": [
        105.91204071044922,
        182.8380889892578,
        112.93742370605469,
        192.20526123046875
      ],
      "text": "20"
    },
    {
      "page_no": 13,
      "bbox": [
        109.42771911621094,
        169.2454376220703,
        112.94041442871094,
        178.61260986328125
      ],
      "text": "0"
    },
    {
      "page_no": 13,
      "bbox": [
        105.91302490234375,
        155.65280151367188,
        112.93840789794922,
        165.0199737548828
      ],
      "text": "20"
    },
    {
      "page_no": 13,
      "bbox": [
        105.91302490234375,
        142.0601348876953,
        112.93840789794922,
        151.42730712890625
      ],
      "text": "40"
    },
    {
      "page_no": 13,
      "bbox": [
        105.91302490234375,
        128.4674835205078,
        112.93840789794922,
        137.83465576171875
      ],
      "text": "60"
    },
    {
      "page_no": 13,
      "bbox": [
        105.91302490234375,
        114.87483215332031,
        112.93840789794922,
        124.24201202392578
      ],
      "text": "80"
    },
    {
      "page_no": 13,
      "bbox": [
        102.39832305908203,
        101.28218078613281,
        112.9364013671875,
        110.64936065673828
      ],
      "text": "100"
    },
    {
      "page_no": 13,
      "bbox": [
        102.39832305908203,
        87.68952941894531,
        112.9364013671875,
        97.05670928955078
      ],
      "text": "120"
    },
    {
      "page_no": 13,
      "bbox": [
        84.84722900390625,
        106.69775390625,
        95.06596374511719,
        175.76463317871094
      ],
      "text": "Relative Improvement"
    },
    {
      "page_no": 13,
      "bbox": [
        91.59576416015625,
        121.71149444580078,
        101.81449890136719,
        161.1645050048828
      ],
      "text": "over Gopher"
    },
    {
      "page_no": 13,
      "bbox": [
        62.36199951171875,
        319.8462829589844,
        533.120849609375,
        344.55584716796875
      ],
      "text": "Figure 7 | BIG-bench results compared to Gopher Chinchilla out performs Gopher on all but four\nBIG-bench tasks considered. Full results are in Table A7."
    },
    {
      "page_no": 13,
      "bbox": [
        62.36199951171875,
        368.84698486328125,
        254.90760803222656,
        379.7560729980469
      ],
      "text": "4.2.6. Closed-book question answering"
    },
    {
      "page_no": 13,
      "bbox": [
        61.457000732421875,
        390.9127197265625,
        534.4348754882812,
        496.66583251953125
      ],
      "text": "Results on closed-book question answering benchmarks are reported in Table 9. On the Natural\nQuestions dataset (Kwiatkowski et al., 2019), Chinchilla achieves new closed-book SOTA accuracies:\n31.5% 5-shot and 35.5% 64-shot, compared to 21% and 28% respectively, for Gopher. On TriviaQA\n(Joshi et al., 2017) we show results for both the ﬁltered (previously used in retrieval and open-book\nwork) and unﬁltered set (previously used in large language model evaluations). In both cases,\nChinchilla substantially out performs Gopher. On the ﬁltered version, Chinchilla lags behind the open\nbook SOTA (Izacard and Grave, 2020) by only 7.9%. On the unﬁltered set, Chinchilla outperforms\nGPT-3—see Table 9."
    },
    {
      "page_no": 13,
      "bbox": [
        62.36199951171875,
        516.825927734375,
        217.6967010498047,
        527.7350463867188
      ],
      "text": "4.2.7. Gender bias and toxicity"
    },
    {
      "page_no": 13,
      "bbox": [
        62.36199951171875,
        538.8917236328125,
        534.4254760742188,
        577.2686157226562
      ],
      "text": "Large Language Models carry potential risks such as outputting oﬀensive language, propagating\nsocial biases, and leaking private information (Bender et al., 2021; Weidinger et al., 2021). We\nexpect Chinchilla to carry risks similar to Gopher because Chinchilla is trained on the same data,"
    },
    {
      "page_no": 13,
      "bbox": [
        179.2429962158203,
        601.6867065429688,
        486.02593994140625,
        612.9404296875
      ],
      "text": "Chinchilla\nGopher\nGPT-3\nMT-NLG 530B\nSupervised SOTA"
    },
    {
      "page_no": 13,
      "bbox": [
        109.25201416015625,
        620.708740234375,
        460.7090148925781,
        686.1046752929688
      ],
      "text": "HellaSWAG\n80.8%\n79.2%\n78.9%\n80.2%\n93.9%\nPIQA\n81.8%\n81.8%\n81.0%\n82.0%\n90.1%\nWinogrande\n74.9%\n70.1%\n70.2%\n73.0%\n91.3%\nSIQA\n51.3%\n50.6%\n-\n-\n83.2%\nBoolQ\n83.7%\n79.3%\n60.5%\n78.2%\n91.4%"
    },
    {
      "page_no": 13,
      "bbox": [
        62.03499984741211,
        700.9122314453125,
        532.9172973632812,
        752.7198486328125
      ],
      "text": "Table 8 | Zero-shot comparison on Common Sense benchmarks. We show a comparison between\nChinchilla, Gopher, and MT-NLG 530B on various Common Sense benchmarks. We see that Chinchilla\nmatches or outperforms Gopher and GPT-3 on all tasks. On all but one Chinchilla outperforms the\nmuch larger MT-NLG 530B model."
    },
    {
      "page_no": 13,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "13"
    },
    {
      "page_no": 14,
      "bbox": [
        222.50900268554688,
        89.71675109863281,
        507.70147705078125,
        100.97047424316406
      ],
      "text": "Method\nChinchilla\nGopher\nGPT-3\nSOTA (open book)"
    },
    {
      "page_no": 14,
      "bbox": [
        91.91200256347656,
        108.73872375488281,
        479.53265380859375,
        146.745849609375
      ],
      "text": "Natural Questions (dev)\n0-shot\n16.6%\n10.1%\n14.6%\n54.4%\n5-shot\n31.5%\n24.5%\n-\n64-shot\n35.5%\n28.2%\n29.9%"
    },
    {
      "page_no": 14,
      "bbox": [
        87.57599639892578,
        154.8596954345703,
        466.0110168457031,
        192.8668212890625
      ],
      "text": "TriviaQA (unﬁltered, test)\n0-shot\n67.0%\n52.8%\n64.3 %\n-\n5-shot\n73.2%\n63.6%\n-\n64-shot\n72.3%\n61.3%\n71.2%"
    },
    {
      "page_no": 14,
      "bbox": [
        93.8270034790039,
        200.97975158691406,
        479.5326843261719,
        238.98687744140625
      ],
      "text": "TriviaQA (ﬁltered, dev)\n0-shot\n55.4%\n43.5%\n-\n72.5%\n5-shot\n64.1%\n57.0%\n-\n64-shot\n64.6%\n57.2%\n-"
    },
    {
      "page_no": 14,
      "bbox": [
        62.03499984741211,
        254.08425903320312,
        534.4310302734375,
        305.892822265625
      ],
      "text": "Table 9 | Closed-book question answering. For Natural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), Chinchilla outperforms Gopher in all cases. On Natural Questions,\nChinchilla outperforms GPT-3. On TriviaQA we show results on two diﬀerent evaluation sets to allow\nfor comparison to GPT-3 and to open book SOTA (FiD + Distillation (Izacard and Grave, 2020))."
    },
    {
      "page_no": 14,
      "bbox": [
        62.36199951171875,
        329.9327392578125,
        533.10205078125,
        381.48883056640625
      ],
      "text": "albeit with slightly diﬀerent relative weights, and because it has a similar architecture. Here, we\nexamine gender bias (particularly gender and occupation bias) and generation of toxic language. We\nselect a few common evaluations to highlight potential issues, but stress that our evaluations are not\ncomprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs."
    },
    {
      "page_no": 14,
      "bbox": [
        61.457000732421875,
        406.16571044921875,
        534.7326049804688,
        511.9188232421875
      ],
      "text": "Gender bias.\nAs discussed in Rae et al. (2021), large language models reﬂect contemporary and\nhistorical discourse about diﬀerent groups (such as gender groups) from their training dataset, and\nwe expect the same to be true for Chinchilla. Here, we test if potential gender and occupation biases\nmanifest in unfair outcomes on coreference resolutions, using the Winogender dataset (Rudinger\net al., 2018) in a zero-shot setting. Winogender tests whether a model can correctly determine if\na pronoun refers to diﬀerent occupation words. An unbiased model would correctly predict which\nword the pronoun refers to regardless of pronoun gender. We follow the same setup as in Rae et al.\n(2021) (described further in Section H.3)."
    },
    {
      "page_no": 14,
      "bbox": [
        61.457000732421875,
        521.333740234375,
        533.1912231445312,
        654.1848754882812
      ],
      "text": "As shown in Table 10, Chinchilla correctly resolves pronouns more frequently than Gopher across\nall groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase\nof 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also\nconsider gotcha examples, in which the correct pronoun resolution contradicts gender stereotypes\n(determined by labor statistics). Again, we see that Chinchilla resolves pronouns more accurately\nthan Gopher. When breaking up examples by male/female gender and gotcha/not gotcha, the largest\nimprovement is on female gotcha examples (improvement of 10%). Thus, though Chinchilla uniformly\novercomes gender stereotypes for more coreference examples than Gopher, the rate of improvement\nis higher for some pronouns than others, suggesting that the improvements conferred by using a more\ncompute-optimal model can be uneven."
    },
    {
      "page_no": 14,
      "bbox": [
        62.36199951171875,
        678.8616943359375,
        534.7401733398438,
        757.516845703125
      ],
      "text": "Sample toxicity.\nLanguage models are capable of generating toxic language—including insults,\nhate speech, profanities and threats (Gehman et al., 2020; Rae et al., 2021). While toxicity is an\numbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021),\nautomatic classiﬁer scores can provide an indication for the levels of harmful text that a LM generates.\nRae et al. (2021) found that improving language modelling loss by increasing the number of model\nparameters has only a negligible eﬀect on toxic text generation (unprompted); here we analyze"
    },
    {
      "page_no": 14,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "14"
    },
    {
      "page_no": 15,
      "bbox": [
        144.26800537109375,
        90.06137084960938,
        236.92327880859375,
        100.97047424316406
      ],
      "text": "Chinchilla\nGopher"
    },
    {
      "page_no": 15,
      "bbox": [
        96.50399017333984,
        108.73872375488281,
        234.11965942382812,
        160.29583740234375
      ],
      "text": "All\n78.3%\n71.4%\nMale\n71.2%\n68.0%\nFemale\n79.6%\n71.3%\nNeutral\n84.2%\n75.0%"
    },
    {
      "page_no": 15,
      "bbox": [
        416.0119934082031,
        90.06137084960938,
        508.6673278808594,
        100.97047424316406
      ],
      "text": "Chinchilla\nGopher"
    },
    {
      "page_no": 15,
      "bbox": [
        318.8399963378906,
        108.73872375488281,
        505.8636474609375,
        160.6404571533203
      ],
      "text": "Male gotcha\n62.5%\n59.2%\nMale not gotcha\n80.0%\n76.7%\nFemale gotcha\n76.7%\n66.7%\nFemale not gotcha\n82.5%\n75.8%"
    },
    {
      "page_no": 15,
      "bbox": [
        62.03499984741211,
        175.39328002929688,
        534.7379150390625,
        213.99647521972656
      ],
      "text": "Table 10 | Winogender results. Left: Chinchilla consistently resolves pronouns better than Gopher.\nRight: Chinchilla performs better on examples which contradict gender stereotypes (gotcha examples).\nHowever, diﬀerence in performance across groups suggests Chinchilla exhibits bias."
    },
    {
      "page_no": 15,
      "bbox": [
        61.9370002746582,
        237.6917266845703,
        533.1240234375,
        370.5438232421875
      ],
      "text": "whether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar\nto the protocol of Rae et al. (2021), we generate 25,000 unprompted samples from Chinchilla, and\ncompare their PerspectiveAPI toxicity score distribution to that of Gopher-generated samples. Several\nsummary statistics indicate an absence of major diﬀerences: the mean (median) toxicity score for\nGopher is 0.081 (0.064), compared to 0.087 (0.066) for Chinchilla, and the 95th percentile scores\nare 0.230 for Gopher, compared to 0.238 for Chinchilla. That is, the large majority of generated\nsamples are classiﬁed as non-toxic, and the diﬀerence between the models is negligible. In line with\nprior ﬁndings (Rae et al., 2021), this suggests that toxicity levels in unconditional text generation\nare largely independent of the model quality (measured in language modelling loss), i.e. that better\nmodels of the training dataset are not necessarily more toxic."
    },
    {
      "page_no": 15,
      "bbox": [
        62.36199951171875,
        394.8089904785156,
        226.33973693847656,
        407.7604064941406
      ],
      "text": "5. Discussion & Conclusion"
    },
    {
      "page_no": 15,
      "bbox": [
        62.03499984741211,
        421.40374755859375,
        534.43505859375,
        527.1567993164062
      ],
      "text": "The trend so far in large language model training has been to increase the model size, often without\nincreasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now\nover 3× larger than GPT-3’s 170 billion parameters from just two years ago. However, this model,\nas well as the majority of existing large models, have all been trained for a comparable number\nof tokens—around 300 billion. While the desire to train these mega-models has led to substantial\nengineering innovation, we hypothesize that the race to train larger and larger models is resulting in\nmodels that are substantially underperforming compared to what could be achieved with the same\ncompute budget."
    },
    {
      "page_no": 15,
      "bbox": [
        62.36199951171875,
        536.5717163085938,
        534.648681640625,
        615.2268676757812
      ],
      "text": "We propose three predictive approaches towards optimally setting model size and training dura-\ntion, based on the outcome of over 400 training runs. All three approaches predict that Gopher is\nsubstantially over-sized and estimate that for the same compute budget a smaller model trained on\nmore data will perform better. We directly test this hypothesis by training Chinchilla, a 70B parameter\nmodel, and show that it outperforms Gopher and even larger models on nearly every measured\nevaluation task."
    },
    {
      "page_no": 15,
      "bbox": [
        61.457000732421875,
        624.6417236328125,
        534.735107421875,
        757.82958984375
      ],
      "text": "Whilst our method allows us to make predictions on how to scale large models when given\nadditional compute, there are several limitations. Due to the cost of training large models, we only\nhave two comparable training runs at large scale (Chinchilla and Gopher), and we do not have\nadditional tests at intermediate scales. Furthermore, we assume that the eﬃcient computational\nfrontier can be described by a power-law relationship between the compute budget, model size, and\nnumber of training tokens. However, we observe some concavity in log \u0000\n𝑁𝑜𝑝𝑡\n\u0001 at high compute budgets\n(see Appendix E). This suggests that we may still be overestimating the optimal size of large models.\nFinally, the training runs for our analysis have all been trained on less than an epoch of data; future\nwork may consider the multiple epoch regime. Despite these limitations, the comparison of Chinchilla\nto Gopher validates our performance predictions, that have thus enabled training a better (and more"
    },
    {
      "page_no": 15,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "15"
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        294.1148681640625,
        98.18084716796875
      ],
      "text": "lightweight) model at the same compute budget."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        107.59474182128906,
        534.4345703125,
        267.54486083984375
      ],
      "text": "Though there has been signiﬁcant recent work allowing larger and larger models to be trained,\nour analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that\nscaling to larger and larger datasets is only beneﬁcial when the data is high-quality. This calls for\nresponsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require\nextra care to ensure train-test set overlap is properly accounted for, both in the language modelling\nloss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical\nand privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and\nprivate information. With even larger datasets being used, the quantity (if not the frequency) of such\ninformation increases, which makes dataset introspection all the more important. Chinchilla does\nsuﬀer from bias and toxicity but interestingly it seems less aﬀected than Gopher. Better understanding\nhow performance of large language models and toxicity interact is an important future research\nquestion."
    },
    {
      "page_no": 16,
      "bbox": [
        61.9370002746582,
        276.959716796875,
        534.4325561523438,
        328.516845703125
      ],
      "text": "While we have applied our methodology towards the training of auto-regressive language models,\nwe expect that there is a similar trade-oﬀbetween model size and the amount of data in other\nmodalities. As training large models is very expensive, choosing the optimal model size and training\nsteps beforehand is essential. The methods we propose are easy to reproduce in new settings."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        352.7820129394531,
        195.7873077392578,
        365.7334289550781
      ],
      "text": "6. Acknowledgements"
    },
    {
      "page_no": 16,
      "bbox": [
        61.86000061035156,
        379.376708984375,
        534.4304809570312,
        444.48284912109375
      ],
      "text": "We’d like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis,\nGeoﬀrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on\nthe manuscript. We’d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and\nother colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA\nteam for their support and assistance."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        468.7480163574219,
        126.97654724121094,
        481.6994323730469
      ],
      "text": "References"
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        495.34271240234375,
        534.4256591796875,
        546.8998413085938
      ],
      "text": "M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru,\nG. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O’Horo,\nJ. Wang, L. Zettlemoyer, M. Diab, Z. Kozareva, and V. Stoyanov. Eﬃcient Large Scale Language\nModeling with Mixtures of Experts. arXiv:2112.10684, 2021."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        558.5057373046875,
        534.4299926757812,
        596.5138549804688
      ],
      "text": "E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots:\nCan language models be too big?\nIn Proceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, pages 610–623, 2021."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        608.1197509765625,
        532.9120483398438,
        633.640869140625
      ],
      "text": "BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of\nlanguage models. In preparation, 2021. URL https://github.com/google/BIG-bench/."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        644.1846923828125,
        532.916259765625,
        682.1918334960938
      ],
      "text": "Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in natural\nlanguage. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages\n7432–7439, 2020."
    },
    {
      "page_no": 16,
      "bbox": [
        62.36199951171875,
        693.7987060546875,
        534.7393188476562,
        758.9048461914062
      ],
      "text": "S. Borgeaud, A. Mensch, J. Hoﬀmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J.-B.\nLespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,\nL. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero,\nK. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from\ntrillions of tokens. arXiv 2112.04426, 2021."
    },
    {
      "page_no": 16,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "16"
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        534.6439208984375,
        126.34183502197266
      ],
      "text": "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-\nderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms. 2018. URL http://github.com/google/jax."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        136.26271057128906,
        534.4346923828125,
        243.0798797607422
      ],
      "text": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb49674\n18bfb8ac142f64a-Paper.pdf."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        253.00074768066406,
        532.913818359375,
        263.90985107421875
      ],
      "text": "S. Bubeck. Convex Optimization: Algorithms and Complexity. Foundations and Trends in Machine"
    },
    {
      "page_no": 17,
      "bbox": [
        73.20899963378906,
        266.52178955078125,
        532.9097900390625,
        292.0708923339844
      ],
      "text": "Learning, 8(3-4):231–357, 2015. URL http://www.nowpublishers.com/article/Detail\ns/MAL-050."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        301.99273681640625,
        534.4305419921875,
        368.161865234375
      ],
      "text": "A. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. Hoﬀmann, B. Damoc, B. Hechtman,\nT. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican,\nA. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen,\nK. Kavukcuoglu, and K. Simonyan. Uniﬁed scaling laws for routed language models, 2022. URL\nhttps://arxiv.org/abs/2202.01169."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        378.082763671875,
        532.9135131835938,
        429.63983154296875
      ],
      "text": "C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising diﬃculty of natural yes/no questions. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 2924–2936, 2019."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        440.62371826171875,
        534.6463623046875,
        493.24285888671875
      ],
      "text": "N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph,\nL. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-\nHellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Eﬃcient scaling of\nlanguage models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        503.16473388671875,
        532.9122314453125,
        527.6228637695312
      ],
      "text": "W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        538.606689453125,
        534.4299926757812,
        576.6148071289062
      ],
      "text": "L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\nS. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027, 2020."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        587.5987548828125,
        532.9179077148438,
        653.7678833007812
      ],
      "text": "S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating\nneural toxic degeneration in language models. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2\n020.findings-emnlp.301."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        663.688720703125,
        532.9119873046875,
        688.1478271484375
      ],
      "text": "K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model\npre-training, 2020."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        699.1317138671875,
        532.9121704101562,
        723.58984375
      ],
      "text": "D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. arXiv preprint arXiv:2009.03300, 2020."
    },
    {
      "page_no": 17,
      "bbox": [
        62.36199951171875,
        734.5457763671875,
        536.0692749023438,
        760.0958862304688
      ],
      "text": "T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX. 2020. URL http:\n//github.com/deepmind/dm-haiku."
    },
    {
      "page_no": 17,
      "bbox": [
        291.57196044921875,
        778.01171875,
        303.702880859375,
        788.9208374023438
      ],
      "text": "17"
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        488.8531188964844,
        98.18084716796875
      ],
      "text": "D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer, 2021."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        109.78675842285156,
        533.1961059570312,
        134.24481201171875
      ],
      "text": "P. J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics, 35\n(1):73–101, Mar. 1964. ISSN 0003-4851, 2168-8990. doi: 10.1214/aoms/1177703732. URL"
    },
    {
      "page_no": 18,
      "bbox": [
        72.89399719238281,
        136.85675048828125,
        533.23291015625,
        175.9558563232422
      ],
      "text": "https://projecteuclid.org/journals/annals-of-mathematical-statistics/vol\nume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/11\n77703732.full."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        186.4987335205078,
        534.734130859375,
        197.4078369140625
      ],
      "text": "G. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering, 2020."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        209.0147247314453,
        532.9119873046875,
        233.47283935546875
      ],
      "text": "M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        245.07972717285156,
        534.7363891601562,
        406.0928649902344
      ],
      "text": "N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,\nA. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,\nT. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt,\nD. Hurt, J. Ibarz, A. Jaﬀey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy,\nJ. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony,\nK. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,\nA. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter,\nD. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter,\nW. Wang, E. Wilcox, and D. H. Yoon. In-datacenter performance analysis of a tensor processing unit.\nIn Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA ’17,\npage 1–12, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348928.\ndoi: 10.1145/3079856.3080246. URL https://doi.org/10.1145/3079856.3080246."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        416.63671875,
        534.6204833984375,
        441.0948486328125
      ],
      "text": "J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        452.70074462890625,
        534.4300537109375,
        477.1588439941406
      ],
      "text": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        488.7657470703125,
        533.1265258789062,
        513.2238159179688
      ],
      "text": "T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        524.8306884765625,
        534.4312744140625,
        576.3868408203125
      ],
      "text": "T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nM. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and\nS. Petrov. Natural questions: a benchmark for question answering research. Transactions of the\nAssociation of Computational Linguistics, 2019."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        587.9937133789062,
        532.9177856445312,
        640.6138916015625
      ],
      "text": "G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehension dataset from\nexaminations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 785–794, Copenhagen, Denmark, Sept. 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        651.15673828125,
        534.7387084960938,
        662.0658569335938
      ],
      "text": "Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention."
    },
    {
      "page_no": 18,
      "bbox": [
        73.20899963378906,
        664.7056884765625,
        261.58770751953125,
        675.6148071289062
      ],
      "text": "arXiv preprint arXiv:2006.12467, 2020."
    },
    {
      "page_no": 18,
      "bbox": [
        62.36199951171875,
        687.2217407226562,
        534.42626953125,
        738.7788696289062
      ],
      "text": "P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih,\nT. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 9459–9474,\n2020."
    },
    {
      "page_no": 18,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "18"
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        532.9139404296875,
        98.18084716796875
      ],
      "text": "O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White"
    },
    {
      "page_no": 19,
      "bbox": [
        73.20899963378906,
        100.82075500488281,
        185.0166778564453,
        111.7298583984375
      ],
      "text": "Paper. AI21 Labs, 2021."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        123.14772033691406,
        533.1859130859375,
        134.05682373046875
      ],
      "text": "S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv"
    },
    {
      "page_no": 19,
      "bbox": [
        73.20899963378906,
        136.6967315673828,
        233.9327392578125,
        147.6058349609375
      ],
      "text": "preprint arXiv:2109.07958, 2021."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        159.0237579345703,
        532.9135131835938,
        169.932861328125
      ],
      "text": "I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on"
    },
    {
      "page_no": 19,
      "bbox": [
        73.20899963378906,
        172.54473876953125,
        522.53271484375,
        184.5448455810547
      ],
      "text": "Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        194.8997344970703,
        534.4259643554688,
        219.35882568359375
      ],
      "text": "S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training,\n2018."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        230.77674865722656,
        532.9136962890625,
        241.68585205078125
      ],
      "text": "S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. International"
    },
    {
      "page_no": 19,
      "bbox": [
        73.20899963378906,
        244.3257598876953,
        298.74395751953125,
        255.23486328125
      ],
      "text": "Conference on Learning Representations, 2017."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        266.6527099609375,
        534.64404296875,
        304.65985107421875
      ],
      "text": "M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Ge-\nbru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability,\nand transparency, pages 220–229, 2019."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        316.0787353515625,
        537.3239135742188,
        355.14886474609375
      ],
      "text": "J. Nocedal. Updating Quasi-Newton Matrices with Limited Storage. Mathematics of Computation,\n35(151):773–782, 1980. ISSN 0025-5718. doi: 10.2307/2006193. URL https://www.jstor.\norg/stable/2006193."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        365.50372314453125,
        534.4346313476562,
        403.5118408203125
      ],
      "text": "D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nand R. Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context,\n2016."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        414.92974853515625,
        534.73828125,
        561.3308715820312
      ],
      "text": "J. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoﬀmann, F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.\nHendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor,\nI. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden,\nE. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh,\nE. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev,\nD. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d’Autume, Y. Li,\nT. Terzi, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson, L. Weidinger,\nI. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,\nL. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis\n& insights from training Gopher. arXiv 2112.11446, 2021."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        572.7487182617188,
        534.7310791015625,
        610.755859375
      ],
      "text": "J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, K. Choromanski, V. Likhosherstov, D. Dohan,\nX. Song, A. Gane, T. Sarlos, et al. Compressive transformers for long-range sequence modelling.\nAdvances in Neural Information Processing Systems, 33:6154–6158, 2020."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        622.1737060546875,
        532.9138793945312,
        661.244873046875
      ],
      "text": "C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning\nResearch, 21(140):1–67, 2020a. URL http://jmlr.org/papers/v21/20-074.html."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        671.5997314453125,
        532.9138793945312,
        709.6068725585938
      ],
      "text": "C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning\nResearch, 21(140):1–67, 2020b."
    },
    {
      "page_no": 19,
      "bbox": [
        62.36199951171875,
        721.0247192382812,
        534.4302368164062,
        759.0328369140625
      ],
      "text": "S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC20: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1–16. IEEE, 2020."
    },
    {
      "page_no": 19,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "19"
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        534.4300537109375,
        111.7298583984375
      ],
      "text": "H. Robbins and S. Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics,\n22(3):400–407, Sept. 1951."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        123.33570861816406,
        534.7376098632812,
        174.892822265625
      ],
      "text": "R. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme. Gender bias in coreference resolu-\ntion. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, New Orleans, Louisiana, June 2018.\nAssociation for Computational Linguistics."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        186.4987335205078,
        534.424560546875,
        224.5068359375
      ],
      "text": "K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema\nchallenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34,\npages 8732–8740, 2020."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        236.1127471923828,
        533.1016845703125,
        274.120849609375
      ],
      "text": "M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. SocialIQA: Commonsense reasoning about\nsocial interactions. Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing, 2019."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        285.72674560546875,
        532.9166259765625,
        310.18585205078125
      ],
      "text": "C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the eﬀects\nof data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        321.791748046875,
        534.737548828125,
        332.7008361816406
      ],
      "text": "J. W. Siegel and J. Xu. Approximation rates for neural networks with general activation functions."
    },
    {
      "page_no": 20,
      "bbox": [
        73.208984375,
        335.3127746582031,
        532.9137573242188,
        360.86285400390625
      ],
      "text": "Neural Networks, 128:313–321, Aug. 2020. URL https://www.sciencedirect.com/scienc\ne/article/pii/S0893608020301891."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        371.40576171875,
        534.6443481445312,
        436.5118408203125
      ],
      "text": "S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,\nG. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi,\nY. He, M. Houston, S. Tiwary, and B. Catanzaro. Using Deepspeed and Megatron to Train Megatron-\nturing NLG 530b, A Large-Scale Generative Language Model. arXiv preprint arXiv:2201.11990,\n2022."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        448.0897521972656,
        532.908203125,
        473.63983154296875
      ],
      "text": "J. Steinhardt. Updates and lessons from AI forecasting, 2021. URL https://bounded-regret.g\nhost.io/ai-forecasting/."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        484.1827392578125,
        534.4296264648438,
        508.6408386230469
      ],
      "text": "Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani,\nand D. Metzler. Scale eﬃciently: Insights from pre-training and ﬁne-tuning transformers, 2021."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        520.2476806640625,
        534.6478881835938,
        626.0008544921875
      ],
      "text": "R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin,\nJ. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch,\nM. Pickett, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zeven-\nbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoﬀman-John, J. Lee,\nL. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,\nR. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. LaMDA: Language models for\ndialog applications, 2022."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        637.6077270507812,
        534.7335205078125,
        675.6148071289062
      ],
      "text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in neural information processing systems, pages 5998–6008,\n2017."
    },
    {
      "page_no": 20,
      "bbox": [
        62.36199951171875,
        687.2217407226562,
        534.4296875,
        738.7788696289062
      ],
      "text": "L. Weidinger, J. Mellor, M. Rauh, C. Griﬃn, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle,\nA. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell,\nL. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm\nfrom language models. arXiv submission, 2021."
    },
    {
      "page_no": 20,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "20"
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        537.324462890625,
        153.4398651123047
      ],
      "text": "J. Welbl, A. Glaese, J. Uesato, S. Dathathri, J. Mellor, L. A. Hendricks, K. Anderson, P. Kohli, B. Coppin,\nand P.-S. Huang. Challenges in detoxifying language models. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, pages 2447–2469, Punta Cana, Dominican Republic,\nNov. 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.\nfindings-emnlp.210."
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        163.9837188720703,
        533.8215942382812,
        230.1528778076172
      ],
      "text": "A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying language models\nrisks marginalizing minority voices. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n2390–2397, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021\n.naacl-main.190. URL https://aclanthology.org/2021.naacl-main.190."
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        240.6957550048828,
        534.7327880859375,
        293.3158874511719
      ],
      "text": "G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao.\nTuning large neural networks via zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin,\nP. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\nURL https://openreview.net/forum?id=Bx6qKuBM2AD."
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        303.85870361328125,
        532.9134521484375,
        341.8668518066406
      ],
      "text": "R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really ﬁnish\nyour sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019."
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        353.47271728515625,
        536.0632934570312,
        433.1908874511719
      ],
      "text": "G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which\nalgorithmic choices matter at which batch sizes? insights from a noisy quadratic model. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances\nin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https:\n//proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b621\n4-Paper.pdf."
    },
    {
      "page_no": 21,
      "bbox": [
        62.36199951171875,
        443.7347412109375,
        532.916015625,
        468.1928405761719
      ],
      "text": "B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. Designing eﬀective\nsparse expert models, 2022."
    },
    {
      "page_no": 21,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "21"
    },
    {
      "page_no": 22,
      "bbox": [
        243.18099975585938,
        86.78992462158203,
        352.0954895019531,
        111.5770263671875
      ],
      "text": "Appendix"
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        127.42599487304688,
        179.45559692382812,
        140.3773956298828
      ],
      "text": "A. Training dataset"
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        154.0207061767578,
        533.0939331054688,
        178.8234405517578
      ],
      "text": "In Table A1 we show the training dataset makeup used for Chinchilla and all scaling runs. Note that\nboth the MassiveWeb and Wikipedia subsets are both used for more than one epoch."
    },
    {
      "page_no": 22,
      "bbox": [
        163.0760040283203,
        195.2967071533203,
        498.5744323730469,
        206.205810546875
      ],
      "text": "Disk Size\nDocuments\nSampling proportion\nEpochs in 1.4T tokens"
    },
    {
      "page_no": 22,
      "bbox": [
        96.70600128173828,
        214.31874084472656,
        456.880859375,
        292.973876953125
      ],
      "text": "MassiveWeb\n1.9 TB\n604M\n45% (48%)\n1.24\nBooks\n2.1 TB\n4M\n30% (27%)\n0.75\nC4\n0.75 TB\n361M\n10% (10%)\n0.77\nNews\n2.7 TB\n1.1B\n10% (10%)\n0.21\nGitHub\n3.1 TB\n142M\n4% (3%)\n0.13\nWikipedia\n0.001 TB\n6M\n1% (2%)\n3.40"
    },
    {
      "page_no": 22,
      "bbox": [
        62.03499984741211,
        308.0712585449219,
        533.1011352539062,
        359.8798522949219
      ],
      "text": "Table A1 | MassiveText data makeup. For each subset of MassiveText, we list its total disk size, the\nnumber of documents and the sampling proportion used during training—we use a slightly diﬀerent\ndistribution than in Rae et al. (2021) (shown in parenthesis). In the rightmost column show the\nnumber of epochs that are used in 1.4 trillion tokens."
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        391.0230407714844,
        245.5077667236328,
        403.9744567871094
      ],
      "text": "B. Optimal cosine cycle length"
    },
    {
      "page_no": 22,
      "bbox": [
        61.457000732421875,
        417.61773681640625,
        532.917724609375,
        496.2728271484375
      ],
      "text": "One key assumption is made on the cosine cycle length and the corresponding learning rate drop\n(we use a 10× learning rate decay in line with Rae et al. (2021)).9 We ﬁnd that setting the cosine\ncycle length too much longer than the target number of training steps results in sub-optimally trained\nmodels, as shown in Figure A1. As a result, we assume that an optimally trained model will have the\ncosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we\nfollow this rule in our main analysis."
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        520.5380249023438,
        350.9710388183594,
        533.4893798828125
      ],
      "text": "C. Consistency of scaling results across datasets"
    },
    {
      "page_no": 22,
      "bbox": [
        61.86000061035156,
        547.1326904296875,
        534.3129272460938,
        599.034423828125
      ],
      "text": "We show scaling results from an IsoFLOP (Approach 2) analysis after training on two diﬀerent datasets:\nC4 (Raﬀel et al., 2020b) and GitHub code (we show results with data from Rae et al. (2021)), results\nare shown in Table A2. For both set of experiments using subsets of MassiveText, we use the same\ntokenizer as the MassiveText experiments."
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        608.104736328125,
        534.4300537109375,
        646.11181640625
      ],
      "text": "We ﬁnd that the scaling behaviour on these datasets is very similar to what we found on MassiveText,\nas shown in Figure A2 and Table A2. This suggests that our results are independent of the dataset as\nlong as one does not train for more than one epoch."
    },
    {
      "page_no": 22,
      "bbox": [
        62.36199951171875,
        736.81884765625,
        534.1624755859375,
        758.644287109375
      ],
      "text": "9We ﬁnd the diﬀerence between decaying by 10× and decaying to 0.0 (over the same number of steps) to be small,\nthough decaying by a factor of 10× to be slightly more performant. Decaying by less (5×) is clearly worse."
    },
    {
      "page_no": 22,
      "bbox": [
        291.57196044921875,
        778.01171875,
        303.702880859375,
        788.9208374023438
      ],
      "text": "22"
    },
    {
      "page_no": 23,
      "bbox": [
        108.58296203613281,
        195.57493591308594,
        224.1294403076172,
        211.22608947753906
      ],
      "text": "0\n2\n4\n6\n8\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        186.46253967285156,
        102.59210205078125,
        195.43980407714844
      ],
      "text": "0.0"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        166.9964141845703,
        102.59210205078125,
        175.9736785888672
      ],
      "text": "0.2"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        147.53028869628906,
        102.59210205078125,
        156.50755310058594
      ],
      "text": "0.4"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        128.06417846679688,
        102.59210205078125,
        137.04144287109375
      ],
      "text": "0.6"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        108.59805297851562,
        102.59210205078125,
        117.57532501220703
      ],
      "text": "0.8"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        89.13194274902344,
        102.59210205078125,
        98.10921478271484
      ],
      "text": "1.0"
    },
    {
      "page_no": 23,
      "bbox": [
        84.89067840576172,
        110.41822052001953,
        94.6840591430664,
        173.85000610351562
      ],
      "text": "Learning Rate/Max LR"
    },
    {
      "page_no": 23,
      "bbox": [
        250.735107421875,
        195.57493591308594,
        366.2815856933594,
        211.22608947753906
      ],
      "text": "0\n2\n4\n6\n8\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        191.27796936035156,
        244.74229431152344,
        200.25523376464844
      ],
      "text": "2.70"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        173.4429473876953,
        244.74229431152344,
        182.4202117919922
      ],
      "text": "2.75"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        155.60792541503906,
        244.74229431152344,
        164.58518981933594
      ],
      "text": "2.80"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        137.77291870117188,
        244.74229431152344,
        146.75018310546875
      ],
      "text": "2.85"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        119.93788146972656,
        244.74229431152344,
        128.91514587402344
      ],
      "text": "2.90"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        102.10285949707031,
        244.74229431152344,
        111.08013153076172
      ],
      "text": "2.95"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        84.267822265625,
        244.74229431152344,
        93.2450942993164
      ],
      "text": "3.00"
    },
    {
      "page_no": 23,
      "bbox": [
        223.67440795898438,
        122.6983413696289,
        233.46780395507812,
        160.9421844482422
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        392.8872375488281,
        195.57493591308594,
        482.54461669921875,
        211.22608947753906
      ],
      "text": "0\n2\n4\n6\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        191.27796936035156,
        386.8944396972656,
        200.25523376464844
      ],
      "text": "2.80"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        177.90170288085938,
        386.8944396972656,
        186.87896728515625
      ],
      "text": "2.85"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        164.5254364013672,
        386.8944396972656,
        173.50270080566406
      ],
      "text": "2.90"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        151.14918518066406,
        386.8944396972656,
        160.12644958496094
      ],
      "text": "2.95"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        137.77291870117188,
        386.8944396972656,
        146.75018310546875
      ],
      "text": "3.00"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        124.39663696289062,
        386.8944396972656,
        133.3739013671875
      ],
      "text": "3.05"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        111.02037048339844,
        386.8944396972656,
        119.99764251708984
      ],
      "text": "3.10"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        97.64410400390625,
        386.8944396972656,
        106.62137603759766
      ],
      "text": "3.15"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        84.267822265625,
        386.8944396972656,
        93.2450942993164
      ],
      "text": "3.20"
    },
    {
      "page_no": 23,
      "bbox": [
        365.8265380859375,
        131.09060668945312,
        375.61993408203125,
        153.39712524414062
      ],
      "text": "C4 Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        442.5030517578125,
        90.81432342529297,
        501.3672180175781,
        100.60770416259766
      ],
      "text": "Cosine Cycle Length"
    },
    {
      "page_no": 23,
      "bbox": [
        455.72552490234375,
        99.185791015625,
        499.6994323730469,
        116.03144073486328
      ],
      "text": "1.0× num. steps\n1.1× num. steps"
    },
    {
      "page_no": 23,
      "bbox": [
        455.72552490234375,
        115.06538391113281,
        502.766357421875,
        131.91102600097656
      ],
      "text": "1.25× num. steps\n1.5× num. steps"
    },
    {
      "page_no": 23,
      "bbox": [
        455.72552490234375,
        130.9449920654297,
        499.6994323730469,
        139.92225646972656
      ],
      "text": "2.0× num. steps"
    },
    {
      "page_no": 23,
      "bbox": [
        455.72552490234375,
        138.88479614257812,
        499.6994323730469,
        147.862060546875
      ],
      "text": "5.0× num. steps"
    },
    {
      "page_no": 23,
      "bbox": [
        106.0566635131836,
        323.9870910644531,
        221.3231658935547,
        339.63824462890625
      ],
      "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        314.87469482421875,
        102.59210205078125,
        323.8519592285156
      ],
      "text": "0.0"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        295.4085693359375,
        102.59210205078125,
        304.3858337402344
      ],
      "text": "0.2"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        275.9424743652344,
        102.59210205078125,
        284.91973876953125
      ],
      "text": "0.4"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        256.4763488769531,
        102.59210205078125,
        265.45361328125
      ],
      "text": "0.6"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        237.01022338867188,
        102.59210205078125,
        245.98748779296875
      ],
      "text": "0.8"
    },
    {
      "page_no": 23,
      "bbox": [
        94.17591094970703,
        217.54409790039062,
        102.59210205078125,
        226.5213623046875
      ],
      "text": "1.0"
    },
    {
      "page_no": 23,
      "bbox": [
        84.89067840576172,
        238.83035278320312,
        94.6840591430664,
        302.26214599609375
      ],
      "text": "Learning Rate/Max LR"
    },
    {
      "page_no": 23,
      "bbox": [
        248.20880126953125,
        323.9870910644531,
        363.4753112792969,
        339.63824462890625
      ],
      "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        319.69012451171875,
        244.74229431152344,
        328.6673889160156
      ],
      "text": "2.70"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        301.8551025390625,
        244.74229431152344,
        310.8323669433594
      ],
      "text": "2.75"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        284.02008056640625,
        244.74229431152344,
        292.9973449707031
      ],
      "text": "2.80"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        266.18505859375,
        244.74229431152344,
        275.1623229980469
      ],
      "text": "2.85"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        248.35003662109375,
        244.74229431152344,
        257.3273010253906
      ],
      "text": "2.90"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        230.5150146484375,
        244.74229431152344,
        239.49227905273438
      ],
      "text": "2.95"
    },
    {
      "page_no": 23,
      "bbox": [
        232.95965576171875,
        212.67999267578125,
        244.74229431152344,
        221.65725708007812
      ],
      "text": "3.00"
    },
    {
      "page_no": 23,
      "bbox": [
        223.67440795898438,
        251.11048889160156,
        233.46780395507812,
        289.3543395996094
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        390.3609313964844,
        323.9870910644531,
        506.1548767089844,
        339.63824462890625
      ],
      "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nMillion Sequences"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        319.69012451171875,
        386.8944396972656,
        328.6673889160156
      ],
      "text": "2.80"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        306.3138732910156,
        386.8944396972656,
        315.2911376953125
      ],
      "text": "2.85"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        292.9375915527344,
        386.8944396972656,
        301.91485595703125
      ],
      "text": "2.90"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        279.56134033203125,
        386.8944396972656,
        288.5386047363281
      ],
      "text": "2.95"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        266.18505859375,
        386.8944396972656,
        275.1623229980469
      ],
      "text": "3.00"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        252.80880737304688,
        386.8944396972656,
        261.78607177734375
      ],
      "text": "3.05"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        239.43252563476562,
        386.8944396972656,
        248.4097900390625
      ],
      "text": "3.10"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        226.05625915527344,
        386.8944396972656,
        235.0335235595703
      ],
      "text": "3.15"
    },
    {
      "page_no": 23,
      "bbox": [
        375.1117858886719,
        212.67999267578125,
        386.8944396972656,
        221.65725708007812
      ],
      "text": "3.20"
    },
    {
      "page_no": 23,
      "bbox": [
        365.8265380859375,
        259.5027770996094,
        375.61993408203125,
        281.80926513671875
      ],
      "text": "C4 Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        61.457000732421875,
        350.3682861328125,
        534.7349243164062,
        429.27484130859375
      ],
      "text": "Figure A1 | Grid over cosine cycle length. We show 6 curves with the cosine cycle length set to 1,\n1.1, 1.25, 1.5, 2, and 5× longer than the target number of training steps. When the cosine cycle length\nis too long, and the learning rate does not drop appropriately, then performance is impaired. We ﬁnd\nthat overestimating the number of training steps beyond 25% leads to clear drops in performance.\nWe show results where we have set the number of training steps to two diﬀerent values (top and\nbottom)."
    },
    {
      "page_no": 23,
      "bbox": [
        109.7669677734375,
        540.6680908203125,
        225.95367431640625,
        555.4124755859375
      ],
      "text": "100M\n300M\n1B\n3B\n6B\n30B\nParameters"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        536.52001953125,
        101.70475769042969,
        544.9520263671875
      ],
      "text": "2.0"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        521.472900390625,
        101.70475769042969,
        529.9049072265625
      ],
      "text": "2.2"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        506.4257507324219,
        101.70475769042969,
        514.8577270507812
      ],
      "text": "2.4"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        491.3785705566406,
        101.70475769042969,
        499.810546875
      ],
      "text": "2.6"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        476.3314208984375,
        101.70475769042969,
        484.7633972167969
      ],
      "text": "2.8"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        461.28424072265625,
        101.70475769042969,
        469.7162170410156
      ],
      "text": "3.0"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        446.2370910644531,
        101.70475769042969,
        454.6690673828125
      ],
      "text": "3.2"
    },
    {
      "page_no": 23,
      "bbox": [
        85.03465270996094,
        470.8478088378906,
        94.23318481445312,
        515.7286376953125
      ],
      "text": "C4 Training Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        122.36067962646484,
        507.2376403808594,
        134.90426635742188,
        537.5560302734375
      ],
      "text": "1e19\n1e20\n6e20\n1e21"
    },
    {
      "page_no": 23,
      "bbox": [
        239.03927612304688,
        540.524169921875,
        366.9577941894531,
        555.4124755859375
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 23,
      "bbox": [
        228.47219848632812,
        523.3731689453125,
        242.24876403808594,
        531.80517578125
      ],
      "text": "100M"
    },
    {
      "page_no": 23,
      "bbox": [
        235.67922973632812,
        504.5642395019531,
        242.25180053710938,
        512.9962158203125
      ],
      "text": "1B"
    },
    {
      "page_no": 23,
      "bbox": [
        232.51617431640625,
        485.7552795410156,
        242.250732421875,
        494.187255859375
      ],
      "text": "10B"
    },
    {
      "page_no": 23,
      "bbox": [
        229.35308837890625,
        466.9463195800781,
        242.24969482421875,
        475.3782958984375
      ],
      "text": "100B"
    },
    {
      "page_no": 23,
      "bbox": [
        236.0509033203125,
        448.1373596191406,
        242.25062561035156,
        456.5693359375
      ],
      "text": "1T"
    },
    {
      "page_no": 23,
      "bbox": [
        219.70709228515625,
        477.9794616699219,
        228.90562438964844,
        509.26312255859375
      ],
      "text": "Parameters"
    },
    {
      "page_no": 23,
      "bbox": [
        244.51052856445312,
        468.5274353027344,
        251.49703979492188,
        474.5791015625
      ],
      "text": "73B"
    },
    {
      "page_no": 23,
      "bbox": [
        379.5843811035156,
        540.524169921875,
        507.5028991699219,
        555.4124755859375
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 23,
      "bbox": [
        369.0173034667969,
        536.52001953125,
        382.7938232421875,
        544.9520263671875
      ],
      "text": "100M"
    },
    {
      "page_no": 23,
      "bbox": [
        376.2243347167969,
        517.7110595703125,
        382.7969055175781,
        526.14306640625
      ],
      "text": "1B"
    },
    {
      "page_no": 23,
      "bbox": [
        373.0612487792969,
        498.90216064453125,
        382.7958068847656,
        507.3341369628906
      ],
      "text": "10B"
    },
    {
      "page_no": 23,
      "bbox": [
        369.898193359375,
        480.09320068359375,
        382.79473876953125,
        488.5251770019531
      ],
      "text": "100B"
    },
    {
      "page_no": 23,
      "bbox": [
        376.59600830078125,
        461.28424072265625,
        382.7956848144531,
        469.7162170410156
      ],
      "text": "1T"
    },
    {
      "page_no": 23,
      "bbox": [
        373.4329528808594,
        442.47528076171875,
        382.79461669921875,
        450.9072570800781
      ],
      "text": "10T"
    },
    {
      "page_no": 23,
      "bbox": [
        360.252197265625,
        483.50238037109375,
        369.45074462890625,
        502.87567138671875
      ],
      "text": "Tokens"
    },
    {
      "page_no": 23,
      "bbox": [
        385.0556335449219,
        458.1578063964844,
        392.90924072265625,
        464.20947265625
      ],
      "text": "1.3T"
    },
    {
      "page_no": 23,
      "bbox": [
        109.7669677734375,
        653.5218505859375,
        225.95367431640625,
        668.2662353515625
      ],
      "text": "100M\n300M\n1B\n3B\n6B\n30B\nParameters"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        649.373779296875,
        101.70475769042969,
        657.8057861328125
      ],
      "text": "0.2"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        637.6181640625,
        101.70475769042969,
        646.0501708984375
      ],
      "text": "0.3"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        625.8626708984375,
        101.70475769042969,
        634.2946166992188
      ],
      "text": "0.4"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        614.1070556640625,
        101.70475769042969,
        622.5390014648438
      ],
      "text": "0.5"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        602.3514404296875,
        101.70475769042969,
        610.7833862304688
      ],
      "text": "0.6"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        590.5958251953125,
        101.70475769042969,
        599.0277709960938
      ],
      "text": "0.7"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        578.8402099609375,
        101.70475769042969,
        587.272216796875
      ],
      "text": "0.8"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        567.0845947265625,
        101.70475769042969,
        575.5166015625
      ],
      "text": "0.9"
    },
    {
      "page_no": 23,
      "bbox": [
        93.79976654052734,
        555.3289794921875,
        101.70475769042969,
        563.760986328125
      ],
      "text": "1.0"
    },
    {
      "page_no": 23,
      "bbox": [
        85.03465270996094,
        577.9161376953125,
        94.23318481445312,
        634.3602294921875
      ],
      "text": "GitHub Training Loss"
    },
    {
      "page_no": 23,
      "bbox": [
        204.06365966796875,
        561.6917724609375,
        216.60723876953125,
        592.0101318359375
      ],
      "text": "1e19\n1e20\n6e20\n1e21"
    },
    {
      "page_no": 23,
      "bbox": [
        239.03927612304688,
        653.3778686523438,
        366.9577941894531,
        668.2662353515625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 23,
      "bbox": [
        228.47219848632812,
        636.2269287109375,
        242.24876403808594,
        644.6588745117188
      ],
      "text": "100M"
    },
    {
      "page_no": 23,
      "bbox": [
        235.67922973632812,
        617.41796875,
        242.25180053710938,
        625.8499145507812
      ],
      "text": "1B"
    },
    {
      "page_no": 23,
      "bbox": [
        232.51617431640625,
        598.6090087890625,
        242.250732421875,
        607.0409545898438
      ],
      "text": "10B"
    },
    {
      "page_no": 23,
      "bbox": [
        229.35308837890625,
        579.800048828125,
        242.24969482421875,
        588.2319946289062
      ],
      "text": "100B"
    },
    {
      "page_no": 23,
      "bbox": [
        236.0509033203125,
        560.9910888671875,
        242.25062561035156,
        569.423095703125
      ],
      "text": "1T"
    },
    {
      "page_no": 23,
      "bbox": [
        219.70709228515625,
        590.8331909179688,
        228.90562438964844,
        622.1168823242188
      ],
      "text": "Parameters"
    },
    {
      "page_no": 23,
      "bbox": [
        244.51052856445312,
        583.150146484375,
        251.49703979492188,
        589.2018432617188
      ],
      "text": "59B"
    },
    {
      "page_no": 23,
      "bbox": [
        379.5843811035156,
        653.3778686523438,
        507.5028991699219,
        668.2662353515625
      ],
      "text": "1017\n1019\n1021\n1023\n1025\nFLOPs"
    },
    {
      "page_no": 23,
      "bbox": [
        369.0173034667969,
        649.373779296875,
        382.7938232421875,
        657.8057861328125
      ],
      "text": "100M"
    },
    {
      "page_no": 23,
      "bbox": [
        376.2243347167969,
        630.5648193359375,
        382.7969055175781,
        638.996826171875
      ],
      "text": "1B"
    },
    {
      "page_no": 23,
      "bbox": [
        373.0612487792969,
        611.755859375,
        382.7958068847656,
        620.1878662109375
      ],
      "text": "10B"
    },
    {
      "page_no": 23,
      "bbox": [
        369.898193359375,
        592.9468994140625,
        382.79473876953125,
        601.37890625
      ],
      "text": "100B"
    },
    {
      "page_no": 23,
      "bbox": [
        376.59600830078125,
        574.137939453125,
        382.7956848144531,
        582.5699462890625
      ],
      "text": "1T"
    },
    {
      "page_no": 23,
      "bbox": [
        373.4329528808594,
        555.3289794921875,
        382.79461669921875,
        563.760986328125
      ],
      "text": "10T"
    },
    {
      "page_no": 23,
      "bbox": [
        360.252197265625,
        596.3560791015625,
        369.45074462890625,
        615.7294311523438
      ],
      "text": "Tokens"
    },
    {
      "page_no": 23,
      "bbox": [
        385.0556335449219,
        569.2426147460938,
        392.90924072265625,
        575.2943115234375
      ],
      "text": "1.6T"
    },
    {
      "page_no": 23,
      "bbox": [
        62.36199951171875,
        679.063232421875,
        533.0972290039062,
        717.3218383789062
      ],
      "text": "Figure A2 | C4 and GitHub IsoFLOP curves. Using the C4 dataset (Raﬀel et al., 2020b) and a GitHub\ndataset (Rae et al., 2021), we generate 4 IsoFLOP proﬁles and show the parameter and token count\nscaling, as in Figure 3. Scaling coeﬃcients are shown in Table A2."
    },
    {
      "page_no": 23,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "23"
    },
    {
      "page_no": 24,
      "bbox": [
        126.48699951171875,
        87.2402114868164,
        468.2935791015625,
        100.6258544921875
      ],
      "text": "Approach\nCoef. 𝑎where 𝑁𝑜𝑝𝑡∝𝐶𝑎\nCoef. 𝑏where 𝐷𝑜𝑝𝑡∝𝐶𝑏"
    },
    {
      "page_no": 24,
      "bbox": [
        126.48699951171875,
        108.73872375488281,
        423.9017639160156,
        133.19683837890625
      ],
      "text": "C4\n0.50\n0.50\nGitHub\n0.53\n0.47"
    },
    {
      "page_no": 24,
      "bbox": [
        126.48699951171875,
        141.30970764160156,
        423.9017639160156,
        152.21881103515625
      ],
      "text": "Kaplan et al. (2020)\n0.73\n0.27"
    },
    {
      "page_no": 24,
      "bbox": [
        62.03499984741211,
        167.31631469726562,
        534.6897583007812,
        205.57586669921875
      ],
      "text": "Table A2 | Estimated parameter and data scaling with increased training compute on two al-\nternate datasets. The listed values are the exponents, 𝑎and 𝑏, on the relationship 𝑁𝑜𝑝𝑡∝𝐶𝑎and\n𝐷𝑜𝑝𝑡∝𝐶𝑏. Using IsoFLOP proﬁles, we estimate the scaling on two diﬀerent datasets."
    },
    {
      "page_no": 24,
      "bbox": [
        62.362003326416016,
        226.55703735351562,
        264.9737548828125,
        239.50843811035156
      ],
      "text": "D. Details on the scaling analyses"
    },
    {
      "page_no": 24,
      "bbox": [
        62.362003326416016,
        254.55763244628906,
        407.42779541015625,
        265.46673583984375
      ],
      "text": "D.1. Approach 1: Fixing model sizes and varying training sequences"
    },
    {
      "page_no": 24,
      "bbox": [
        61.86000061035156,
        272.62371826171875,
        534.731201171875,
        355.23883056640625
      ],
      "text": "We use a maximum learning rate of 2 × 10−4 for the smallest models and 1.25 × 10−4 for the largest\nmodels. In all cases, the learning rate drops by a factor of 10× during training, using a cosine schedule.\nWe make the assumption that the cosine cycle length should be approximately matched to the number\nof training steps. We ﬁnd that when the cosine cycle overshoots the number of training steps by more\nthan 25%, performance is noticeably degraded—see Figure A1.10 We use Gaussian smoothing with a\nwindow length of 10 steps to smooth the training curve."
    },
    {
      "page_no": 24,
      "bbox": [
        62.36199951171875,
        377.1775817871094,
        296.40582275390625,
        388.086669921875
      ],
      "text": "D.2. Approach 3: Parametric ﬁtting of the loss"
    },
    {
      "page_no": 24,
      "bbox": [
        62.36199951171875,
        399.2037353515625,
        533.12451171875,
        440.9773864746094
      ],
      "text": "In this section, we ﬁrst show how Equation (2) can be derived. We repeat the equation below for\nclarity,\nˆ𝐿(𝑁, 𝐷) ≜𝐸+ 𝐴"
    },
    {
      "page_no": 24,
      "bbox": [
        309.6940002441406,
        421.74700927734375,
        345.2583923339844,
        446.65240478515625
      ],
      "text": "𝑁𝛼+ 𝐵"
    },
    {
      "page_no": 24,
      "bbox": [
        336.3389892578125,
        429.1320495605469,
        532.91357421875,
        447.0434265136719
      ],
      "text": "𝐷𝛽,\n(5)"
    },
    {
      "page_no": 24,
      "bbox": [
        62.36199951171875,
        452.44573974609375,
        532.913330078125,
        490.4538269042969
      ],
      "text": "based on a decomposition of the expected risk between a function approximation term and an\noptimisation suboptimality term. We then give details on the optimisation procedure for ﬁtting the\nparameters."
    },
    {
      "page_no": 24,
      "bbox": [
        61.9370002746582,
        513.5250244140625,
        534.4247436523438,
        593.3678588867188
      ],
      "text": "Loss decomposition.\nFormally, we consider the task of predicting the next token 𝑦∈Y based on\nthe previous tokens in a sequence 𝑥∈Y𝑠, with 𝑠varying from 0 to 𝑠max—the maximum sequence\nlength. We consider a distribution 𝑃∈D(X × Y) of tokens in Y and their past in X. A predictor\n𝑓: X →D(Y) computes the probability of each token given the past sequence. The Bayes classiﬁer,\n𝑓★, minimizes the cross-entropy of 𝑓(𝑥) with the observed tokens 𝑦, with expectation taken on the\nwhole data distribution. We let 𝐿be the expected risk"
    },
    {
      "page_no": 24,
      "bbox": [
        159.65499877929688,
        601.292236328125,
        532.91357421875,
        624.3233032226562
      ],
      "text": "𝐿( 𝑓) ≜𝔼[log 𝑓(𝑥)𝑦],\nand set\n𝑓★≜\nargmin\n𝑓∈F(X,D(Y))\n𝐿( 𝑓).\n(6)"
    },
    {
      "page_no": 24,
      "bbox": [
        62.03499984741211,
        633.0980224609375,
        533.1953125,
        672.2938232421875
      ],
      "text": "The set of all transformers of size 𝑁, that we denote H𝑁, forms a subset of all functions that map\nsequences to distributions of tokens X →D(Y). Fitting a transformer of size 𝑁on the expected risk\n𝐿( 𝑓) amounts to minimizing such risk on a restricted functional space"
    },
    {
      "page_no": 24,
      "bbox": [
        257.48199462890625,
        681.0579223632812,
        532.91357421875,
        703.248291015625
      ],
      "text": "𝑓𝑁≜argmin\n𝑓∈H𝑁\n𝐿( 𝑓).\n(7)"
    },
    {
      "page_no": 24,
      "bbox": [
        61.86000061035156,
        712.0240478515625,
        532.9188232421875,
        739.844482421875
      ],
      "text": "When we observe a dataset (𝑥𝑖, 𝑦𝑖)𝑖𝑖∈[1,𝐷] of size 𝐷, we do not have access to 𝔼𝑃, but instead to the\nempirical expectation ˆ𝔼𝐷over the empirical distribution ˆ𝑃𝐷. What happens when we are given 𝐷"
    },
    {
      "page_no": 24,
      "bbox": [
        73.302001953125,
        747.77783203125,
        532.8797607421875,
        758.644287109375
      ],
      "text": "10This further emphasises the point of not only determining model size, but also training length before training begins."
    },
    {
      "page_no": 24,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "24"
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        87.27174377441406,
        532.916259765625,
        125.27880859375
      ],
      "text": "datapoints that we can only see once, and when we constrain the size of the hypothesis space to be\n𝑁-dimensional ? We are making steps toward minimizing the empirical risk within a ﬁnite-dimensional\nfunctional space H𝑁:"
    },
    {
      "page_no": 25,
      "bbox": [
        158.06898498535156,
        134.0148468017578,
        532.9135131835938,
        156.20523071289062
      ],
      "text": "ˆ𝐿𝐷( 𝑓) ≜ˆ𝔼𝐷[log 𝑓(𝑥)𝑦],\nsetting\nˆ𝑓𝑁,𝐷≜argmin\n𝑓∈H𝑁\nˆ𝐿𝐷( 𝑓).\n(8)"
    },
    {
      "page_no": 25,
      "bbox": [
        61.86000061035156,
        166.1077423095703,
        534.7354125976562,
        219.79083251953125
      ],
      "text": "We are never able to obtain ˆ𝑓𝑁,𝐷as we typically perform a single epoch over the dataset of size 𝐷.\nInstead, be obtain ¯𝑓𝑁,𝐷, which is the result of applying a certain number of gradient steps based on\nthe 𝐷datapoints—the number of steps to perform depends on the gradient batch size, for which we\nuse well-tested heuristics."
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        226.72921752929688,
        533.1983032226562,
        253.663818359375
      ],
      "text": "Using the Bayes-classiﬁer 𝑓★, the expected-risk minimizer 𝑓𝑁and the “single-epoch empirical-risk\nminimizer” ¯𝑓𝑁,𝐷, we can ﬁnally decompose the loss 𝐿(𝑁, 𝐷) into"
    },
    {
      "page_no": 25,
      "bbox": [
        151.08799743652344,
        256.0152893066406,
        532.91357421875,
        283.3035888671875
      ],
      "text": "𝐿(𝑁, 𝐷) ≜𝐿( ¯𝑓𝑁,𝐷) = 𝐿( 𝑓★) + \u0000\n𝐿( 𝑓𝑁) −𝐿( 𝑓★)\u0001 + \u0000\n𝐿( ¯𝑓𝑁,𝐷) −𝐿( 𝑓𝑁)\u0001\n.\n(9)"
    },
    {
      "page_no": 25,
      "bbox": [
        62.03499984741211,
        286.40869140625,
        533.0970458984375,
        351.51483154296875
      ],
      "text": "The loss comprises three terms: the Bayes risk, i.e. the minimal loss achievable for next-token\nprediction on the full distribution 𝑃, a.k.a the “entropy of natural text.”; a functional approximation\nterm that depends on the size of the hypothesis space; ﬁnally, a stochastic approximation term that\ncaptures the suboptimality of minimizing ˆ𝐿𝐷instead of 𝐿, and of making a single epoch on the provided\ndataset."
    },
    {
      "page_no": 25,
      "bbox": [
        61.457000732421875,
        375.76873779296875,
        534.7350463867188,
        454.423828125
      ],
      "text": "Expected forms of the loss terms.\nIn the decomposition (9), the second term depends entirely on\nthe number of parameters 𝑁that deﬁnes the size of the functional approximation space. On the set\nof two-layer neural networks, it is expected to be proportional to\n1\n𝑁1/2 (Siegel and Xu, 2020). Finally,\ngiven that it corresponds to early stopping in stochastic ﬁrst order methods, the third term should\nscale as the convergence rate of these methods, which is lower-bounded by\n1\n𝐷1/2 (Robbins and Monro,\n1951) (and may attain the bound). This convergence rate is expected to be dimension free (see e.g."
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        457.06475830078125,
        532.9125366210938,
        481.5228271484375
      ],
      "text": "Bubeck, 2015, for a review) and depends only on the loss smoothness; hence we assume that the\nsecond term only depends on 𝐷in (2). Empirically, we ﬁnd after ﬁtting (2) that"
    },
    {
      "page_no": 25,
      "bbox": [
        232.83599853515625,
        487.885009765625,
        313.18707275390625,
        507.4381103515625
      ],
      "text": "𝐿(𝑁, 𝐷) = 𝐸+\n𝐴"
    },
    {
      "page_no": 25,
      "bbox": [
        298.1619873046875,
        487.885009765625,
        350.21435546875,
        513.2614135742188
      ],
      "text": "𝑁0.34 +\n𝐵"
    },
    {
      "page_no": 25,
      "bbox": [
        335.85400390625,
        495.27001953125,
        532.9130249023438,
        513.2614135742188
      ],
      "text": "𝐷0.28 ,\n(10)"
    },
    {
      "page_no": 25,
      "bbox": [
        61.9370002746582,
        520.6040649414062,
        533.1224975585938,
        546.2507934570312
      ],
      "text": "with 𝐸= 1.69, 𝐴= 406.4, 𝐵= 410.7. We note that the parameter/data coeﬃcients are both lower\nthan 1"
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        535.3416748046875,
        534.7385864257812,
        559.7998657226562
      ],
      "text": "2; this is expected for the data-eﬃciency coeﬃcient (but far from the known lower-bound).\nFuture models and training approaches should endeavor to increase these coeﬃcients."
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        584.0546875,
        465.6084289550781,
        595.253662109375
      ],
      "text": "Fitting the decomposition to data.\nWe eﬀectively minimize the following problem"
    },
    {
      "page_no": 25,
      "bbox": [
        159.3889923095703,
        608.398681640625,
        187.61911010742188,
        624.4246826171875
      ],
      "text": "min\n𝑎,𝑏,𝑒,𝛼,𝛽"
    },
    {
      "page_no": 25,
      "bbox": [
        191.7869873046875,
        605.0494384765625,
        206.2197265625,
        615.9585571289062
      ],
      "text": "∑︁"
    },
    {
      "page_no": 25,
      "bbox": [
        189.7679901123047,
        599.832275390625,
        532.9130249023438,
        631.5170288085938
      ],
      "text": "Run 𝑖\nHuber𝛿\n\u0010\nLSE\u0000\n𝑎−𝛼log 𝑁𝑖, 𝑏−𝛽log 𝐷𝑖, 𝑒\u0001 −log 𝐿𝑖\n\u0011\n,\n(11)"
    },
    {
      "page_no": 25,
      "bbox": [
        61.93701171875,
        638.5420532226562,
        454.979736328125,
        650.7101440429688
      ],
      "text": "where 𝐿𝑆𝐸is the log-sum-exp operator. We then set 𝐴, 𝐵, 𝐸= exp(𝑎), exp(𝑏), exp(𝑒)."
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        660.0537109375,
        532.916259765625,
        711.6098022460938
      ],
      "text": "We use the LBFGS algorithm to ﬁnd local minima of the objective above, started on a grid\nof initialisation given by: 𝛼∈{0., 0.5, . . . , 2.}, 𝛽∈{0., 0.5, . . . , 2.}, 𝑒∈{−1., −.5, . . . , 1.}, 𝑎∈\n{0, 5, . . . , 25}, and 𝑏∈{0, 5, . . . , 25}. We ﬁnd that the optimal initialisation is not on the boundary of\nour initialisation sweep."
    },
    {
      "page_no": 25,
      "bbox": [
        62.36199951171875,
        717.064697265625,
        533.1016235351562,
        759.0328369140625
      ],
      "text": "We use 𝛿= 10−3 for the Huber loss. We ﬁnd that using larger values of 𝛿pushes the model to\noverﬁt the small compute regime and poorly predict held-out data from larger runs. We ﬁnd that\nusing a 𝛿smaller than 10−3 does not impact the resulting predictions."
    },
    {
      "page_no": 25,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "25"
    },
    {
      "page_no": 26,
      "bbox": [
        62.36199951171875,
        87.56160736083984,
        375.6387634277344,
        98.470703125
      ],
      "text": "D.3. Predicted compute optimal frontier for all three methods"
    },
    {
      "page_no": 26,
      "bbox": [
        62.0890007019043,
        109.58772277832031,
        532.9169311523438,
        147.5948486328125
      ],
      "text": "For Approaches 2 and 3, we show the estimated model size and number of training tokens for a\nvariety of compute budgets in Table A3. We plot the predicted number of tokens and parameters for a\nvariety of FLOP budgets for the three methods in Figure A3."
    },
    {
      "page_no": 26,
      "bbox": [
        232.63400268554688,
        164.41273498535156,
        426.8377685546875,
        175.32183837890625
      ],
      "text": "Approach 2\nApproach 3"
    },
    {
      "page_no": 26,
      "bbox": [
        132.2969970703125,
        183.43470764160156,
        464.69354248046875,
        194.34381103515625
      ],
      "text": "Parameters\nFLOPs\nTokens\nFLOPs\nTokens"
    },
    {
      "page_no": 26,
      "bbox": [
        130.58399963378906,
        202.4567413330078,
        464.69342041015625,
        321.75982666015625
      ],
      "text": "400 Million\n1.84e+19\n7.7 Billion\n2.21e+19\n9.2 Billion\n1 Billion\n1.20e+20\n20.0 Billion\n1.62e+20\n27.1 Billion\n10 Billion\n1.32e+22\n219.5 Billion\n2.46e+22\n410.1 Billion\n67 Billion\n6.88e+23\n1.7 Trillion\n1.71e+24\n4.1 Trillion\n175 Billion\n4.54e+24\n4.3 Trillion\n1.26e+24\n12.0 Trillion\n280 Billion\n1.18e+25\n7.1 Trillion\n3.52e+25\n20.1 Trillion\n520 Billion\n4.19e+25\n13.4 Trillion\n1.36e+26\n43.5 Trillion\n1 Trillion\n1.59e+26\n26.5 Trillion\n5.65e+26\n94.1 Trillion\n10 Trillion\n1.75e+28\n292.0 Trillion\n8.55e+28\n1425.5 Trillion"
    },
    {
      "page_no": 26,
      "bbox": [
        62.03499984741211,
        336.8572998046875,
        534.652099609375,
        375.1158447265625
      ],
      "text": "Table A3 | Estimated optimal training FLOPs and training tokens for various model sizes. Analo-\ngous to Table 3, we show the model size/token count projections from Approaches 2 and 3 for various\ncompute budgets."
    },
    {
      "page_no": 26,
      "bbox": [
        294.3529968261719,
        377.7557373046875,
        297.3857116699219,
        388.66485595703125
      ],
      "text": "."
    },
    {
      "page_no": 26,
      "bbox": [
        240.55419921875,
        608.60400390625,
        380.2422180175781,
        626.494873046875
      ],
      "text": "1010\n1011\n1012\n1013\nTokens"
    },
    {
      "page_no": 26,
      "bbox": [
        192.3522491455078,
        592.75830078125,
        202.6316680908203,
        602.9298706054688
      ],
      "text": "108"
    },
    {
      "page_no": 26,
      "bbox": [
        192.3522491455078,
        548.0006713867188,
        202.6316680908203,
        558.1722412109375
      ],
      "text": "109"
    },
    {
      "page_no": 26,
      "bbox": [
        189.64675903320312,
        503.2430725097656,
        202.5765838623047,
        513.4146728515625
      ],
      "text": "1010"
    },
    {
      "page_no": 26,
      "bbox": [
        189.64675903320312,
        458.432861328125,
        202.5765838623047,
        468.60443115234375
      ],
      "text": "1011"
    },
    {
      "page_no": 26,
      "bbox": [
        189.64675903320312,
        413.72784423828125,
        202.5765838623047,
        423.8994140625
      ],
      "text": "1012"
    },
    {
      "page_no": 26,
      "bbox": [
        179.09019470214844,
        491.8052978515625,
        190.10263061523438,
        529.2579345703125
      ],
      "text": "Parameters"
    },
    {
      "page_no": 26,
      "bbox": [
        221.55856323242188,
        593.8637084960938,
        243.3821258544922,
        604.8761596679688
      ],
      "text": "1e+18"
    },
    {
      "page_no": 26,
      "bbox": [
        242.77499389648438,
        571.580322265625,
        264.59857177734375,
        582.5927734375
      ],
      "text": "1e+19"
    },
    {
      "page_no": 26,
      "bbox": [
        263.9914245605469,
        549.296875,
        285.81500244140625,
        560.309326171875
      ],
      "text": "1e+20"
    },
    {
      "page_no": 26,
      "bbox": [
        285.2078552246094,
        527.0134887695312,
        307.03143310546875,
        538.0259399414062
      ],
      "text": "1e+21"
    },
    {
      "page_no": 26,
      "bbox": [
        306.4242858886719,
        504.7301330566406,
        328.24786376953125,
        515.7425537109375
      ],
      "text": "1e+22"
    },
    {
      "page_no": 26,
      "bbox": [
        327.6407165527344,
        482.4467468261719,
        349.4643249511719,
        493.45916748046875
      ],
      "text": "1e+23"
    },
    {
      "page_no": 26,
      "bbox": [
        348.857177734375,
        460.1632995605469,
        370.6807556152344,
        471.17572021484375
      ],
      "text": "1e+24"
    },
    {
      "page_no": 26,
      "bbox": [
        370.0735778808594,
        437.8799133300781,
        391.89715576171875,
        448.892333984375
      ],
      "text": "1e+25"
    },
    {
      "page_no": 26,
      "bbox": [
        227.75209045410156,
        415.1824645996094,
        413.1136169433594,
        477.71295166015625
      ],
      "text": "1e+26\nApproach 1\nApproach 2\nApproach 3\nChinchilla\nGopher\nGPT-3\nMegatron-Turing NLG"
    },
    {
      "page_no": 26,
      "bbox": [
        62.05699920654297,
        637.46923828125,
        534.4337158203125,
        675.7288208007812
      ],
      "text": "Figure A3 | Optimal number of tokens and parameters for a training FLOP budget. For a ﬁxed\nFLOP budget, we show the optimal number of tokens and parameters as predicted by Approaches 1,\n2, and 3. For an alternate representation, see Figure 1."
    },
    {
      "page_no": 26,
      "bbox": [
        62.36199951171875,
        712.1649169921875,
        325.0579833984375,
        723.1126708984375
      ],
      "text": "D.4. Small-scale comparison to Kaplan et al. (2020)"
    },
    {
      "page_no": 26,
      "bbox": [
        62.36199951171875,
        730.270751953125,
        532.915283203125,
        758.6888427734375
      ],
      "text": "For 1021 FLOPs, we perform a head-to-head comparison of a model predicted by Approach 1 and\nthat predicted by Kaplan et al. (2020). For both models, we use a batch size of 0.5M tokens and a"
    },
    {
      "page_no": 26,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "26"
    },
    {
      "page_no": 27,
      "bbox": [
        62.36199951171875,
        83.31172180175781,
        533.1822509765625,
        165.92584228515625
      ],
      "text": "maximum learning rate of 1.5 × 10−4 that decays by 10×. From Kaplan et al. (2020), we ﬁnd that\nthe optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86\nbillion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion\nparameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many\nconfounding factors as possible. We ﬁnd that our predicted model outperforms the model predicted\nby Kaplan et al. (2020) as shown in Figure A4."
    },
    {
      "page_no": 27,
      "bbox": [
        124.98966979980469,
        371.8385009765625,
        295.5874938964844,
        399.9793395996094
      ],
      "text": "0\n1\n2\nSequences\n1e7"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        364.1125183105469,
        115.91966247558594,
        380.2536926269531
      ],
      "text": "2.2"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        332.7576904296875,
        115.91966247558594,
        348.89886474609375
      ],
      "text": "2.3"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        301.40283203125,
        115.91966247558594,
        317.54400634765625
      ],
      "text": "2.4"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        270.0479736328125,
        115.91966247558594,
        286.18914794921875
      ],
      "text": "2.5"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        238.69314575195312,
        115.91966247558594,
        254.83432006835938
      ],
      "text": "2.6"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        207.33828735351562,
        115.91966247558594,
        223.47946166992188
      ],
      "text": "2.7"
    },
    {
      "page_no": 27,
      "bbox": [
        100.78730773925781,
        175.9834442138672,
        115.91966247558594,
        192.12461853027344
      ],
      "text": "2.8"
    },
    {
      "page_no": 27,
      "bbox": [
        84.0924072265625,
        242.94383239746094,
        101.70095825195312,
        311.7064514160156
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 27,
      "bbox": [
        331.1156921386719,
        371.8385009765625,
        505.8452453613281,
        399.83062744140625
      ],
      "text": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFLOPs ×1021"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        364.1125183105469,
        326.5879821777344,
        380.2536926269531
      ],
      "text": "2.2"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        332.7576904296875,
        326.5879821777344,
        348.89886474609375
      ],
      "text": "2.3"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        301.40283203125,
        326.5879821777344,
        317.54400634765625
      ],
      "text": "2.4"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        270.0479736328125,
        326.5879821777344,
        286.18914794921875
      ],
      "text": "2.5"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        238.69314575195312,
        326.5879821777344,
        254.83432006835938
      ],
      "text": "2.6"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        207.33828735351562,
        326.5879821777344,
        223.47946166992188
      ],
      "text": "2.7"
    },
    {
      "page_no": 27,
      "bbox": [
        311.45562744140625,
        175.9834442138672,
        326.5879821777344,
        192.12461853027344
      ],
      "text": "2.8"
    },
    {
      "page_no": 27,
      "bbox": [
        294.7607421875,
        242.94383239746094,
        312.3692932128906,
        311.7064514160156
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 27,
      "bbox": [
        406.44622802734375,
        188.16519165039062,
        497.8589782714844,
        218.2779998779297
      ],
      "text": "Kaplan et al (2020)\nApproach 1"
    },
    {
      "page_no": 27,
      "bbox": [
        61.457000732421875,
        407.5006103515625,
        534.732177734375,
        449.1778259277344
      ],
      "text": "Figure A4 | Comparison to Kaplan et al. (2020) at 1021 FLOPs. We train 2.80 and 4.74 billion\nparameter transformers predicted as optimal for 1021 FLOPs by Approach 1 and by Kaplan et al.\n(2020). We ﬁnd that our prediction results in a more performant model at the end of training."
    },
    {
      "page_no": 27,
      "bbox": [
        62.36199951171875,
        485.3559875488281,
        288.169677734375,
        498.3074035644531
      ],
      "text": "E. Curvature of the FLOP-loss frontier"
    },
    {
      "page_no": 27,
      "bbox": [
        61.86000061035156,
        511.95068359375,
        534.7399291992188,
        577.0568237304688
      ],
      "text": "We observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means\nthat projections from very small models lead to diﬀerent predictions than those from larger models.\nIn Figure A5 we show linear ﬁts using the ﬁrst, middle, and ﬁnal third of frontier-points. In this work,\nwe do not take this in to account and we leave this as interesting future work as it suggests that even\nsmaller models may be optimal for large FLOP budgets."
    },
    {
      "page_no": 27,
      "bbox": [
        62.36199951171875,
        600.2780151367188,
        195.1915740966797,
        613.2293701171875
      ],
      "text": "F. FLOPs computation"
    },
    {
      "page_no": 27,
      "bbox": [
        61.86000061035156,
        626.8727416992188,
        533.1228637695312,
        678.4288330078125
      ],
      "text": "We include all training FLOPs, including those contributed to by the embedding matrices, in our\nanalysis. Note that we also count embeddings matrices in the total parameter count. For large models\nthe FLOP and parameter contribution of embedding matrices is small. We use a factor of 2 to describe\nthe multiply accumulate cost. For the forward pass, we consider contributions from:"
    },
    {
      "page_no": 27,
      "bbox": [
        77.74400329589844,
        695.0697021484375,
        147.99868774414062,
        705.9788208007812
      ],
      "text": "• Embeddings"
    },
    {
      "page_no": 27,
      "bbox": [
        102.72599792480469,
        712.5032348632812,
        282.11834716796875,
        723.9536743164062
      ],
      "text": "– 2 × seq_len × vocab_size × d_model"
    },
    {
      "page_no": 27,
      "bbox": [
        77.74400329589844,
        730.438720703125,
        204.4641876220703,
        741.3478393554688
      ],
      "text": "• Attention (Single Layer)"
    },
    {
      "page_no": 27,
      "bbox": [
        102.72599792480469,
        747.8722534179688,
        532.3677368164062,
        759.3226928710938
      ],
      "text": "– Key, query and value projections: 2 × 3 × seq_len × d_model × (key_size × num_heads)"
    },
    {
      "page_no": 27,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "27"
    },
    {
      "page_no": 28,
      "bbox": [
        206.7480926513672,
        277.55194091796875,
        348.72528076171875,
        288.8998718261719
      ],
      "text": "1017\n1018\n1019\n1020\n1021\n1022"
    },
    {
      "page_no": 28,
      "bbox": [
        271.7008361816406,
        285.5689697265625,
        294.4476013183594,
        297.7889709472656
      ],
      "text": "FLOPS"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        259.5462646484375,
        202.281005859375,
        270.7479248046875
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        226.42652893066406,
        202.281005859375,
        237.62818908691406
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        199.36573791503906,
        202.281005859375,
        210.56739807128906
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        176.48617553710938,
        202.281005859375,
        187.68783569335938
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        156.66697692871094,
        202.281005859375,
        167.86863708496094
      ],
      "text": "4.0"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        139.1852264404297,
        202.281005859375,
        150.3868865966797
      ],
      "text": "4.5"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        123.54725646972656,
        202.281005859375,
        134.74891662597656
      ],
      "text": "5.0"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        109.40098571777344,
        202.281005859375,
        120.60264587402344
      ],
      "text": "5.5"
    },
    {
      "page_no": 28,
      "bbox": [
        191.7794647216797,
        96.4864501953125,
        202.281005859375,
        107.6881103515625
      ],
      "text": "6.0"
    },
    {
      "page_no": 28,
      "bbox": [
        179.67886352539062,
        160.10140991210938,
        191.8988494873047,
        205.81112670898438
      ],
      "text": "Training loss"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        270.5084533691406,
        390.89007568359375,
        281.7101135253906
      ],
      "text": "75"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        224.0681610107422,
        395.0906982421875,
        235.2698211669922
      ],
      "text": "250"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        197.3317108154297,
        395.0906982421875,
        208.5333709716797
      ],
      "text": "500"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        170.59527587890625,
        399.29132080078125,
        181.79693603515625
      ],
      "text": "1000"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        135.2516326904297,
        399.29132080078125,
        146.4532928466797
      ],
      "text": "2500"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        108.51519775390625,
        399.29132080078125,
        119.71685791015625
      ],
      "text": "5000"
    },
    {
      "page_no": 28,
      "bbox": [
        382.48883056640625,
        84.99497985839844,
        403.491943359375,
        96.19664001464844
      ],
      "text": "10000"
    },
    {
      "page_no": 28,
      "bbox": [
        401.8036804199219,
        149.8061981201172,
        414.023681640625,
        216.86483764648438
      ],
      "text": "Million Parameters"
    },
    {
      "page_no": 28,
      "bbox": [
        62.36199951171875,
        309.5102844238281,
        534.425537109375,
        334.2208251953125
      ],
      "text": "Figure A5 | Training curve envelopes. We ﬁt to the ﬁrst third (orange), the middle third (green),\nand the last third (blue) of all points along the loss frontier. We plot only a subset of the points."
    },
    {
      "page_no": 28,
      "bbox": [
        102.72599792480469,
        356.68927001953125,
        489.6607360839844,
        408.7876892089844
      ],
      "text": "– Key @ Query logits: 2 × seq_len × seq_len × (key_size × num_heads)\n– Softmax: 3 × num_heads × seq_len × seq_len\n– Softmax @ query reductions: 2 × seq_len × seq_len × (key_size × num_heads)\n– Final Linear: 2 × seq_len × (key_size × num_heads) × d_model"
    },
    {
      "page_no": 28,
      "bbox": [
        77.7440185546875,
        415.29071044921875,
        218.02423095703125,
        426.1998291015625
      ],
      "text": "• Dense Block (Single Layer)"
    },
    {
      "page_no": 28,
      "bbox": [
        102.72601318359375,
        432.7422790527344,
        382.3387145996094,
        444.19268798828125
      ],
      "text": "– 2 × seq_len × (d_model × ﬀw_size + d_model × ﬀw_size)"
    },
    {
      "page_no": 28,
      "bbox": [
        77.74398803710938,
        450.69573974609375,
        144.86776733398438,
        461.6048278808594
      ],
      "text": "• Final Logits"
    },
    {
      "page_no": 28,
      "bbox": [
        102.72598266601562,
        468.14727783203125,
        282.11859130859375,
        479.5976867675781
      ],
      "text": "– 2 × seq_len × d_model × vocab_size"
    },
    {
      "page_no": 28,
      "bbox": [
        77.74398803710938,
        485.8492736816406,
        532.9135131835938,
        497.2996826171875
      ],
      "text": "• Total forward pass FLOPs: embeddings+num_layers×(total_attention+dense_block) + logits"
    },
    {
      "page_no": 28,
      "bbox": [
        61.457000732421875,
        513.74169921875,
        534.733154296875,
        578.8478393554688
      ],
      "text": "As in Kaplan et al. (2020) we assume that the backward pass has twice the FLOPs of the forward pass.\nWe show a comparison between our calculation and that using the common approximation 𝐶= 6𝐷𝑁\n(Kaplan et al., 2020) where 𝐶is FLOPs, 𝐷is the number of training tokens, and 𝑁is the number of\nparameters in Table A4. We ﬁnd the diﬀerences in FLOP calculation to be very small and they do not\nimpact our analysis. Compared to the results presented in Rae et al. (2021), we use a slightly more"
    },
    {
      "page_no": 28,
      "bbox": [
        68.33999633789062,
        593.8180541992188,
        534.9680786132812,
        605.9148559570312
      ],
      "text": "Parameters\nnum_layers\nd_model\nﬀw_size\nnum_heads\nk/q size\nFLOP Ratio (Ours/6𝑁𝐷)"
    },
    {
      "page_no": 28,
      "bbox": [
        81.0,
        614.0277099609375,
        488.4490966796875,
        692.6828002929688
      ],
      "text": "73M\n10\n640\n2560\n10\n64\n1.03\n305M\n20\n1024\n4096\n16\n64\n1.10\n552M\n24\n1280\n5120\n10\n128\n1.08\n1.1B\n26\n1792\n7168\n14\n128\n1.04\n1.6B\n28\n2048\n8192\n16\n128\n1.03\n6.8B\n40\n3584\n14336\n28\n128\n0.99"
    },
    {
      "page_no": 28,
      "bbox": [
        62.03499984741211,
        707.7802734375,
        532.9163818359375,
        732.4898071289062
      ],
      "text": "Table A4 | FLOP comparison. For a variety of diﬀerent model sizes, we show the ratio of the FLOPs\nthat we compute per sequence to that using the 6𝑁𝐷approximation."
    },
    {
      "page_no": 28,
      "bbox": [
        62.36198425292969,
        744.1636962890625,
        490.96875,
        759.0328369140625
      ],
      "text": "accurate calculation giving a slightly diﬀerent value (6.3 × 1023 compared to 5.76 × 1023)."
    },
    {
      "page_no": 28,
      "bbox": [
        291.5719299316406,
        778.01171875,
        303.7028503417969,
        788.9208374023438
      ],
      "text": "28"
    },
    {
      "page_no": 29,
      "bbox": [
        62.36199951171875,
        85.93616485595703,
        377.6518249511719,
        98.93342590332031
      ],
      "text": "G. Other diﬀerences between Chinchilla and Gopher"
    },
    {
      "page_no": 29,
      "bbox": [
        61.9370002746582,
        112.57673645019531,
        534.4267578125,
        177.68182373046875
      ],
      "text": "Beyond diﬀerences in model size and number of training tokens, there are some additional minor\ndiﬀerences between Chinchilla and Gopher. Speciﬁcally, Gopher was trained with Adam (Kingma and\nBa, 2014) whereas Chinchilla was trained with AdamW (Loshchilov and Hutter, 2019). Furthermore,\nas discussed in Lessons Learned in Rae et al. (2021), Chinchilla stored a higher-precision copy of the\nweights in the sharded optimiser state."
    },
    {
      "page_no": 29,
      "bbox": [
        61.86000061035156,
        187.0967559814453,
        534.738525390625,
        225.1048583984375
      ],
      "text": "We show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7.\nWe ﬁnd that, independent of the learning rate schedule, AdamW trained models outperform models\ntrained with Adam. In Figure A6 we show a comparison of an 680 million parameter model trained"
    },
    {
      "page_no": 29,
      "bbox": [
        111.74555969238281,
        344.54473876953125,
        221.09181213378906,
        360.0722961425781
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        340.2817077636719,
        105.80010223388672,
        349.1880798339844
      ],
      "text": "2.45"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        319.5205383300781,
        105.80010223388672,
        328.4269104003906
      ],
      "text": "2.50"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        298.7593688964844,
        105.80010223388672,
        307.6657409667969
      ],
      "text": "2.55"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        277.9981994628906,
        105.80010223388672,
        286.9045715332031
      ],
      "text": "2.60"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        257.2370300292969,
        105.80010223388672,
        266.1434020996094
      ],
      "text": "2.65"
    },
    {
      "page_no": 29,
      "bbox": [
        94.11048126220703,
        236.47586059570312,
        105.80010223388672,
        245.38223266601562
      ],
      "text": "2.70"
    },
    {
      "page_no": 29,
      "bbox": [
        84.89856719970703,
        273.4232482910156,
        94.61461639404297,
        311.3650817871094
      ],
      "text": "Training Loss"
    },
    {
      "page_no": 29,
      "bbox": [
        252.775146484375,
        344.54473876953125,
        363.4412536621094,
        360.0722961425781
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        340.2817077636719,
        246.83253479003906,
        349.1880798339844
      ],
      "text": "17"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        328.7477111816406,
        246.83253479003906,
        337.6540832519531
      ],
      "text": "18"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        317.2137451171875,
        246.83253479003906,
        326.1201171875
      ],
      "text": "19"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        305.67974853515625,
        246.83253479003906,
        314.58612060546875
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        294.1457824707031,
        246.83253479003906,
        303.0521545410156
      ],
      "text": "21"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        282.6117858886719,
        246.83253479003906,
        291.5181579589844
      ],
      "text": "22"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        271.07781982421875,
        246.83253479003906,
        279.98419189453125
      ],
      "text": "23"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        259.5438232421875,
        246.83253479003906,
        268.4501953125
      ],
      "text": "24"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        248.00982666015625,
        246.83253479003906,
        256.91619873046875
      ],
      "text": "25"
    },
    {
      "page_no": 29,
      "bbox": [
        240.15274047851562,
        236.47586059570312,
        246.83253479003906,
        245.38223266601562
      ],
      "text": "26"
    },
    {
      "page_no": 29,
      "bbox": [
        230.9408416748047,
        260.08868408203125,
        240.65689086914062,
        325.1220397949219
      ],
      "text": "Wikitext103 Perplexity"
    },
    {
      "page_no": 29,
      "bbox": [
        393.8046875,
        344.54473876953125,
        504.4708251953125,
        360.0722961425781
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        340.2817077636719,
        387.8592224121094,
        349.1880798339844
      ],
      "text": "2.60"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        327.30596923828125,
        387.8592224121094,
        336.21234130859375
      ],
      "text": "2.65"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        314.33026123046875,
        387.8592224121094,
        323.23663330078125
      ],
      "text": "2.70"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        301.3545227050781,
        387.8592224121094,
        310.2608947753906
      ],
      "text": "2.75"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        288.3787841796875,
        387.8592224121094,
        297.28515625
      ],
      "text": "2.80"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        275.4030456542969,
        387.8592224121094,
        284.3094177246094
      ],
      "text": "2.85"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        262.42730712890625,
        387.8592224121094,
        271.33367919921875
      ],
      "text": "2.90"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        249.4515838623047,
        387.8592224121094,
        258.35797119140625
      ],
      "text": "2.95"
    },
    {
      "page_no": 29,
      "bbox": [
        376.16961669921875,
        236.47586059570312,
        387.8592224121094,
        245.38223266601562
      ],
      "text": "3.00"
    },
    {
      "page_no": 29,
      "bbox": [
        366.9577331542969,
        281.7492370605469,
        376.67376708984375,
        303.8796081542969
      ],
      "text": "C4 Loss"
    },
    {
      "page_no": 29,
      "bbox": [
        434.9136657714844,
        242.9706268310547,
        503.23431396484375,
        283.4034118652344
      ],
      "text": "Training Setup\nAdam w/ High Precision\nAdamW w/ High Precision\nAdam No High Precision\nAdamW No High Precision"
    },
    {
      "page_no": 29,
      "bbox": [
        61.457000732421875,
        370.80029296875,
        532.9182739257812,
        422.95245361328125
      ],
      "text": "Figure A6 | Comparison of other diﬀerences. Using an 680 million parameter model, we show a\ncomparison between the setup used to train Gopher and Chinchilla— the change in optimiser and\nusing a higher precision copy of the weights in the optimiser state. The setup used for Chinchilla\n(orange) clearly outperforms the setup used to train Gopher (green)."
    },
    {
      "page_no": 29,
      "bbox": [
        102.9941177368164,
        555.282958984375,
        226.70753479003906,
        570.7493286132812
      ],
      "text": "0\n25\n50\n75\n100\n125\n150\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        551.0367431640625,
        102.39495849609375,
        559.9080200195312
      ],
      "text": "2.3"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        530.3572998046875,
        102.39495849609375,
        539.2285766601562
      ],
      "text": "2.4"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        509.67791748046875,
        102.39495849609375,
        518.5491943359375
      ],
      "text": "2.5"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        488.99847412109375,
        102.39495849609375,
        497.8697814941406
      ],
      "text": "2.6"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        468.319091796875,
        102.39495849609375,
        477.19036865234375
      ],
      "text": "2.7"
    },
    {
      "page_no": 29,
      "bbox": [
        94.07811737060547,
        447.6396484375,
        102.39495849609375,
        456.5109558105469
      ],
      "text": "2.8"
    },
    {
      "page_no": 29,
      "bbox": [
        84.90248107910156,
        492.7347412109375,
        94.58026885986328,
        514.7779541015625
      ],
      "text": "C4 Loss"
    },
    {
      "page_no": 29,
      "bbox": [
        243.46832275390625,
        555.282958984375,
        367.18170166015625,
        570.7493286132812
      ],
      "text": "0\n25\n50\n75\n100\n125\n150\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        551.0367431640625,
        242.8672637939453,
        559.9080200195312
      ],
      "text": "10.0"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        538.1121215820312,
        242.8672637939453,
        546.9833984375
      ],
      "text": "12.5"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        525.1875,
        242.8672637939453,
        534.0587768554688
      ],
      "text": "15.0"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        512.2628173828125,
        242.8672637939453,
        521.1340942382812
      ],
      "text": "17.5"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        499.33819580078125,
        242.8672637939453,
        508.20947265625
      ],
      "text": "20.0"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        486.41357421875,
        242.8672637939453,
        495.28485107421875
      ],
      "text": "22.5"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        473.48895263671875,
        242.8672637939453,
        482.3602294921875
      ],
      "text": "25.0"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        460.56427001953125,
        242.8672637939453,
        469.4355773925781
      ],
      "text": "27.5"
    },
    {
      "page_no": 29,
      "bbox": [
        231.22366333007812,
        447.6396484375,
        242.8672637939453,
        456.5109558105469
      ],
      "text": "30.0"
    },
    {
      "page_no": 29,
      "bbox": [
        222.04803466796875,
        471.1595153808594,
        231.72581481933594,
        535.936767578125
      ],
      "text": "Wikitext103 Perplexity"
    },
    {
      "page_no": 29,
      "bbox": [
        383.9425048828125,
        555.282958984375,
        507.6559143066406,
        570.7493286132812
      ],
      "text": "0\n25\n50\n75\n100\n125\n150\nMillion Sequences"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        546.3368530273438,
        383.3433532714844,
        555.2081298828125
      ],
      "text": "0.0"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        531.604736328125,
        383.3433532714844,
        540.4760131835938
      ],
      "text": "0.1"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        516.8726806640625,
        383.3433532714844,
        525.7439575195312
      ],
      "text": "0.2"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        502.14056396484375,
        383.3433532714844,
        511.0118408203125
      ],
      "text": "0.3"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        487.408447265625,
        383.3433532714844,
        496.2797546386719
      ],
      "text": "0.4"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        472.67633056640625,
        383.3433532714844,
        481.5476379394531
      ],
      "text": "0.5"
    },
    {
      "page_no": 29,
      "bbox": [
        375.0265197753906,
        457.94427490234375,
        383.3433532714844,
        466.8155517578125
      ],
      "text": "0.6"
    },
    {
      "page_no": 29,
      "bbox": [
        365.85089111328125,
        475.7281799316406,
        375.5286865234375,
        531.820556640625
      ],
      "text": "LAMBADA Accuracy"
    },
    {
      "page_no": 29,
      "bbox": [
        404.96051025390625,
        520.2163696289062,
        443.1551513671875,
        552.1243286132812
      ],
      "text": "417M, Adam\n417M, AdamW\n1.4B, Adam\n1.4B, AdamW"
    },
    {
      "page_no": 29,
      "bbox": [
        62.01300048828125,
        581.4762573242188,
        532.9094848632812,
        606.1868286132812
      ],
      "text": "Figure A7 | Adam vs AdamW. For a 417M (blue) and 1.4B model (green), we ﬁnd that training with\nAdamW improves performance over training with Adam."
    },
    {
      "page_no": 29,
      "bbox": [
        61.9370002746582,
        622.478759765625,
        533.0572509765625,
        633.3878784179688
      ],
      "text": "with and without the higher precision copy of the weights and with Adam/AdamW for comparison."
    },
    {
      "page_no": 29,
      "bbox": [
        62.36199951171875,
        657.654052734375,
        125.30581665039062,
        670.6054077148438
      ],
      "text": "H. Results"
    },
    {
      "page_no": 29,
      "bbox": [
        62.36199951171875,
        685.6546020507812,
        129.47479248046875,
        696.563720703125
      ],
      "text": "H.1. The Pile"
    },
    {
      "page_no": 29,
      "bbox": [
        62.36199951171875,
        707.6807250976562,
        534.4293823242188,
        746.7508544921875
      ],
      "text": "In Table A5 we show the bits-per-byte (bpb) on The Pile (Gao et al., 2020) of Chinchilla, Gopher,\nand Jurassic-1. Chinchilla outperforms Gopher on all subsets. Jurassic-1 outperforms Chinchilla on 2\nsubsets— dm_mathematics and ubuntu_irc."
    },
    {
      "page_no": 29,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "29"
    },
    {
      "page_no": 30,
      "bbox": [
        118.0,
        89.71675109863281,
        477.27740478515625,
        100.97047424316406
      ],
      "text": "Subset\nChinchilla (70B)\nGopher (280B)\nJurassic-1 (170B)"
    },
    {
      "page_no": 30,
      "bbox": [
        117.99996948242188,
        108.73872375488281,
        477.2772521972656,
        363.82281494140625
      ],
      "text": "pile_cc\n0.667\n0.691\n0.669\npubmed_abstracts\n0.559\n0.578\n0.587\nstackexchange\n0.614\n0.641\n0.655\ngithub\n0.337\n0.377\n0.358\nopenwebtext2\n0.647\n0.677\n-\narxiv\n0.627\n0.662\n0.680\nuspto_backgrounds\n0.526\n0.546\n0.537\nfreelaw\n0.476\n0.513\n0.514\npubmed_central\n0.504\n0.525\n0.579\ndm_mathematics\n1.111\n1.142\n1.037\nhackernews\n0.859\n0.890\n0.869\nnih_exporter\n0.572\n0.590\n0.590\nopensubtitles\n0.871\n0.900\n0.879\neuroparl\n0.833\n0.938\n-\nbooks3\n0.675\n0.712\n0.835\nphilpapers\n0.656\n0.695\n0.742\ngutenberg_pg_19\n0.548\n0.656\n0.890\nbookcorpus2\n0.714\n0.741\n-\nubuntu_irc\n1.026\n1.090\n0.857"
    },
    {
      "page_no": 30,
      "bbox": [
        62.03499984741211,
        378.6302795410156,
        532.9137573242188,
        403.33984375
      ],
      "text": "Table A5 | Bits-per-Byte on The Pile. We show the bpb on The Pile for Chinchilla compared to Gopher\nand Jurassic-1."
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        426.0126037597656,
        121.41297149658203,
        436.92169189453125
      ],
      "text": "H.2. MMLU"
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        448.03875732421875,
        483.0699157714844,
        459.2924499511719
      ],
      "text": "In Table A6 we show the performance of Chinchilla and Gopher on each subset of MMLU."
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        480.9425964355469,
        182.17665100097656,
        491.8516845703125
      ],
      "text": "H.3. Winogender Setup"
    },
    {
      "page_no": 30,
      "bbox": [
        61.04199981689453,
        502.96875,
        532.9183959960938,
        608.7218627929688
      ],
      "text": "We follow the same setup as in Rae et al. (2021). To test coreference resolution in Chinchilla, we\ninput a sentence which includes a pronoun reference (e.g., “The librarian helped the child pick out a\nbook because {pronoun} liked to encourage reading.”), then measure the probability of the model\ncompleting the sentence “‘{Pronoun}’ refers to the” with diﬀerent sentence roles (“librarian” and\n“child” in this example). Each example is annotated with the correct pronoun resolution (the pronoun\ncorresponds to the librarian in this example). Each sentence is tested with a female, male, and\ngender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers\nto regardless of pronoun gender."
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        630.716552734375,
        141.01661682128906,
        641.6256713867188
      ],
      "text": "H.4. BIG-bench"
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        652.7427368164062,
        534.7302856445312,
        663.9710693359375
      ],
      "text": "In Table A7 we show Chinchilla and Gopher performance on each subset of BIG-bench that we consider."
    },
    {
      "page_no": 30,
      "bbox": [
        62.36199951171875,
        686.674072265625,
        146.76629638671875,
        699.6254272460938
      ],
      "text": "I. Model Card"
    },
    {
      "page_no": 30,
      "bbox": [
        61.86000061035156,
        713.2687377929688,
        532.91552734375,
        737.7268676757812
      ],
      "text": "We present the Chinchilla model card in Table A8, following the framework presented by Mitchell\net al. (2019)."
    },
    {
      "page_no": 30,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "30"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        89.35792541503906,
        541.4846801757812,
        99.63524627685547
      ],
      "text": "Task\nChinchilla\nGopher\nTask\nChinchilla\nGopher"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        106.78596496582031,
        530.4354248046875,
        451.4935302734375
      ],
      "text": "abstract_algebra\n31.0\n25.0\nanatomy\n70.4\n56.3\nastronomy\n73.0\n65.8\nbusiness_ethics\n72.0\n70.0\nclinical_knowledge\n75.1\n67.2\ncollege_biology\n79.9\n70.8\ncollege_chemistry\n51.0\n45.0\ncollege_computer_science\n51.0\n49.0\ncollege_mathematics\n32.0\n37.0\ncollege_medicine\n66.5\n60.1\ncollege_physics\n46.1\n34.3\ncomputer_security\n76.0\n65.0\nconceptual_physics\n67.2\n49.4\neconometrics\n38.6\n43.0\nelectrical_engineering\n62.1\n60.0\nelementary_mathematics\n41.5\n33.6\nformal_logic\n33.3\n35.7\nglobal_facts\n39.0\n38.0\nhigh_school_biology\n80.3\n71.3\nhigh_school_chemistry\n58.1\n47.8\nhigh_school_computer_science\n58.0\n54.0\nhigh_school_european_history\n78.8\n72.1\nhigh_school_geography\n86.4\n76.8\nhigh_school_gov_and_politics\n91.2\n83.9\nhigh_school_macroeconomics\n70.5\n65.1\nhigh_school_mathematics\n31.9\n23.7\nhigh_school_microeconomics\n77.7\n66.4\nhigh_school_physics\n36.4\n33.8\nhigh_school_psychology\n86.6\n81.8\nhigh_school_statistics\n58.8\n50.0\nhigh_school_us_history\n83.3\n78.9\nhigh_school_world_history\n85.2\n75.1\nhuman_aging\n77.6\n66.4\nhuman_sexuality\n86.3\n67.2\ninternational_law\n90.9\n77.7\njurisprudence\n79.6\n71.3\nlogical_fallacies\n80.4\n72.4\nmachine_learning\n41.1\n41.1\nmanagement\n82.5\n77.7\nmarketing\n89.7\n83.3\nmedical_genetics\n69.0\n69.0\nmiscellaneous\n84.5\n75.7\nmoral_disputes\n77.5\n66.8\nmoral_scenarios\n36.5\n40.2\nnutrition\n77.1\n69.9\nphilosophy\n79.4\n68.8\nprehistory\n81.2\n67.6\nprofessional_accounting\n52.1\n44.3\nprofessional_law\n56.5\n44.5\nprofessional_medicine\n75.4\n64.0\nprofessional_psychology\n75.7\n68.1\npublic_relations\n73.6\n71.8\nsecurity_studies\n75.9\n64.9\nsociology\n91.0\n84.1\nus_foreign_policy\n92.0\n81.0\nvirology\n53.6\n47.0\nworld_religions\n87.7\n84.2"
    },
    {
      "page_no": 31,
      "bbox": [
        62.03499984741211,
        466.3022766113281,
        533.3416748046875,
        491.3564453125
      ],
      "text": "Table A6 | Chinchilla MMLU results. For each subset of MMLU (Hendrycks et al., 2020), we show\nChinchilla’s accuracy compared to Gopher."
    },
    {
      "page_no": 31,
      "bbox": [
        263.1600036621094,
        533.5475463867188,
        332.9018859863281,
        544.4566650390625
      ],
      "text": "Model Details"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        556.395751953125,
        294.53448486328125,
        567.3048706054688
      ],
      "text": "Organization Developing the Model\nDeepMind"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        575.417724609375,
        302.8471984863281,
        586.3268432617188
      ],
      "text": "Model Date\nMarch 2022"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        594.439697265625,
        527.928466796875,
        618.8988647460938
      ],
      "text": "Model Type\nAutoregressive Transformer Language Model (Section 4.1 for\ndetails)"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        626.9837646484375,
        415.4903259277344,
        652.5328979492188
      ],
      "text": "Feedback on the Model\n{jordanhoffmann, sborgeaud,\namensch,sifre}@deepmind.com"
    },
    {
      "page_no": 31,
      "bbox": [
        260.79901123046875,
        663.9885864257812,
        332.2318420410156,
        674.897705078125
      ],
      "text": "Intended Uses"
    },
    {
      "page_no": 31,
      "bbox": [
        68.33999633789062,
        686.8377075195312,
        529.1226806640625,
        724.8448486328125
      ],
      "text": "Primary Intended Uses\nThe primary use is research on language models, including:\nresearch on the scaling behaviour of language models along\nwith those listed in Rae et al. (2021)."
    },
    {
      "page_no": 31,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "31"
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        87.27174377441406,
        527.725341796875,
        111.7298583984375
      ],
      "text": "Primary Intended Users\nDeepMind researchers. We will not make this model available\npublicly."
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        119.84272766113281,
        529.4572143554688,
        171.39886474609375
      ],
      "text": "Out-of-Scope Uses\nUses of the language model for language generation in harm-\nful or deceitful settings. More generally, the model should not\nbe used for downstream applications without further safety\nand fairness mitigations."
    },
    {
      "page_no": 32,
      "bbox": [
        278.5589904785156,
        186.75257873535156,
        314.47174072265625,
        197.66168212890625
      ],
      "text": "Factors"
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        209.6017608642578,
        529.4573364257812,
        315.3548583984375
      ],
      "text": "Card Prompts – Relevant Factor\nRelevant factors include which language is used. Our model is\ntrained on English data. Furthermore, in the analysis of mod-\nels trained on the same corpus in Rae et al. (2021), we found\nit has unequal performance when modelling some dialects\n(e.g., African American English). Our model is designed for\nresearch. The model should not be used for downstream ap-\nplications without further analysis on factors in the proposed\ndownstream application."
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        323.46771240234375,
        527.7215576171875,
        347.9268493652344
      ],
      "text": "Card Prompts – Evaluation Factors\nSee the results in Rae et al. (2021) which analyzes models\ntrained on the same text corpus."
    },
    {
      "page_no": 32,
      "bbox": [
        277.9700012207031,
        363.2806091308594,
        315.0609130859375,
        374.189697265625
      ],
      "text": "Metrics"
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        386.12872314453125,
        208.5546875,
        397.037841796875
      ],
      "text": "Model Performance Measures"
    },
    {
      "page_no": 32,
      "bbox": [
        260.77099609375,
        399.677734375,
        529.2391357421875,
        532.5298461914062
      ],
      "text": "• Perplexity and bits per byte on language modelling\ndatasets\n• Accuracy on completion tasks, reading comprehension,\nMMLU, BIG-bench and fact checking.\n• Exact match accuracy for question answering.\n• Generation toxicity from Real Toxicity Prompts (RTP)\nalongside toxicity classiﬁcation accuracy.\n• Gender and occupation bias. Test include comparing\nthe probability of generating diﬀerent gender terms\nand the Winogender coreference resolution task."
    },
    {
      "page_no": 32,
      "bbox": [
        244.88699340820312,
        548.7197265625,
        527.7266235351562,
        573.5223999023438
      ],
      "text": "We principally focus on Chinchilla’s performance compared\nto Gopher on text likelihood prediction."
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        581.2907104492188,
        265.3853759765625,
        592.1998291015625
      ],
      "text": "Decision thresholds\nN/A"
    },
    {
      "page_no": 32,
      "bbox": [
        68.33999633789062,
        600.312744140625,
        234.7655487060547,
        624.7708129882812
      ],
      "text": "Approaches to Uncertainty and Vari-\nability"
    },
    {
      "page_no": 32,
      "bbox": [
        245.38900756835938,
        600.312744140625,
        529.5453491210938,
        692.516845703125
      ],
      "text": "Due to the costs of training large language models, we did\nnot train Chinchilla multiple times. However, the breadth\nof our evaluation on a range of diﬀerent task types gives a\nreasonable estimate of the overall performance of the model.\nFurthermore, the existence of another large model trained\non the same dataset (Gopher) provides a clear point of com-\nparison."
    },
    {
      "page_no": 32,
      "bbox": [
        256.4949951171875,
        707.8705444335938,
        336.53509521484375,
        718.7796630859375
      ],
      "text": "Evaluation Data"
    },
    {
      "page_no": 32,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "32"
    },
    {
      "page_no": 33,
      "bbox": [
        68.33999633789062,
        87.27174377441406,
        108.1909408569336,
        98.18084716796875
      ],
      "text": "Datasets"
    },
    {
      "page_no": 33,
      "bbox": [
        260.77099609375,
        109.78675842285156,
        529.4628295898438,
        351.0318298339844
      ],
      "text": "• Language modelling on LAMBADA, Wikitext103 (Mer-\nity et al., 2017), C4 (Raﬀel et al., 2020a), PG-19 (Rae\net al., 2020) and the Pile (Gao et al., 2020).\n• Language understanding,\nreal world knowledge,\nmathematical and logical reasoning on the Massive\nMultitask Language Understanding (MMLU) bench-\nmark (Hendrycks et al., 2020) and on the “Beyond the\nImitation Game Benchmark” (BIG-bench) (BIG-bench\ncollaboration, 2021).\n• Question answering (closed book) on Natural Ques-\ntions (Kwiatkowski et al., 2019) and TriviaQA (Joshi\net al., 2017).\n• Reading comprehension on RACE (Lai et al., 2017)\n• Common sense understanding on HellaSwag (Zellers\net al., 2019),\nPIQA (Bisk et al., 2020),\nWino-\ngrande (Sakaguchi et al., 2020), SIQA (Sap et al., 2019),\nBoolQ (Clark et al., 2019), and TruthfulQA (Lin et al.,\n2021)."
    },
    {
      "page_no": 33,
      "bbox": [
        68.33999633789062,
        372.69476318359375,
        527.7214965820312,
        397.4974365234375
      ],
      "text": "Motivation\nWe chose evaluations from Rae et al. (2021) to allow us to\nmost directly compare to Gopher."
    },
    {
      "page_no": 33,
      "bbox": [
        68.33999633789062,
        405.2657470703125,
        527.9329833984375,
        456.8228454589844
      ],
      "text": "Preprocessing\nInput text is tokenized using a SentencePiece tokenizer with\na vocabulary of size 32,000. Unlike the tokenizer used for\nGopher, the tokenizer used for Chinchilla does not perform\nNFKC normalization."
    },
    {
      "page_no": 33,
      "bbox": [
        261.6109924316406,
        484.131591796875,
        331.4183044433594,
        495.0406799316406
      ],
      "text": "Training Data"
    },
    {
      "page_no": 33,
      "bbox": [
        68.43800354003906,
        506.979736328125,
        524.5908203125,
        517.8888549804688
      ],
      "text": "The same dataset is used as in Rae et al. (2021). Diﬀerences in sampling are shown in Table A1."
    },
    {
      "page_no": 33,
      "bbox": [
        243.04400634765625,
        533.2426147460938,
        353.0186767578125,
        544.1517333984375
      ],
      "text": "Quantitative Analyses"
    },
    {
      "page_no": 33,
      "bbox": [
        68.33999633789062,
        556.0916748046875,
        527.7255249023438,
        580.5498657226562
      ],
      "text": "Unitary Results\nSection 4.2 gives a detailed description of our analysis. Main\ntake-aways include:"
    },
    {
      "page_no": 33,
      "bbox": [
        260.77099609375,
        592.15673828125,
        529.515380859375,
        738.557861328125
      ],
      "text": "• Our model is capable of outputting toxic language as\nmeasured by the PerspectiveAPI. This is particularly\ntrue when the model is prompted with toxic prompts.\n• Gender: Our model emulates stereotypes found in our\ndataset, with occupations such as “dietician” and “re-\nceptionist” being more associated with women and “car-\npenter” and “sheriﬀ” being more associated with men.\n• Race/religion/country sentiment:\nPrompting our\nmodel to discuss some groups leads to sentences with\nlower or higher sentiment, likely reﬂecting text in our\ndataset."
    },
    {
      "page_no": 33,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "33"
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        87.27174377441406,
        452.4217529296875,
        98.18084716796875
      ],
      "text": "Intersectional Results\nWe did not investigate intersectional biases."
    },
    {
      "page_no": 34,
      "bbox": [
        241.5709991455078,
        113.5345687866211,
        354.4910888671875,
        124.44366455078125
      ],
      "text": "Ethical Considerations"
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        136.38270568847656,
        503.9453125,
        147.29180908203125
      ],
      "text": "Data\nThe data is the same as described in Rae et al. (2021)."
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        155.4047393798828,
        527.72119140625,
        179.86285400390625
      ],
      "text": "Human Life\nThe model is not intended to inform decisions about matters\ncentral to human life or ﬂourishing."
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        187.97572326660156,
        527.9103393554688,
        266.630859375
      ],
      "text": "Mitigations\nWe considered ﬁltering the dataset to remove toxic content\nbut decided against it due to the observation that this can\nintroduce new biases as studied by Welbl et al. (2021). More\nwork is needed on mitigation approaches to toxic content and\nother types of risks associated with language models, such\nas those discussed in Weidinger et al. (2021)."
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        274.74468994140625,
        529.45751953125,
        339.849853515625
      ],
      "text": "Risks and Harms\nThe data is collected from the internet, and thus undoubtedly\nthere is toxic/biased content in our training dataset. Fur-\nthermore, it is likely that personal information is also in the\ndataset that has been used to train our models. We defer to\nthe more detailed discussion in Weidinger et al. (2021)."
    },
    {
      "page_no": 34,
      "bbox": [
        68.33999633789062,
        347.96270751953125,
        529.45751953125,
        426.6178283691406
      ],
      "text": "Use Cases\nEspecially fraught use cases include the generation of fac-\ntually incorrect information with the intent of distributing\nit or using the model to generate racist, sexist or otherwise\ntoxic text with harmful intent. Many more use cases that\ncould cause harm exist. Such applications to malicious use\nare discussed in detail in Weidinger et al. (2021)."
    },
    {
      "page_no": 34,
      "bbox": [
        60.9119987487793,
        441.71527099609375,
        517.787353515625,
        453.1656799316406
      ],
      "text": "Table A8 | Chinchilla model card. We follow the framework presented in Mitchell et al. (2019)."
    },
    {
      "page_no": 34,
      "bbox": [
        62.36199951171875,
        479.6679992675781,
        209.02369689941406,
        492.6194152832031
      ],
      "text": "J. List of trained models"
    },
    {
      "page_no": 34,
      "bbox": [
        62.36199951171875,
        506.2626953125,
        532.9090576171875,
        530.7208251953125
      ],
      "text": "In Table A9 we list the model size and conﬁguration of all models used in this study. Many models\nhave been trained multiple times, for a diﬀerent number of training steps."
    },
    {
      "page_no": 34,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "34"
    },
    {
      "page_no": 35,
      "bbox": [
        68.33999633789062,
        209.3809356689453,
        541.8526611328125,
        219.6582489013672
      ],
      "text": "Task\nChinchilla\nGopher\nTask\nChinchilla\nGopher"
    },
    {
      "page_no": 35,
      "bbox": [
        68.33999633789062,
        226.8089141845703,
        530.80419921875,
        595.426513671875
      ],
      "text": "hyperbaton\n54.2\n51.7\nmovie_dialog_same_or_diﬀ\n54.5\n50.7\ncausal_judgment\n57.4\n50.8\nwinowhy\n62.5\n56.7\nformal_fallacies_syllogisms_neg\n52.1\n50.7\nmovie_recommendation\n75.6\n50.5\ncrash_blossom\n47.6\n63.6\nmoral_permissibility\n57.3\n55.1\ndiscourse_marker_prediction\n13.1\n11.7\nstrategyqa\n68.3\n61.0\ngeneral_knowledge_json\n94.3\n93.9\nnonsense_words_grammar\n78.0\n61.4\nsports_understanding\n71.0\n54.9\nmetaphor_boolean\n93.1\n59.3\nimplicit_relations\n49.4\n36.4\nnavigate\n52.6\n51.1\npenguins_in_a_table\n48.7\n40.6\npresuppositions_as_nli\n49.9\n34.0\nintent_recognition\n92.8\n88.7\ntemporal_sequences\n32.0\n19.0\nreasoning_about_colored_objects\n59.7\n49.2\nquestion_selection\n52.6\n41.4\nlogic_grid_puzzle\n44.0\n35.1\nlogical_fallacy_detection\n72.1\n58.9\ntimedial\n68.8\n50.9\nphysical_intuition\n79.0\n59.7\nepistemic_reasoning\n60.6\n56.4\nphysics_mc\n65.5\n50.9\nruin_names\n47.1\n38.6\nidentify_odd_metaphor\n68.8\n38.6\nhindu_knowledge\n91.4\n80.0\nunderstanding_fables\n60.3\n39.6\nmisconceptions\n65.3\n61.7\nlogical_sequence\n64.1\n36.4\nimplicatures\n75.0\n62.0\nmathematical_induction\n47.3\n57.6\ndisambiguation_q\n54.7\n45.5\nfantasy_reasoning\n69.0\n64.1\nknown_unknowns\n65.2\n63.6\nSNARKS\n58.6\n48.3\ndark_humor_detection\n66.2\n83.1\ncrass_ai\n75.0\n56.8\nanalogical_similarity\n38.1\n17.2\nentailed_polarity\n94.0\n89.5\nsentence_ambiguity\n71.7\n69.1\nirony_identiﬁcation\n73.0\n69.7\nriddle_sense\n85.7\n68.2\nevaluating_info_essentiality\n17.6\n16.7\ndate_understanding\n52.3\n44.1\nphrase_relatedness\n94.0\n81.8\nanalytic_entailment\n67.1\n53.0\nnovel_concepts\n65.6\n59.1\nodd_one_out\n70.9\n32.5\nempirical_judgments\n67.7\n52.5\nlogical_args\n56.2\n59.1\nﬁgure_of_speech_detection\n63.3\n52.7\nalignment_questionnaire\n91.3\n79.2\nenglish_proverbs\n82.4\n57.6\nsimilarities_abstraction\n87.0\n81.8\nHuman_organs_senses_mcc\n85.7\n84.8\nanachronisms\n69.1\n56.4\ngre_reading_comprehension\n53.1\n27.3"
    },
    {
      "page_no": 35,
      "bbox": [
        62.03499984741211,
        610.2352294921875,
        534.4320068359375,
        635.2894287109375
      ],
      "text": "Table A7 | Chinchilla BIG-bench results. For each subset of BIG-bench (BIG-bench collaboration,\n2021), we show Chinchilla and Gopher’s accuracy."
    },
    {
      "page_no": 35,
      "bbox": [
        291.572021484375,
        778.01171875,
        303.70294189453125,
        788.9208374023438
      ],
      "text": "35"
    },
    {
      "page_no": 36,
      "bbox": [
        147.69700622558594,
        99.54886627197266,
        447.5762634277344,
        108.51526641845703
      ],
      "text": "Parameters (million)\nd_model\nﬀw_size\nkv_size\nn_heads\nn_layers"
    },
    {
      "page_no": 36,
      "bbox": [
        200.822998046875,
        115.98087310791016,
        447.5759582519531,
        661.9332885742188
      ],
      "text": "44\n512\n2048\n64\n8\n8\n57\n576\n2304\n64\n9\n9\n74\n640\n2560\n64\n10\n10\n90\n640\n2560\n64\n10\n13\n106\n640\n2560\n64\n10\n16\n117\n768\n3072\n64\n12\n12\n140\n768\n3072\n64\n12\n15\n163\n768\n3072\n64\n12\n18\n175\n896\n3584\n64\n14\n14\n196\n896\n3584\n64\n14\n16\n217\n896\n3584\n64\n14\n18\n251\n1024\n4096\n64\n16\n16\n278\n1024\n4096\n64\n16\n18\n306\n1024\n4096\n64\n16\n20\n425\n1280\n5120\n128\n10\n18\n489\n1280\n5120\n128\n10\n21\n509\n1408\n5632\n128\n11\n18\n552\n1280\n5120\n128\n10\n24\n587\n1408\n5632\n128\n11\n21\n632\n1536\n6144\n128\n12\n19\n664\n1408\n5632\n128\n11\n24\n724\n1536\n6144\n128\n12\n22\n816\n1536\n6144\n128\n12\n25\n893\n1792\n7168\n128\n14\n20\n1,018\n1792\n7168\n128\n14\n23\n1,143\n1792\n7168\n128\n14\n26\n1,266\n2048\n8192\n128\n16\n22\n1,424\n2176\n8704\n128\n17\n22\n1,429\n2048\n8192\n128\n16\n25\n1,593\n2048\n8192\n128\n16\n28\n1,609\n2176\n8704\n128\n17\n25\n1,731\n2304\n9216\n128\n18\n24\n1,794\n2176\n8704\n128\n17\n28\n2,007\n2304\n9216\n128\n18\n28\n2,283\n2304\n9216\n128\n18\n32\n2,298\n2560\n10240\n128\n20\n26\n2,639\n2560\n10240\n128\n20\n30\n2,980\n2560\n10240\n128\n20\n34\n3,530\n2688\n10752\n128\n22\n36\n3,802\n2816\n11264\n128\n22\n36\n4,084\n2944\n11776\n128\n22\n36\n4,516\n3072\n12288\n128\n24\n36\n6,796\n3584\n14336\n128\n28\n40\n9,293\n4096\n16384\n128\n32\n42\n11,452\n4352\n17408\n128\n32\n47\n12,295\n4608\n18432\n128\n36\n44\n12,569\n4608\n18432\n128\n32\n47\n13,735\n4864\n19456\n128\n32\n47\n14,940\n4992\n19968\n128\n32\n49\n16,183\n5120\n20480\n128\n40\n47"
    },
    {
      "page_no": 36,
      "bbox": [
        60.78900146484375,
        674.8292236328125,
        533.4854736328125,
        713.0878295898438
      ],
      "text": "Table A9 | All models. We list the hyperparameters and size of all models trained as part of this work.\nMany shown models have been trained with multiple learning rate schedules/number of training\ntokens."
    },
    {
      "page_no": 36,
      "bbox": [
        291.5719909667969,
        778.01171875,
        303.7029113769531,
        788.9208374023438
      ],
      "text": "36"
    }
  ],
  "pictures": [
    {
      "page_no": 1,
      "bbox": [
        62.36199951171875,
        36.45838928222656,
        161.98074340820312,
        61.04998779296875
      ],
      "xref": 0,
      "image_path": "../data/parsed_documents/2203.15556/images/2203.15556_p1_blk1_crop.png"
    },
    {
      "page_no": 5,
      "bbox": [
        184.7340087890625,
        94.20172119140625,
        190.91700744628906,
        158.77972412109375
      ],
      "xref": 16,
      "image_path": "../data/parsed_documents/2203.15556/images/2203.15556_p5_blk1_crop.png"
    },
    {
      "page_no": 7,
      "bbox": [
        338.7550964355469,
        97.07447814941406,
        346.6255798339844,
        255.41021728515625
      ],
      "xref": 15,
      "image_path": "../data/parsed_documents/2203.15556/images/2203.15556_p7_blk1_crop.png"
    },
    {
      "page_no": 28,
      "bbox": [
        369.1360778808594,
        89.755859375,
        378.5714416503906,
        276.31866455078125
      ],
      "xref": 20,
      "image_path": "../data/parsed_documents/2203.15556/images/2203.15556_p28_blk1_crop.png"
    }
  ],
  "tables": [
    {
      "page_no": 1,
      "index": 1,
      "flavor": "stream",
      "nrows": 11,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p1_table1_stream.csv"
    },
    {
      "page_no": 2,
      "index": 1,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p2_table1_lattice.csv"
    },
    {
      "page_no": 2,
      "index": 2,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p2_table2_lattice.csv"
    },
    {
      "page_no": 3,
      "index": 1,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p3_table1_stream.csv"
    },
    {
      "page_no": 4,
      "index": 1,
      "flavor": "stream",
      "nrows": 40,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p4_table1_stream.csv"
    },
    {
      "page_no": 5,
      "index": 1,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p5_table1_lattice.csv"
    },
    {
      "page_no": 5,
      "index": 2,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p5_table2_lattice.csv"
    },
    {
      "page_no": 6,
      "index": 1,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p6_table1_lattice.csv"
    },
    {
      "page_no": 6,
      "index": 2,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p6_table2_lattice.csv"
    },
    {
      "page_no": 6,
      "index": 3,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p6_table3_lattice.csv"
    },
    {
      "page_no": 7,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 20,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p7_table1_lattice.csv"
    },
    {
      "page_no": 8,
      "index": 1,
      "flavor": "stream",
      "nrows": 15,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p8_table1_stream.csv"
    },
    {
      "page_no": 9,
      "index": 1,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p9_table1_stream.csv"
    },
    {
      "page_no": 9,
      "index": 2,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p9_table2_stream.csv"
    },
    {
      "page_no": 10,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p10_table1_lattice.csv"
    },
    {
      "page_no": 11,
      "index": 1,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p11_table1_stream.csv"
    },
    {
      "page_no": 11,
      "index": 2,
      "flavor": "stream",
      "nrows": 39,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p11_table2_stream.csv"
    },
    {
      "page_no": 12,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p12_table1_lattice.csv"
    },
    {
      "page_no": 13,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p13_table1_lattice.csv"
    },
    {
      "page_no": 14,
      "index": 1,
      "flavor": "stream",
      "nrows": 11,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p14_table1_stream.csv"
    },
    {
      "page_no": 14,
      "index": 2,
      "flavor": "stream",
      "nrows": 37,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p14_table2_stream.csv"
    },
    {
      "page_no": 15,
      "index": 1,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p15_table1_stream.csv"
    },
    {
      "page_no": 15,
      "index": 2,
      "flavor": "stream",
      "nrows": 46,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p15_table2_stream.csv"
    },
    {
      "page_no": 16,
      "index": 1,
      "flavor": "stream",
      "nrows": 26,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p16_table1_stream.csv"
    },
    {
      "page_no": 16,
      "index": 2,
      "flavor": "stream",
      "nrows": 19,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p16_table2_stream.csv"
    },
    {
      "page_no": 17,
      "index": 1,
      "flavor": "stream",
      "nrows": 41,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p17_table1_stream.csv"
    },
    {
      "page_no": 18,
      "index": 1,
      "flavor": "stream",
      "nrows": 16,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p18_table1_stream.csv"
    },
    {
      "page_no": 19,
      "index": 1,
      "flavor": "stream",
      "nrows": 43,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p19_table1_stream.csv"
    },
    {
      "page_no": 20,
      "index": 1,
      "flavor": "stream",
      "nrows": 41,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p20_table1_stream.csv"
    },
    {
      "page_no": 21,
      "index": 1,
      "flavor": "stream",
      "nrows": 25,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p21_table1_stream.csv"
    },
    {
      "page_no": 22,
      "index": 1,
      "flavor": "stream",
      "nrows": 11,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p22_table1_stream.csv"
    },
    {
      "page_no": 23,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table1_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 2,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table2_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 3,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table3_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 4,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table4_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 5,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table5_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 6,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p23_table6_lattice.csv"
    },
    {
      "page_no": 24,
      "index": 1,
      "flavor": "stream",
      "nrows": 28,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p24_table1_stream.csv"
    },
    {
      "page_no": 25,
      "index": 1,
      "flavor": "stream",
      "nrows": 38,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p25_table1_stream.csv"
    },
    {
      "page_no": 26,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p26_table1_lattice.csv"
    },
    {
      "page_no": 27,
      "index": 1,
      "flavor": "stream",
      "nrows": 6,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p27_table1_stream.csv"
    },
    {
      "page_no": 27,
      "index": 2,
      "flavor": "stream",
      "nrows": 13,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p27_table2_stream.csv"
    },
    {
      "page_no": 28,
      "index": 1,
      "flavor": "lattice",
      "nrows": 10,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p28_table1_lattice.csv"
    },
    {
      "page_no": 29,
      "index": 1,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p29_table1_stream.csv"
    },
    {
      "page_no": 29,
      "index": 2,
      "flavor": "stream",
      "nrows": 25,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p29_table2_stream.csv"
    },
    {
      "page_no": 29,
      "index": 3,
      "flavor": "stream",
      "nrows": 23,
      "ncols": 21,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p29_table3_stream.csv"
    },
    {
      "page_no": 30,
      "index": 1,
      "flavor": "stream",
      "nrows": 40,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p30_table1_stream.csv"
    },
    {
      "page_no": 30,
      "index": 2,
      "flavor": "stream",
      "nrows": 24,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p30_table2_stream.csv"
    },
    {
      "page_no": 31,
      "index": 1,
      "flavor": "stream",
      "nrows": 31,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p31_table1_stream.csv"
    },
    {
      "page_no": 31,
      "index": 2,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p31_table2_stream.csv"
    },
    {
      "page_no": 32,
      "index": 1,
      "flavor": "stream",
      "nrows": 17,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p32_table1_stream.csv"
    },
    {
      "page_no": 32,
      "index": 2,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p32_table2_stream.csv"
    },
    {
      "page_no": 33,
      "index": 1,
      "flavor": "stream",
      "nrows": 18,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p33_table1_stream.csv"
    },
    {
      "page_no": 33,
      "index": 2,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p33_table2_stream.csv"
    },
    {
      "page_no": 34,
      "index": 1,
      "flavor": "stream",
      "nrows": 22,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p34_table1_stream.csv"
    },
    {
      "page_no": 35,
      "index": 1,
      "flavor": "stream",
      "nrows": 33,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p35_table1_stream.csv"
    },
    {
      "page_no": 36,
      "index": 1,
      "flavor": "stream",
      "nrows": 51,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2203.15556/2203.15556_p36_table1_stream.csv"
    }
  ]
}