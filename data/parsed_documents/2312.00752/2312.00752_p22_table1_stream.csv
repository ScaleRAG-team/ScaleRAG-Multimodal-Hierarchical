"","In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312–31327."
"[98]","Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. “Simplified State Space Layers for Sequence"
"","Modeling”. In: The International Conference on Learning Representations (ICLR). 2023."
"[99]","Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. “Roformer: Enhanced Transformer"
"","with Rotary Position Embedding”. In: arXiv preprint arXiv:2104.09864 (2021)."
"[100]","Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. “Retentive"
"","network: A successor to transformer for large language models”. In: arXiv preprint arXiv:2307.08621 (2023)."
"",""
"","Ilya Sutskever, Oriol Vinyals, and Quoc V Le. “Sequence to Sequence Learning with Neural Networks”. In: Advances"
"","in Neural Information Processing Systems (NeurIPS) 27 (2014)."
"",""
"","Corentin Tallec and Yann Ollivier. “Can Recurrent Neural Networks Warp Time?” In: The International Conference"
"","on Learning Representations (ICLR). 2018."
"[103]","Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian"
"","Ruder, and Donald Metzler. “Long Range Arena: A Benchmark for Efficient Transformers”.
In:"
"","International"
"","Conference on Learning Representations (ICLR). 2021."
"",""
"","Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efficient Transformers: A Survey”. In: ACM Computing"
"","Surveys 55.6 (2022), pp. 1–28."
"[105]","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste"
"","Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and Efficient Foundation Language Models”."
"","In: arXiv preprint arXiv:2302.13971 (2023)."
"[106]","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and"
"","Illia Polosukhin. “Attention Is All You Need”. In: Advances in Neural Information Processing Systems (NeurIPS). 2017."
"[107]","Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. “On Orthogonality and Learning Recurrent"
"","Networks with Long Term Dependencies”. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570–"
"","3578."
"[108]","Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. “Selective Structured"
"","State-Spaces for Long-form Video Understanding”. In: Proceedings of the IEEE/CVF Conference on Computer Vision"
"","and Pattern Recognition. 2023, pp. 6387–6397."
"",""
"[109]","Pete Warden. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition”. In: ArXiv abs/1804.03209"
"","(2018)."
"[110]","Samuel Williams, Andrew Waterman, and David Patterson. “Roofline: An Insightful Visual Performance Model for"
"","Multicore Architectures”. In: Communications of the ACM 52.4 (2009), pp. 65–76."
"[111]","Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. “CondConv: Conditionally Parameterized Convolu-"
"","tions for Efficient Inference”. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019)."
"[112]","Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a Machine Really Finish"
"","Your Sentence?” In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."
