"evaluation."
"1
Introduction"
"Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged"
"as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on"
"arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and"
"genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever,"
"Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are"
"predominantly based on a single type of sequence model:
the Transformer (Vaswani et al. 2017) and its core attention"
"layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely"
"within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks:"
"an inability to model anything outside of a finite window, and quadratic scaling with respect
to the window length."
"An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay,"
"Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these"
"variants have been shown to be empirically effective at scale across domains."
"Recently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have"
"emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of"
"recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space"
"models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with"
"linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range"
"dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range"
