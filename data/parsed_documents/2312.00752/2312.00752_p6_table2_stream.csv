"3.3.1","Motivation of Prior Models"
"We first revisit this motivation and overview our approach to overcome limitations of prior methods.",""
"","• At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in"
"","Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize"
"hidden state dimension without paying speed and memory costs.",""
"","• Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding"
"","the former (2) (Gu, Goel, and Ré 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and"
"","materializing the latent state ℎ with shape (B, L, D, N), which is much larger (by a factor of 𝑁 , the SSM state dimension)"
"","than the input 𝑥 and output 𝑦 of shape (B, L, D). Thus the more efficient convolution mode was introduced which could"
"","bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D)."
"","• Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by"
"","a factor of 𝑁 (≈ 10 − 100), much larger than traditional RNNs, without efficiency penalties."
