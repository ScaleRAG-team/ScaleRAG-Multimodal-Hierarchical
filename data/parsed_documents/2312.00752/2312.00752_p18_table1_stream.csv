"[8]","Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. “PIQA: Reasoning about Physical Commonsense in"
"","Natural Language”. In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 2020."
"[9]","Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,"
"","Kyle McDonell, Jason Phang, et al. “Gpt-NeoX-20B: An Open-source Autoregressive Language Model”. In: arXiv"
"","preprint arXiv:2204.06745 (2022)."
"[10]","Guy E Blelloch. “Prefix Sums and Their Applications”. In: (1990)."
"[11]","James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. “Quasi-recurrent Neural Networks”. In:"
"","arXiv preprint arXiv:1611.01576 (2016)."
"[12]","Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,"
"","Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-shot Learners”. In: Advances in"
"","Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877–1901."
"[13]","Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. “Scaling Transformer to 1M tokens and Beyond with RMT”."
"","In: arXiv preprint arXiv:2304.11062 (2023)."
"[14]","Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. “Generating Long Sequences with Sparse Transformers”."
"","In: arXiv preprint arXiv:1904.10509 (2019)."
"[15]","Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter"
"","Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. “Rethinking Attention with Performers”. In: The"
"","International Conference on Learning Representations (ICLR). 2021."
"[16]","Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,"
"","Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “PaLM: Scaling Language Modeling with Pathways”."
"","In: Journal of Machine Learning Research 24.240 (2023), pp. 1–113. url: http://jmlr.org/papers/v24/22-"
"","1144.html."
"[17]","Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. “Empirical Evaluation of Gated Recurrent"
"","Neural Networks on Sequence Modeling”. In: arXiv preprint arXiv:1412.3555 (2014)."
"[18]","Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord."
"","“Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge”. In: arXiv preprint arXiv:1803.05457"
"","(2018)."
"",""
"","Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. In: The International"
"","Conference on Learning Representations (ICLR). 2024."
"[20]","Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-Efficient"
"","Exact Attention with IO-Awareness”. In: Advances in Neural Information Processing Systems (NeurIPS). 2022."
"[21]","Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. “Hungry Hungry Hippos:"
"","Towards Language Modeling with State Space Models”. In: The International Conference on Learning Representations"
"","(ICLR). 2023."
"[22]","Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. “Language Modeling with Gated Convolutional"
"","Networks”. In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933–941."
"",""
"","DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017."
"[24]","Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. “LongNet:"
"","Scaling Transformers to 1,000,000,000 Tokens”. In: arXiv preprint arXiv:2307.02486 (2023)."
