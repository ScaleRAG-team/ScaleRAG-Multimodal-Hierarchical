"[70]","Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke"
"","Zettlemoyer. “Mega: Moving Average Equipped Gated Attention”. In: The International Conference on Learning"
"","Representations (ICLR). 2023."
"",""
"","Eric Martin and Chris Cundy. “Parallelizing Linear Recurrent Neural Nets Over Sequence Length”. In: The Interna-"
"","tional Conference on Learning Representations (ICLR). 2018."
"[72]","Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and"
"","Yoshua Bengio. “SampleRNN: An Unconditional End-to-End Neural Audio Generation Model”. In: The International"
"","Conference on Learning Representations (ICLR). 2017."
"[73]","Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. “Long Range Language Modeling via Gated"
"","State Spaces”. In: The International Conference on Learning Representations (ICLR). 2023."
"[74]","Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. “Efficient Orthogonal Parametrisation"
"","of Recurrent Neural Networks using Householder Reflections”. In: International Conference on Machine Learning."
"","PMLR. 2017, pp. 2401–2409."
"[75]","Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher"
"","Ré. “S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces”. In: Advances in Neural"
"","Information Processing Systems (NeurIPS). 2022."
"[76]","Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,"
"","Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. “HyenaDNA: Long-range Genomic Sequence Modeling"
"","at Single Nucleotide Resolution”. In: Advances in Neural Information Processing Systems (NeurIPS). 2023."
"[77]","Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda"
"","Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,"
"","Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,"
"","Jared Kaplan, Sam McCandlish, and Chris Olah. “In-context Learning and Induction Heads”. In: Transformer Circuits"
"","Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html."
"[78]","Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,"
"",""
"","In: arXiv preprint"
"","arXiv:1609.03499 (2016)."
"[79]","Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De."
"","“Resurrecting Recurrent Neural Networks for Long Sequences”. In: The International Conference on Machine Learning"
"","(ICML). 2023."
"[80]","Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,"
"","Marco Baroni, Gemma Boleda, and Raquel Fernández. “The LAMBADA Dataset: Word Prediction Requiring a Broad"
"","Discourse Context”. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016,"
"","pp. 1525–1534."
"[81]","Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. “On the Difficulty of Training Recurrent Neural Networks”."
"","In: International Conference on Machine Learning. 2013, pp. 1310–1318."
"[82]","Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,"
"","Matteo Grella, Kranthi Kiran GV, et al. “RWKV: Reinventing RNNs for the Transformer Era”. In: arXiv preprint"
"","arXiv:2305.13048 (2023)."
"[83]","Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. “Random Feature"
"","Attention”. In: The International Conference on Learning Representations (ICLR). 2021."
"[84]","Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,"
"","and Christopher Ré. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In: The International"
"","Conference on Machine Learning (ICML). 2023."
"[85]","Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and"
"",""
"","In: The International Conference on Learning"
"","Representations (ICLR). 2023."
"[86]","Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. “The devil in"
"","linear transformer”. In: arXiv preprint arXiv:2210.10340 (2022)."
"[87]","Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran"
"","Zhong. “CosFormer: Rethinking Softmax in Attention”. In: The International Conference on Learning Representations"
"","(ICLR). 2022."
"",""
"","In: Advances in Neural"
"","Information Processing Systems (NeurIPS) 20 (2007)."
