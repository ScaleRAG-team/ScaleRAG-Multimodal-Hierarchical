"constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency"
"together yield performance improvements on real data up to sequence length 1M."
"We empirically validate Mamba’s potential as a general sequence FM backbone, in both pretraining quality and domain-"
"specific task performance, on several types of modalities and settings:"
""
"Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to"
"large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (>1M tokens)."
"• Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers"
"on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing"
"FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer"
"context up to million-length sequences."
"• Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance,"
"both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba"
"exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based"
"on LLaMa (Touvron et al. 2023). Our Mamba language model has 5× generation throughput compared to Transformers"
"of similar size, and Mamba-3B’s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common"
"sense reasoning compared to Pythia-3B and even exceeding Pythia-7B)."
