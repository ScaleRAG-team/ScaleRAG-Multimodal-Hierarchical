"Copying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones"
"(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0)."
"‚Ä¢ RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a"
"special case where the state dimension is ùëÅ = 1. Although not framed as such, its recurrence can be viewed as a special"
"case of a linear SSM."
"Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as another"
"method to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention"
"variants was first done by H3, but not extensively used since this requires a proportional amount of extra computation."
"RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention"
"instead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA."
"‚Ä¢ RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free"
"Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main ‚ÄúWKV‚Äù mechanism involves LTI recurrences"
"and can be seen as the ratio of two SSMs."
