"For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1𝑒 − 3, 2𝑒 − 3, 4𝑒 −
Training."
"3, 8𝑒 − 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning"
"rate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable"
"and improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible"
"that our results are still suboptimal.)"
"Note that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The"
"optimal LR should go down for larger models, but we didn’t find a noticeable effect at the small model sizes (at most a few"
"million parameters) we considered."
"E.3.3
Scaling: Context Length Details"
"We use a total batch size of 224 ≈ 16𝑀 tokens per training step, for every sequence length (e.g. at length 220 there are"
"16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size relative to the"
"model size by usual LM standards, but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs and"
"sequence length of 220, and that HyenaDNA used much larger batches of 228."
"The learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate"
"of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length."
"Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during"
"Sequence Length Warmup."
"pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 210 = 1024."
"(Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally."
"In particular, each stage up to length 217 processes the same number of tokens, but 4× as many tokens are processed at"
"length 218, 8× as many at length 219, and 16× as many at length 220.)"
"Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively"
"halved as the sequence lengths are doubled in each stage."
"Remark E.1. We also note that the schedule was not tuned, and we never experimented with turning off sequence length"
"warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar"
"lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either."
"E.3.4
Species (Great Apes) Classification"
"Models are causal and therefore only the last element (across the sequence length) of the model’s output is used for the"
"classification head. Note that we control for the total number of elements in the loss function per gradient step. The"
"pretraining objective includes all positions across the sequence length, so that batch_size × sequence_length is held"
"constant; in other words, the batch size decreases as the sequence length increases. However, for a classification task, since"
"only the last position enters the loss, the batch size itself is held constant. Note that this also means that fine-tuning models"
"with longer sequence lengths is more computationally expensive."
"Training consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all"
"independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly"
"picking a contiguous segment of DNA."
"Following (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214 = 16384 use sequence length"
"warmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length 216 = 65536, and so on up to"
"the maximum sequence length. For example, the model with 220 = 1048576 context undergoes 6 epochs of sequence length"
"warmup before 4 more epochs at its maximum sequence length."
"The learning rate for all Hyena models is 4e − 5, while the learning rate for all Mamba models is 1e − 4. These were found"
"by performing learning rate sweeps for each model among {1𝑒 − 5, 2𝑒 − 5, 4𝑒 − 5, 1𝑒 − 4, 2𝑒 − 4} for the smaller sequence"
"lengths (210, 212, 214, 216), and these values were consistently found to be the best for each model. An abridged learning rate"
"sweep was done at length 218, which agreed with these values, and a single run at length 220 was performed (as described"
"above, the computational cost of these experiments is proportional to the sequence length). The learning rate followed"
"a cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of"
"cosine decay down to 1𝑒 − 6. The unusually long learning rate warmup schedule was chosen because the sequence length"
