"3.3.2
Overview of Selective Scan: Hardware-Aware State Expansion"
"The selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to"
"revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and"
"recomputation. We make two main observations:"
"• The naive recurrent computation uses 𝑂 (𝐵𝐿𝐷𝑁 ) FLOPs while the convolutional computation uses 𝑂 (𝐵𝐿𝐷 log(𝐿)) FLOPs,"
"and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension 𝑁 , the recurrent"
"mode can actually use fewer FLOPs."
"• The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like"
"the convolutional mode, we can attempt to not actually materialize the full state ℎ."
"The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state ℎ only in more efficient"
"levels of the memory hierarchy.
In particular, most operations (except matrix multiplication) are bounded by memory"
"bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our"
"scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared"
"to a standard implementation."
"Concretely, instead of preparing the scan input (𝑨, 𝑩) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load"
"the SSM parameters (Δ, 𝑨, 𝑩, 𝑪) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM,"
"and then write the final outputs of size (B, L, D) back to HBM."
"To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient"
"parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023)."
"Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply"
"the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but"
"recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan"
"layer has the same memory requirements as an optimized transformer implementation with FlashAttention."
"Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated"
"in Figure 1."
"3.4
A Simplified SSM Architecture"
"As with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into"
"neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are"
"generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We"
"simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is"
"inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention."
"This architecture involves expanding the model dimension 𝐷 by a controllable expansion factor 𝐸. For each block, most"
"of
for output projection) while
the parameters (3𝐸𝐷 2) are in the linear projections (2𝐸𝐷 2
for input projections, 𝐸𝐷 2"
""
"the inner SSM contributes less.
The number of SSM parameters (projections for Δ, 𝑩, 𝑪, and the matrix 𝑨) are much"
"smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form"
"the Mamba architecture. We always fix to 𝐸 = 2 in our experiments and use two stacks of the block to match the 12𝐷 2"
"parameters of a Transformer’s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation"
"function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP"
"becomes the popular “SwiGLU” variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023)."
"Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),"
"motivated by RetNet’s usage of a normalization layer in a similar location (Y. Sun et al. 2023)."
"3.5
Properties of Selection Mechanisms"
"The selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or"
"CNNs, to different parameters (e.g. 𝑨 in Algorithm 2), or using different transformations 𝑠 (𝑥)."
