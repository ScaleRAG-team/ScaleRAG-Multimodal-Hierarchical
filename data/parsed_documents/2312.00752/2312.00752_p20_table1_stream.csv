"[46]","Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. “Liquid"
"","Structural State-Space Models”. In: The International Conference on Learning Representations (ICLR). 2023."
"[47]","Mikael Henaff, Arthur Szlam, and Yann LeCun. “Recurrent Orthogonal Networks and Long-Memory Tasks”. In:"
"","The International Conference on Machine Learning (ICML). 2016."
"",""
"","Dan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: arXiv preprint arXiv:1606.08415"
"","(2016)."
"","Sepp Hochreiter. “Untersuchungen zu dynamischen neuronalen Netzen”."
"[49]","In: Diploma, Technische Universität"
"","München 91.1 (1991), p. 31."
"",""
"","Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. Gradient Flow in Recurrent Nets: The"
"","Difficulty of Learning Long-term Dependencies. 2001."
"",""
"","Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural Computation 9.8 (1997), pp. 1735–"
"","1780."
"[52]","Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego"
"","de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. “An Empirical Analysis of Compute-"
"","Optimal Large Language Model Training”. In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022),"
"","pp. 30016–30030."
"",""
"","Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. “Transformer Quality in Linear Time”. In: The International"
"","Conference on Machine Learning (ICML). PMLR. 2022, pp. 9099–9117."
"[54]","Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. “Deep"
"","Learning for Time Series Classification: A Review”. In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917–"
"","963."
"[55]","Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. “Data Movement is All You Need: A"
"","Case Study on Optimizing Transformers”. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711–732."
"[56]","Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. “Gated"
"","Orthogonal Recurrent Units: On Learning to Forget”. In: Neural Computation 31.4 (2019), pp. 765–783."
"[57]","Rudolph Emil Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: (1960)."
"[58]","Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast"
"","Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020,"
"","pp. 5156–5165."
"",""
"","Shiva Kaul. “Linear Dynamical Systems as a Core Computational Primitive”. In: Advances in Neural Information"
"","Processing Systems 33 (2020), pp. 16808–16820."
"[60]","Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. “DiffWave: A Versatile Diffusion Model"
"","for Audio Synthesis”. In: International Conference on Learning Representations. 2021."
"[61]","Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. “Time-Parameterized Convolutional Neural"
"","Networks for Irregularly Sampled Time Series”. In: arXiv preprint arXiv:2308.03210 (2023)."
"[62]","Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with Deep Convolutional Neural"
"","Networks”. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012)."
"",""
"","Tao Lei. “When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute”. In: Proceedings"
"","of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633–7648."
"[64]","Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. “Simple Recurrent Units for Highly Parallelizable"
"","Recurrence”. In: arXiv preprint arXiv:1709.02755 (2017)."
"[65]","Mario Lezcano-Casado and David Martínez-Rubio. “Cheap Orthogonal Constraints in Neural Networks: A Simple"
"",""
"","In: The International Conference on Machine Learning"
"","(ICML). 2019."
"[66]","Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. “What Makes Convolutional Models Great"
"","on Long Sequence Modeling?” In: The International Conference on Learning Representations (ICLR). 2023."
"",""
"","Vasileios Lioutas and Yuhong Guo. “Time-aware Large Kernel Convolutions”. In: The International Conference on"
"","Machine Learning (ICML). PMLR. 2020, pp. 6172–6183."
"[68]","Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani."
"","“Structured State Space Models for In-Context Reinforcement Learning”. In: Advances in Neural Information Processing"
"","Systems (NeurIPS). 2023."
"",""
"","Shahar Lutati, Itamar Zimerman, and Lior Wolf. “Focus Your Attention (with Adaptive IIR Filters)”. In: arXiv preprint"
"","arXiv:2305.14952 (2023)."
