"","Figure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of"
"","• What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking"
"","a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks."
"","Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither"
"","change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except"
"Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact",""
"","that many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,"
"Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).",""
"","Next we ablate differences between the Hyena and H3++ models, our weakest"
"H3 Architecture: Training Recipes.",""
"and strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.",""
"• Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).",""
"• Hyena+: The same architecture but with the improved training recipe described above.",""
"","• H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel."
"","• H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSM"
"recurrence but does not increase parameters.",""
"","Our general convention is that “Model+” represents the base model with the improved training recipe, and “Model++” also"
"allows for architectural changes.",""
"Figure 9 (Right) shows that",""
"","• A large improvement is achieved by the improved training recipe, which was used for many of the models in the main"
"Figure 4 (RetNet, H3++, Transformer++, Mamba).",""
"• The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.",""
"","• The head dimension expansion improves performance, consistent with one of our main themes that expanded state"
"dimension improves performance for SSMs (Section 3).",""
"E.2.3","Downstream Evaluation Details"
"","This pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoX"
"","tokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent"
"with the GPT3 specifications. We report the perplexity on the Pile validation set, and for this metric only compare to",""
"models trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.",""
"","For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most"
"work in this area. We evaluate on the following tasks/datasets that measure common sense reasoning:",""
