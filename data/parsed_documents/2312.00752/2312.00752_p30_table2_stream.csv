"Our models are:"
"Architecture and Training Details."
"• Transformer: The standard Transformer based on GPT3 (Table 12)."
"• Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and"
"SwiGLU MLP (Shazeer 2020), and the improved training recipe above."
"• Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with"
"standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly"
"increased by 1.5× to preserve parameter count."
"• H3++: The H3 architecture with a few modifications, including (i) using the same “thin” Hyena dimensions above (ii) the"
"improved training recipe above (iii) a linear attention head dimension of 8."
"• RWKV: The default RWKV model from B. Peng et al. (2023), including its modified MLP block. We also used as much of"
"its specified training recipe as possible, such as increasing the learning rates by 2× or 3× on certain parameters."
