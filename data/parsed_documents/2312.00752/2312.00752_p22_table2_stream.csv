"",""
"","Prajit Ramachandran, Barret Zoph, and Quoc V Le. “Swish: A Self-gated Activation Function”. In: arXiv preprint"
"","arXiv:1710.05941 7.1 (2017), p. 5."
"[90]","David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. “CKConv: Continuous"
"","Kernel Convolution For Sequential Data”. In: arXiv preprint arXiv:2102.02611 (2021)."
"[91]","Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. “Winogrande: An Adversarial Winograd"
"","Schema Challenge at Scale”. In: Communications of the ACM 64.9 (2021), pp. 99–106."
"[92]","George Saon, Ankit Gupta, and Xiaodong Cui. “Diagonal State Space Augmented Transformers for Speech Recogni-"
"","tion”. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE."
"","2023, pp. 1–5."
"[93]","Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. “Linear Transformers are Secretly Fast Weight Programmers”."
"","In: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355–9366."
"[94]","Jürgen Schmidhuber. “Learning to control fast-weight memories: An alternative to dynamic recurrent networks”."
"","In: Neural Computation 4.1 (1992), pp. 131–139."
"",""
"","Noam Shazeer. “GLU Variants Improve Transformer”. In: arXiv preprint arXiv:2002.05202 (2020)."
"[96]","Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny"
"","Zhou. “Large Language Models can be Easily Distracted by Irrelevant Context”. In: The International Conference on"
"","Machine Learning (ICML). PMLR. 2023, pp. 31210–31227."
"[97]","Jiaxin Shi, Ke Alexander Wang, and Emily Fox. “Sequence Modeling with Multiresolution Convolutional Memory”."
"","In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312–31327."
"[98]","Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. “Simplified State Space Layers for Sequence"
"","Modeling”. In: The International Conference on Learning Representations (ICLR). 2023."
"[99]","Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. “Roformer: Enhanced Transformer"
"","with Rotary Position Embedding”. In: arXiv preprint arXiv:2104.09864 (2021)."
"[100]","Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. “Retentive"
"","network: A successor to transformer for large language models”. In: arXiv preprint arXiv:2307.08621 (2023)."
"",""
"","Ilya Sutskever, Oriol Vinyals, and Quoc V Le. “Sequence to Sequence Learning with Neural Networks”. In: Advances"
"","in Neural Information Processing Systems (NeurIPS) 27 (2014)."
"",""
"","Corentin Tallec and Yann Ollivier. “Can Recurrent Neural Networks Warp Time?” In: The International Conference"
"","on Learning Representations (ICLR). 2018."
"[103]","Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian"
"","Ruder, and Donald Metzler. “Long Range Arena: A Benchmark for Efficient Transformers”.
In:"
"","International"
"","Conference on Learning Representations (ICLR). 2021."
"",""
"","Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efficient Transformers: A Survey”. In: ACM Computing"
"","Surveys 55.6 (2022), pp. 1–28."
"[105]","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste"
"","Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and Efficient Foundation Language Models”."
"","In: arXiv preprint arXiv:2302.13971 (2023)."
"[106]","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and"
"","Illia Polosukhin. “Attention Is All You Need”. In: Advances in Neural Information Processing Systems (NeurIPS). 2017."
"[107]","Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. “On Orthogonality and Learning Recurrent"
"","Networks with Long Term Dependencies”. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570–"
"","3578."
"[108]","Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. “Selective Structured"
"","State-Spaces for Long-form Video Understanding”. In: Proceedings of the IEEE/CVF Conference on Computer Vision"
"","and Pattern Recognition. 2023, pp. 6387–6397."
"",""
"[109]","Pete Warden. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition”. In: ArXiv abs/1804.03209"
"","(2018)."
"[110]","Samuel Williams, Andrew Waterman, and David Patterson. “Roofline: An Insightful Visual Performance Model for"
"","Multicore Architectures”. In: Communications of the ACM 52.4 (2009), pp. 65–76."
"[111]","Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. “CondConv: Conditionally Parameterized Convolu-"
"","tions for Efficient Inference”. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019)."
"[112]","Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a Machine Really Finish"
"","Your Sentence?” In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."
