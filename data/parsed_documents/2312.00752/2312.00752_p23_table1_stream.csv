"[113]","Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind."
"","“An Attention Free Transformer”. In: arXiv preprint arXiv:2105.14103 (2021)."
"[114]","Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Ré. “Effectively Modeling Time"
"","Series with Simple Discrete State Spaces”. In: The International Conference on Learning Representations (ICLR). 2023."
"[115]","Lin Zheng, Chong Wang, and Lingpeng Kong. “Linear complexity randomized self-attention mechanism”.
In:"
"","International Conference on Machine Learning. PMLR. 2022, pp. 27011–27041."
"[116]","Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. “Efficient Long"
"","Sequence Modeling via State Space Augmented Transformer”. In: arXiv preprint arXiv:2212.08136 (2022)."
