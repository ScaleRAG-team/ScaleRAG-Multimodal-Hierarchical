"",""
"","Chris Donahue, Julian McAuley, and Miller Puckette. “Adversarial Audio Synthesis”. In: The International Conference"
"","on Learning Representations (ICLR). 2019."
"[26]","Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,"
"","Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. “An Image is Worth 16x16 Words:"
"","Transformers for Image Recognition at Scale”. In: The International Conference on Learning Representations (ICLR)."
"","2020."
"[27]","Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao"
"","Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,"
"","Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,"
"","Sam McCandlish, and Chris Olah. “A Mathematical Framework for Transformer Circuits”. In: Transformer Circuits"
"","Thread (2021). https://transformer-circuits.pub/2021/framework/index.html."
"[28]","Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. “Block-State"
"","Transformer”. In: arXiv preprint arXiv:2306.09539 (2023)."
"[29]","Yassir Fathullah, Chunyang Wu, Yuan Shangguan,
Junteng Jia, Wenhan Xiong,
Jay Mahadeokar, Chunxi Liu,"
"","Yangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. “Multi-Head State Space Model
for Speech"
"","Recognition”. In: Proc. INTERSPEECH 2023. 2023, pp. 241–245. doi: 10.21437/Interspeech.2023-1036."
"",""
"","Karl J Friston, Lee Harrison, and Will Penny. “Dynamic Causal Modelling”. In: Neuroimage 19.4 (2003), pp. 1273–"
"","1302."
"[31]","Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher"
"","Ré. “Simple Hardware-efficient Long Convolutions for Sequence Modeling”. In: The International Conference on"
"","Machine Learning (ICML) (2023)."
"[32]","Ken-ichi Funahashi and Yuichi Nakamura. “Approximation of Dynamical Systems by Continuous Time Recurrent"
"","Neural Networks”. In: Neural Networks 6.6 (1993), pp. 801–806."
"[33]","Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,"
"","Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. “The Pile: An 800GB Dataset of Diverse Text for"
"","Language Modeling”. In: arXiv preprint arXiv:2101.00027 (2020)."
"[34]","Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey"
"","Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin"
"","Wang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021. doi:"
"","10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628."
"[35]","Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. “It’s Raw! Audio Generation with State-Space Models”."
"","In: The International Conference on Machine Learning (ICML). 2022."
"[36]","Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. “HIPPO: Recurrent Memory with Optimal"
"","Polynomial Projections”. In: Advances in Neural Information Processing Systems (NeurIPS). 2020."
"[37]","Albert Gu, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”."
"","In: The International Conference on Learning Representations (ICLR). 2022."
"[38]","Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. “Improving the Gating Mechanism"
"","of Recurrent Neural Networks”. In: The International Conference on Machine Learning (ICML). 2020."
"[39]","Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. “On the Parameterization and Initialization of Diagonal"
"","State Space Models”. In: Advances in Neural Information Processing Systems (NeurIPS). 2022."
"[40]","Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. “Combining Recurrent,"
"","Convolutional, and Continuous-time Models with the Linear State Space Layer”. In: Advances in Neural Information"
"","Processing Systems (NeurIPS). 2021."
"[41]","Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. “How to Train Your HIPPO: State Space"
"","Models with Generalized Basis Projections”. In: The International Conference on Learning Representations (ICLR)."
"","2023."
"[42]","Ankit Gupta, Albert Gu, and Jonathan Berant. “Diagonal State Spaces are as Effective as Structured State Spaces”."
"","In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982–22994."
"[43]","Ankit Gupta, Harsh Mehta, and Jonathan Berant. “Simplifying and Understanding State Space Models with Diagonal"
"","Linear RNNs”. In: arXiv preprint arXiv:2212.00768 (2022)."
"",""
"","David Ha, Andrew Dai, and Quoc V. Le. “HyperNetworks”. In: The International Conference on Learning Representa-"
"","tions (ICLR). 2017."
"[45]","Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. “Dream to Control: Learning Behaviors by"
"","Latent Imagination”. In: The International Conference on Learning Representations (ICLR). 2020."
