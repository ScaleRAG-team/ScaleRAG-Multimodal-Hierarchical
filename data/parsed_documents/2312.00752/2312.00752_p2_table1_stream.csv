"Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs","(Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu,"
"and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been",""
"successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022;",""
"Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such",""
"as text.",""
"We propose a new class of selective state space models, that improves on prior work on several axes to achieve the",""
"modeling power of Transformers while scaling linearly in sequence length.",""
"",""
"Selection Mechanism.","the ability to efficiently select data in an"
"input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic",""
"tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM",""
"parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information",""
"indefinitely.",""
"","This simple change poses a technical challenge for the computation of the model; in"
"Hardware-aware Algorithm.",""
"fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this",""
"with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not",""
"materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The",""
"resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to",""
"pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3× faster on A100 GPUs).",""
"","We simplify prior deep sequence model architectures by combining the design of prior SSM architectures"
"Architecture.",""
"(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous",""
"architecture design (Mamba) incorporating selective state spaces.",""
"Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them",""
"suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong",""
"performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory",""
"scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only",""
"constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency",""
"together yield performance improvements on real data up to sequence length 1M.",""
"We empirically validate Mamba’s potential as a general sequence FM backbone, in both pretraining quality and domain-",""
"specific task performance, on several types of modalities and settings:",""
