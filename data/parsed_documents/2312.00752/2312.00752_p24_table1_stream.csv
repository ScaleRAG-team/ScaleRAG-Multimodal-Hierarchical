"A
Discussion: Selection Mechanism"
"Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It"
"can also be viewed as related to â€œfast weightsâ€ (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with"
"the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept"
"that is worth clarifying."
"Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber"
"Gating."
"1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism"
"for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal"
"through time and causes inputs to interact along the sequence length dimension."
"However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction"
"(often with an activation function). For example, elementwise multiplicative components of neural network architectures"
"(that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta"
"et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of RNN"
"gating versus the popular usage of multiplicative gating actually have a very different semantic meaning."
"Hypernetworks refer to neural networks whose parameters are themselves generated by smaller"
"Hypernetworks."
"neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose"
"recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber"
"1992)."
"Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the"
"Data-dependence."
"model depend on the data (Poli et al. 2023)."
"Example: GLU Activation.
To illustrate the issues with these concepts, consider a simple diagonal linear layer ğ‘¦ = ğ‘«ğ‘¥,"
"where ğ‘« is a diagonal weight parameter. Now suppose that ğ‘« is itself generated from a linear transformation of ğ‘¥,"
"with an optional nonlinearity: ğ‘« = ğœ (ğ‘¾ğ‘¥). Since it is diagonal, the multiplication becomes an elementwise product:"
"ğ‘¦ = ğœ (ğ‘¾ğ‘¥) â—¦ ğ‘¥."
"This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative"
"â€œbranchâ€), hypernetworks (since the parameter ğ‘« is generated by another layer), and data-dependent (since ğ‘« depends"
"on the data ğ‘¥). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an"
"activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer."
"Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating,"
"Selection."
"hypernetworks, or data-dependence, so can an enormous range of other constructionsâ€”essentially anything with a"
"multiplication,
including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as"
"wellâ€”and we find it uninformative to think of them as such."
"Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1)"
"and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of Î” (Funahashi"
"and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term â€œgatingâ€ in favor of selection to"
"clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select"
"or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated"
"RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and"
"Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention."
