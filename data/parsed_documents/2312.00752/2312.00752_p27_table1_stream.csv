"features, which also allows the softmax normalization term. TransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed"
"that the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al."
"2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality."
"Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance"
"sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed"
"numerator)."
"Aside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022)"
"offers an extensive categorization of many of these."
"B.5
Long Context Models"
"Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences."
"However, these are often from a computational standpoint and have not been extensively validated. These include:"
"â€¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer"
"backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result"
"is similar to our Induction Heads extrapolation experiment (Table 2)."
"â€¢ LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100ð¾ for actual tasks."
"â€¢ Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. How-"
"ever, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality"
"improvements at 1M context are due to context length or due to more data and computation."
"â€¢
Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to"
"model audio waveforms of length 220 = 1048576, although did not discuss performance tradeoffs when controlling for"
"computation and model size."
