"B.1
S4 Variants and Derivatives"
"We describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our"
"method."
"‚Ä¢
S4 (Gu, Goel, and R√© 2022; Gu, Johnson, Goel, et al. 2021)
introduced the first structured SSM, describing diagonal"
"structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a"
"connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020)."
"‚Ä¢ DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximat-"
"ing the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022)."
"‚Ä¢
S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the"
"first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state"
"dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to"
"MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO"
"dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the"
"computation issue, (iii) adding the selection mechanism."
"Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their"
"mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where ùë® is manually set to 0,"
"instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically"
"to this setting and probe if the model has learned to automatically reset its state on episode boundaries."
"‚Ä¢ Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of"
"being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step"
"of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that"
"real-valued SSMs are empirically effective in certain settings or when combined with different architectural components."
"‚Ä¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this"
"perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally"
"and close to LTI."
"‚Ä¢
SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox"
"2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of"
"S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do"
"fast autoregressive inference directly."
