"• ARC-challenge (P. Clark et al. 2018)",""
"• ARC-easy: an easy subset of ARC-challenge",""
"• WinoGrande (Sakaguchi et al. 2021)",""
"We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for",""
"HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).",""
"E.3
DNA Modeling",""
"E.3.1
Pretraining Details",""
"We describe the dataset and training procedure of the HG38 pretraining task in more detail.",""
"","The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a"
"","total of 𝑆 = 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens"
"","(DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if"
"necessary (e.g. to get longer segments).",""
"We deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a fixed sub-segment",""
"","(e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is fixed"
"","to 34021 samples and doesn’t necessarily go through the whole genome. On the other hand, we use the entire training"
"data:",""
"","• When the context length 𝐿 is less than (or equal to) 217, we divide up each segment into non-overlapping sub-segments"
"of length 𝐿, so that there are 𝑆 × 217","total samples and 𝑆 × 217 ≈ 4.5𝐵 tokens per epoch."
"𝐿",""
"","• When the context length 𝐿 is greater than 217, we turn each segment into two samples, one that begins with the prescribed"
"","segment and one that ends with the prescribed segment. Thus each epoch has 2𝑆 items and 2𝑆𝐿 tokens per epoch. For"
