"2.2
Standard Attention Implementation",""
"","Given input sequences Q, K, V âˆˆ Rğ‘ Ã—ğ‘‘ where ğ‘ is the sequence length and ğ‘‘ is the head dimension, we want"
"to compute the attention output O âˆˆ Rğ‘ Ã—ğ‘‘:",""
"S = QKâŠ¤ âˆˆ Rğ‘ Ã— ğ‘ ,
P = softmax(S) âˆˆ Rğ‘ Ã— ğ‘ ,","O = PV âˆˆ Rğ‘ Ã—ğ‘‘,"
"where softmax is applied row-wise.2 For multi-head attention (MHA), this same computation is performed in",""
"parallel across many heads, and parallel over the batch dimension (number of","input sequences in a batch)."
"","The backward pass of attention proceeds as follows. Let dO âˆˆ Rğ‘ Ã—ğ‘‘ be the gradient of O with respect to"
"some loss function. Then by the chain rule (aka backpropagation):",""
