"compute:"
"ğ‘š = max(rowmax(S(1) ), rowmax(S(2) )) âˆˆ Rğµğ‘Ÿ"
"â„“ = rowsum(ğ‘’S(1) âˆ’ğ‘š) + rowsum(ğ‘’S(2) âˆ’ğ‘š) âˆˆ Rğµğ‘Ÿ"
"ğ‘’S(1) âˆ’ğ‘š
ğ‘’S(2) âˆ’ğ‘š(cid:105)
âˆˆ Rğµğ‘Ÿ Ã—2ğµğ‘
P = (cid:2)P(1)
P(2) (cid:3) = diag(â„“) âˆ’1 (cid:104)"
""
"(cid:20)V(1)"
"= diag(â„“) âˆ’1ğ‘’S(1) âˆ’ğ‘šV(1) + ğ‘’S(2) âˆ’ğ‘šV(2) âˆˆ Rğµğ‘Ÿ Ã—ğ‘‘.
O = (cid:2)P(1)
P(2) (cid:3)"
"V(2)"
"Online softmax instead computes â€œlocalâ€
softmax with respect to each block and rescale to get the right"
"output at the end:"
"ğ‘š (1) = rowmax(S(1) ) âˆˆ Rğµğ‘Ÿ"
") âˆˆ Rğµğ‘Ÿ
â„“ (1) = rowsum(ğ‘’S(1) âˆ’ğ‘š(1)"
"âˆˆ Rğµğ‘Ÿ Ã—ğµğ‘
P(1) = diag(â„“ (1) ) âˆ’1ğ‘’S(1) âˆ’ğ‘š(1)"
"O(1) = ËœP(1) V(1) = diag(â„“ (1) ) âˆ’1ğ‘’S(1) âˆ’ğ‘š(1)
V(1) âˆˆ Rğµğ‘Ÿ Ã—ğ‘‘"
"ğ‘š (2) = max(ğ‘š (1) , rowmax(S(2) )) = ğ‘š"
"â„“ (2) = ğ‘’ğ‘š(1) âˆ’ğ‘š(2) â„“ (1) + rowsum(ğ‘’S(2) âˆ’ğ‘š(2)
) = rowsum(ğ‘’S(1) âˆ’ğ‘š) + rowsum(ğ‘’S(2) âˆ’ğ‘š) = â„“"
"P(2) = diag(â„“ (2) ) âˆ’1ğ‘’S(2) âˆ’ğ‘š(2)"
"O(2) = diag(â„“ (1) /â„“ (2) ) âˆ’1O(1) + ËœP(2) V(2) = diag(â„“ (2) ) âˆ’1ğ‘’ğ‘  (1) âˆ’ğ‘šV(1) + diag(â„“ (2) ) âˆ’1ğ‘’ğ‘  (2) âˆ’ğ‘šV(2) = O."
"We show how FlashAttention uses online softmax to enable tiling (Fig. 1) to reduce memory reads/writes."
"Figure 1: Diagram of how FlashAttention forward pass is performed, when the key K is partitioned into"
"two blocks and the value V is also partitioned into two blocks. By computing attention with respect to"
"each block and rescaling the output, we get the right answer at the end, while avoiding expensive memory"
"reads/writes of the intermediate matrices S and P. We simplify the diagram, omitting the step in softmax"
"that subtracts each element by the row-wise max."
"4"
