"As with FlashAttention, Algorithm 1 returns
Correctness, runtime, and memory requirement."
"the correct output O = softmax(QKâŠ¤)V (with no approximation), using ğ‘‚ (ğ‘ 2ğ‘‘) FLOPs and requires ğ‘‚ (ğ‘)"
"is almost the same as
additional memory beyond inputs and output (to store the logsumexp ğ¿). The proof"
"the proof of Dao et al.
[5, Theorem 1], so we omit it here."
"3.1.2
Backward pass"
"The backward pass of FlashAttention-2 is almost the same as that of FlashAttention. We make a"
"minor tweak to only use the row-wise logsumexp ğ¿ instead of both the row-wise max and row-wise sum of"
"exponentials in the softmax. We include the backward pass description in Algorithm 2 for completeness."
"Algorithm 2 FlashAttention-2 Backward Pass"
"Require: Matrices Q, K, V, O, dO âˆˆ Rğ‘ Ã—ğ‘‘ in HBM, vector ğ¿ âˆˆ Rğ‘ in HBM, block sizes ğµğ‘, ğµğ‘Ÿ ."
""
"(cid:109) blocks"
"1: Divide Q into ğ‘‡ğ‘Ÿ =
(cid:109) blocks Q1, . . . , Qğ‘‡ğ‘Ÿ of size ğµğ‘Ÿ Ã— ğ‘‘ each, and divide K, V in to ğ‘‡ğ‘ ="
