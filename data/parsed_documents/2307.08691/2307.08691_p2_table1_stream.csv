"with no approximation, and as a result FlashAttention has seen wide adoption in large-scale training and"
"inference of Transformers."
"However, context length increases even more, FlashAttention is still not nearly as efficient as other"
"primitives such as matrix-multiply (GEMM). In particular, while FlashAttention is already 2-4× faster"
"than a standard attention implementation, the forward pass only reaches 30-50% of the theoretical maximum"
"FLOPs/s of the device (Fig. 5), while the backward pass is even more challenging, reaching only 25-35%"
"of maximum throughput on A100 GPU (Fig. 6).
In contrast, optimized GEMM can reach up to 80-90% of"
"the theoretical maximum device throughput. Through careful profiling, we observe that FlashAttention"
"still has suboptimal work partitioning between different thread blocks and warps on the GPU, causing either"
"low-occupancy or unnecessary shared memory reads/writes."
"Building on FlashAttention, we propose FlashAttention-2 with better parallelism and work"
"partitioning to address these challenges."
"In Section 3.1, we tweak the algorithms to reduce the number of non-matmul FLOPs while not changing
1."
"the output. While the non-matmul FLOPs only account for a small
fraction of the total FLOPs, they"
"take longer to perform as GPUs have specialized units for matrix multiply, and as a result the matmul"
"It is thus important to reduce
throughput can be up to 16× higher than non-matmul throughput."
"non-matmul FLOPs and spend as much time as possible doing matmul FLOPs."
"2. We propose to parallelize both the forward pass and backward pass along the sequence length dimension,"
"in addition to the batch and number of heads dimension. This increases occupancy (utilization of GPU"
"resources) in the case where the sequences are long (and hence batch size is often small)."
"3. Even within one block of attention computation, we partition the work between different warps of a"
"thread block to reduce communication and shared memory reads/writes."
"In Section 4, we empirically validate that FlashAttention-2 yields significant speedup compared to"
"even FlashAttention. Benchmarks on different settings (with or without causal mask, different head"
"dimensions) show that FlashAttention-2 achieves around 2× speedup over FlashAttention, reaching"
"up to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max"
"throughput in the backward pass. When used end-to-end to train GPT-style models, we reach training speed"
"of up to 225 TFLOPs/s per A100 GPU."
