{
  "title": null,
  "authors": [],
  "source_path": "../data/pdf/2312.11514.pdf",
  "page_count": 23,
  "processed_pages": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23
  ],
  "counts": {
    "texts": 510,
    "pictures": 8,
    "tables": 30
  },
  "stats_per_page": [
    {
      "page": 1,
      "text_blocks": 25,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 2,
      "text_blocks": 22,
      "layout_blocks": 2,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 2,
      "tables_found": 1
    },
    {
      "page": 3,
      "text_blocks": 7,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 4,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 5,
      "text_blocks": 29,
      "layout_blocks": 3,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 3,
      "tables_found": 1
    },
    {
      "page": 6,
      "text_blocks": 81,
      "layout_blocks": 3,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 3,
      "tables_found": 2
    },
    {
      "page": 7,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 8,
      "text_blocks": 28,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 9,
      "text_blocks": 20,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 10,
      "text_blocks": 16,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 11,
      "text_blocks": 23,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 12,
      "text_blocks": 19,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 13,
      "text_blocks": 3,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 14,
      "text_blocks": 18,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 15,
      "text_blocks": 60,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 16,
      "text_blocks": 12,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 17,
      "text_blocks": 10,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 18,
      "text_blocks": 23,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 19,
      "text_blocks": 8,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 20,
      "text_blocks": 56,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 4
    },
    {
      "page": 21,
      "text_blocks": 8,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 22,
      "text_blocks": 8,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 23,
      "text_blocks": 8,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    }
  ],
  "texts": [
    {
      "page_no": 1,
      "bbox": [
        98.26400756835938,
        78.74005126953125,
        497.51885986328125,
        109.02625274658203
      ],
      "text": "LLM in a flash:\nEfficient Large Language Model Inference with Limited Memory"
    },
    {
      "page_no": 1,
      "bbox": [
        97.21707153320312,
        132.74920654296875,
        498.0629577636719,
        175.2865753173828
      ],
      "text": "Keivan Alizadeh, Iman Mirzadeh* , Dmitry Belenko* , S. Karen Khatamifard,\nMinsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar\nApple †"
    },
    {
      "page_no": 1,
      "bbox": [
        157.75808715820312,
        221.69024658203125,
        202.24337768554688,
        233.64544677734375
      ],
      "text": "Abstract"
    },
    {
      "page_no": 1,
      "bbox": [
        87.51599884033203,
        245.060791015625,
        273.8641052246094,
        637.6570434570312
      ],
      "text": "Large language models (LLMs) are central to\nmodern natural language processing, delivering\nexceptional performance in various tasks. How-\never, their substantial computational and mem-\nory requirements present challenges, especially\nfor devices with limited DRAM capacity. This\npaper tackles the challenge of efficiently run-\nning LLMs that exceed the available DRAM\ncapacity by storing the model parameters in\nflash memory, but bringing them on demand\nto DRAM. Our method involves constructing\nan inference cost model that takes into account\nthe characteristics of flash memory, guiding\nus to optimize in two critical areas: reduc-\ning the volume of data transferred from flash\nand reading data in larger, more contiguous\nchunks. Within this hardware-informed frame-\nwork, we introduce two principal techniques.\nFirst, “windowing” strategically reduces data\ntransfer by reusing previously activated neu-\nrons, and second, “row-column bundling”, tai-\nlored to the sequential data access strengths\nof flash memory, increases the size of data\nchunks read from flash memory. These meth-\nods collectively enable running models up to\ntwice the size of the available DRAM, with\nup to 4x and 20x increase in inference speed\ncompared to naive loading approaches in CPU\nand GPU, respectively. Our integration of spar-\nsity awareness, context-adaptive loading, and\na hardware-oriented design paves the way for\neffective inference of LLMs on devices with\nlimited memory."
    },
    {
      "page_no": 1,
      "bbox": [
        70.86599731445312,
        649.8841552734375,
        153.67967224121094,
        661.83935546875
      ],
      "text": "1\nIntroduction"
    },
    {
      "page_no": 1,
      "bbox": [
        70.4729995727539,
        671.599365234375,
        291.0414733886719,
        736.8140258789062
      ],
      "text": "In recent years, large language models (LLMs)\nhave demonstrated strong performance across a\nwide range of natural language tasks (Brown et al.,\n2020; Chowdhery et al., 2022; Touvron et al.,\n2023a; Jiang et al., 2023; Gemini Team, 2023)."
    },
    {
      "page_no": 1,
      "bbox": [
        70.86599731445312,
        742.7809448242188,
        290.2557678222656,
        775.0003662109375
      ],
      "text": "* Major Contribution\n† {kalizadehvahid, imirzadeh, d_belenko, skhatamifard,\nminsik, cdelmundo, mrastegari, farajtabar}@apple.com"
    },
    {
      "page_no": 1,
      "bbox": [
        346.32666015625,
        344.953369140625,
        363.7876892089844,
        352.7283630371094
      ],
      "text": "Naive"
    },
    {
      "page_no": 1,
      "bbox": [
        348.8096618652344,
        360.7927551269531,
        385.39141845703125,
        368.5677490234375
      ],
      "text": "Llama 2-7B"
    },
    {
      "page_no": 1,
      "bbox": [
        356.8095703125,
        369.2261657714844,
        377.40679931640625,
        377.00115966796875
      ],
      "text": "(CPU)"
    },
    {
      "page_no": 1,
      "bbox": [
        371.73028564453125,
        344.953369140625,
        424.0616149902344,
        352.7283630371094
      ],
      "text": "Ours\nNaive"
    },
    {
      "page_no": 1,
      "bbox": [
        411.0590515136719,
        360.7927551269531,
        443.6925048828125,
        368.5677490234375
      ],
      "text": "OPT-6.7B"
    },
    {
      "page_no": 1,
      "bbox": [
        417.0834655761719,
        369.2261657714844,
        437.68072509765625,
        377.00115966796875
      ],
      "text": "(CPU)"
    },
    {
      "page_no": 1,
      "bbox": [
        432.00421142578125,
        344.953369140625,
        484.3355407714844,
        352.7283630371094
      ],
      "text": "Ours\nNaive"
    },
    {
      "page_no": 1,
      "bbox": [
        471.33294677734375,
        360.7927551269531,
        503.9664611816406,
        368.5677490234375
      ],
      "text": "OPT-6.7B"
    },
    {
      "page_no": 1,
      "bbox": [
        477.25262451171875,
        369.2261657714844,
        498.05792236328125,
        377.00115966796875
      ],
      "text": "(GPU)"
    },
    {
      "page_no": 1,
      "bbox": [
        324.6019592285156,
        335.67462158203125,
        507.1577453613281,
        352.7283630371094
      ],
      "text": "Ours\n100"
    },
    {
      "page_no": 1,
      "bbox": [
        321.3009948730469,
        305.6564636230469,
        334.4931335449219,
        330.7500915527344
      ],
      "text": "450\n700\n1000"
    },
    {
      "page_no": 1,
      "bbox": [
        321.3009948730469,
        263.9646301269531,
        334.4892578125,
        270.7138366699219
      ],
      "text": "2250"
    },
    {
      "page_no": 1,
      "bbox": [
        321.3009948730469,
        235.61416625976562,
        334.4892578125,
        242.3633575439453
      ],
      "text": "3100"
    },
    {
      "page_no": 1,
      "bbox": [
        310.6363830566406,
        249.55435180664062,
        319.3746032714844,
        327.62554931640625
      ],
      "text": "Inference Latency (ms)"
    },
    {
      "page_no": 1,
      "bbox": [
        348.4883117675781,
        225.72308349609375,
        517.0791625976562,
        232.52621459960938
      ],
      "text": "Compute\nLoad From Flash\nMemory Management"
    },
    {
      "page_no": 1,
      "bbox": [
        305.7829895019531,
        391.1817932128906,
        526.0612182617188,
        460.98809814453125
      ],
      "text": "Figure 1: Average inference latency for a single token\nwhen only half of the model’s memory is available: Our\nmethod selectively loads parameters on demand for each\ntoken generation step. The latency represents the time\nrequired to repeatedly load parameters from flash mem-\nory, combined with the time needed for computations."
    },
    {
      "page_no": 1,
      "bbox": [
        305.7489929199219,
        489.0953369140625,
        526.3173828125,
        775.4959716796875
      ],
      "text": "However, the unprecedented capabilities of these\nmodels come with substantial computational and\nmemory requirements for inference. LLMs can\ncontain hundreds of billions or even trillions of pa-\nrameters, which makes them challenging to load\nand run efficiently, especially on personal devices.\nCurrently, the standard approach is to load the en-\ntire model into DRAM (Dynamic Random Access\nMemory) for inference (Rajbhandari et al., 2021;\nAminabadi et al., 2022). However, this severely\nlimits the maximum model size that can be run.\nFor example, a 7 billion parameter model requires\nover 14GB of memory just to load the parameters\nin half-precision floating point format, exceeding\nthe capabilities of most personal devices such as\nsmartphones. While it is possible to employ tech-\nniques such as quantization to reduce the model\nsize, still, this cannot address the main limitation\nof loading the entire model into DRAM.\nTo address this limitation, we propose to store\nthe model parameters in flash memory, which is"
    },
    {
      "page_no": 1,
      "bbox": [
        10.940000534057617,
        269.90997314453125,
        37.619998931884766,
        609.8900146484375
      ],
      "text": "arXiv:2312.11514v3  [cs.CL]  30 Jul 2024"
    },
    {
      "page_no": 2,
      "bbox": [
        96.42500305175781,
        194.70901489257812,
        267.2528991699219,
        203.6754150390625
      ],
      "text": "(a) Bandwidth in a unified memory architecture"
    },
    {
      "page_no": 2,
      "bbox": [
        367.2564392089844,
        169.71127319335938,
        518.3425903320312,
        186.209716796875
      ],
      "text": "4 8\n16\n32\n64\nChunk Size (KB)"
    },
    {
      "page_no": 2,
      "bbox": [
        356.9232482910156,
        162.3938751220703,
        359.4715270996094,
        167.61026000976562
      ],
      "text": "0"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        148.01841735839844,
        359.4624938964844,
        153.23480224609375
      ],
      "text": "1000"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        133.6429443359375,
        359.4624938964844,
        138.8593292236328
      ],
      "text": "2000"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        119.26747131347656,
        359.4624938964844,
        124.4838638305664
      ],
      "text": "3000"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        104.89200592041016,
        359.4624938964844,
        110.1083984375
      ],
      "text": "4000"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        90.51653289794922,
        359.4624938964844,
        95.73292541503906
      ],
      "text": "5000"
    },
    {
      "page_no": 2,
      "bbox": [
        349.2694091796875,
        76.14106750488281,
        359.4624938964844,
        81.35746002197266
      ],
      "text": "6000"
    },
    {
      "page_no": 2,
      "bbox": [
        340.74505615234375,
        71.18525695800781,
        347.9677429199219,
        170.00564575195312
      ],
      "text": "Random Read Throughput (MB/s)"
    },
    {
      "page_no": 2,
      "bbox": [
        364.7930908203125,
        80.49015808105469,
        450.9979553222656,
        94.63958740234375
      ],
      "text": "Upper Bound (Sequential Read)\nThreads"
    },
    {
      "page_no": 2,
      "bbox": [
        378.532470703125,
        96.7864761352539,
        383.73712158203125,
        133.35220336914062
      ],
      "text": "32\n16\n8\n4\n2"
    },
    {
      "page_no": 2,
      "bbox": [
        349.60101318359375,
        194.70901489257812,
        512.9688720703125,
        203.6754150390625
      ],
      "text": "(b) Random read throughput of flash memory"
    },
    {
      "page_no": 2,
      "bbox": [
        70.86599731445312,
        215.18255615234375,
        524.4069213867188,
        249.14608764648438
      ],
      "text": "Figure 2: (a) Flash memory offers significantly higher capacity but suffers from much lower bandwidth compared\nto DRAM and CPU/GPU caches and registers. (b) The throughput for random reads in flash memory increases with\nthe size of sequential chunks and the number of threads."
    },
    {
      "page_no": 2,
      "bbox": [
        70.52799987792969,
        272.8830871582031,
        291.04449462890625,
        338.041748046875
      ],
      "text": "at least an order of magnitude larger than DRAM.\nThen, during inference, we directly load the re-\nquired subset of parameters from the flash memory,\navoiding the need to fit the entire model in DRAM.\nTo this end, our work makes several contributions:"
    },
    {
      "page_no": 2,
      "bbox": [
        70.86599731445312,
        347.1393127441406,
        290.9472961425781,
        616.561767578125
      ],
      "text": "• First, we study the hardware characteristics of\nstorage systems (e.g., flash, DRAM). We show\nthat hardware constraints such as capacity and\nbandwidth limitations can have significant con-\nsiderations when designing efficient algorithms\nfor serving LLMs from flash (Section 2).\n• Motivated by our findings, we propose several\ntechniques that can help with (i) reducing the\nrequired data transfer, (ii) increasing the transfer\nthroughput, and (iii) managing loaded parameters\nefficiently in DRAM (Section 3).\n• Finally, as partially demonstrated in Figure 1,\nwe show that our proposed techniques for opti-\nmizing the cost model and selectively loading\nparameters on demand allows us to run models\n2x larger than the device’s DRAM capacity and\nspeed up inference up to 4x, 7x, and 20x com-\npared to naive implementation in CPU, Metal and\nNVIDIA GPU backends, respectively (Section\n4)."
    },
    {
      "page_no": 2,
      "bbox": [
        70.86599731445312,
        647.7141723632812,
        257.91705322265625,
        659.6693725585938
      ],
      "text": "2\nFlash Memory & LLM Inference"
    },
    {
      "page_no": 2,
      "bbox": [
        70.4729995727539,
        683.183349609375,
        290.94915771484375,
        775.4697265625
      ],
      "text": "In this section, we explore the characteristics of\nmemory storage systems (e.g., flash, DRAM), and\ntheir implications for large language model (LLM)\ninference. We aim to understand the challenges and\nhardware-specific considerations essential for algo-\nrithm design, particularly in optimizing inference\nwhen working with flash memory."
    },
    {
      "page_no": 2,
      "bbox": [
        306.1419982910156,
        272.8370666503906,
        496.2002868652344,
        283.7461853027344
      ],
      "text": "2.1\nBandwidth and Energy Constraints"
    },
    {
      "page_no": 2,
      "bbox": [
        305.6289978027344,
        290.4803161621094,
        526.2252197265625,
        572.53076171875
      ],
      "text": "While modern NAND flash memories offer high\nbandwidth and low latency, they fall well short\nof the performance levels of DRAM (Dynamic\nRandom-Access Memory), in terms of both latency\nand throughput. Figure 2a illustrates these differ-\nences. A naive inference implementation that relies\non NAND flash memory might necessitate reload-\ning the entire model for each forward pass. This\nprocess is not only time-consuming, often taking\nseconds for even compressed models, but it also\nconsumes more energy than transferring data from\nDRAM to the CPU or GPU’s internal memory.\nLoad times for the models can be a problem\neven in the traditional DRAM-resident setup where\nweights are not reloaded partially – the initial, full\nload of the model still incurs a penalty, particu-\nlarly in situations requiring rapid response times\nfor the first token. Our approach, leveraging activa-\ntion sparsity in LLMs, addresses these challenges\nby enabling selective reading of model weights,\nthereby reducing the response latency."
    },
    {
      "page_no": 2,
      "bbox": [
        306.1419982910156,
        584.1690673828125,
        414.4475402832031,
        595.078125
      ],
      "text": "2.2\nRead Throughput"
    },
    {
      "page_no": 2,
      "bbox": [
        306.1419982910156,
        601.8133544921875,
        526.3155517578125,
        775.4959716796875
      ],
      "text": "Flash memory systems perform optimally with\nlarge sequential reads. For instance, benchmarks\non an Apple MacBook M1 Max with 1TB flash\nmemory demonstrate speeds exceeding 6 GiB/s for\na 1GiB linear read of an uncached file. However,\nthis high bandwidth cannot be achieved for smaller,\nrandom reads due to the inherent multi-phase na-\nture of these reads, encompassing the operating\nsystem, drivers, interrupt handling, and the flash\ncontroller, among others. Each phase introduces\nlatency, disproportionately affecting smaller reads.\nTo circumvent these limitations, we advocate\ntwo primary strategies, which can be employed"
    },
    {
      "page_no": 3,
      "bbox": [
        70.4729995727539,
        73.55166625976562,
        291.0416564941406,
        477.6717529296875
      ],
      "text": "jointly. The first involves reading larger chunks of\ndata. For smaller blocks, a substantial part of the\noverall read time is spent waiting for data transfer\nto begin.\nThis is often referred to as latency\nto first byte.\nThis latency reduces the overall\nthroughput of each read operation considerably\nbecause the overall measured throughput has to\ntake into account not just the speed of transfer\nonce it begins, but the latency before it begins as\nwell, which penalizes small reads. This means\nthat if we coalesce the reads for rows and columns\nof the FFN matrices, we can pay the latency cost\nonly once for any given row/column pair in both\nmatrices and higher throughput can be realized.\nThis principle is depicted in Figure 2b. Perhaps\na counterintuitive yet interesting observation is\nthat in some scenarios, it will be worthwhile to\nread more than needed (but in larger chunks) and\nthen discard, rather than only reading strictly the\nnecessary parts but in smaller chunks. The second\nstrategy leverages parallelized reads, utilizing\nthe inherent parallelism within storage stacks\nand flash controllers.\nOur results indicate that\nthroughputs appropriate for sparse LLM inference\nare achievable on modern hardware using 32KiB\nor larger random reads across multiple threads.\nMotivated by the challenges described in this sec-\ntion, in Section 3, we propose methods to optimize\ndata transfer volume and enhance read throughput\nto significantly enhance inference speeds."
    },
    {
      "page_no": 3,
      "bbox": [
        70.86599731445312,
        490.61016845703125,
        177.59007263183594,
        502.56536865234375
      ],
      "text": "3\nLoad From Flash"
    },
    {
      "page_no": 3,
      "bbox": [
        70.4729995727539,
        513.1846313476562,
        291.04791259765625,
        715.292724609375
      ],
      "text": "This section addresses the challenge of conducting\ninference on devices where the available DRAM\nis substantially smaller than the size of the model.\nThis necessitates storing the full model weights in\nflash memory. Our primary metric for evaluating\nvarious flash loading strategies is latency, dissected\ninto three distinct components: the I/O cost of load-\ning from flash, the overhead of managing memory\nwith newly loaded data, and the compute cost for\ninference operations.\nOur proposed solutions for reducing latency un-\nder memory constraints are categorized into areas:\n1. Reducing Data Load: Aiming to decrease la-\ntency associated with flash I/O operations by\nloading less data1."
    },
    {
      "page_no": 3,
      "bbox": [
        70.86599731445312,
        724.6417236328125,
        290.6233825683594,
        775.0003662109375
      ],
      "text": "1It is notable that, by data we often refer to the weights of\nthe neural network. However, the techniques we have devel-\noped can be easily generalized to other data types transferred\nand used for LLM inference, such as activations or KV cache,\nas suggested by Sheng et al. (2023)."
    },
    {
      "page_no": 3,
      "bbox": [
        306.1419982910156,
        73.53621673583984,
        525.9263916015625,
        262.0947265625
      ],
      "text": "2. Optimizing Data Chunk Size: Enhancing flash\nthroughput by increasing the size of data chunks\nloaded, thereby mitigating latency.\n3. Efficient Management of Loaded Data:\nStreamlining the management of data once it\nis loaded into memory to minimize overhead.\nIt is important to note that our focus is not on\noptimizing the compute, as it is orthogonal to the\ncore concerns of our work. Instead, we concentrate\non optimizing flash memory interactions and\nmemory management to achieve efficient inference\non memory-constrained devices. We will elaborate\non the implementation details of these strategies\nin the experimental setup section."
    },
    {
      "page_no": 3,
      "bbox": [
        306.1419982910156,
        272.7630310058594,
        443.203857421875,
        283.6721496582031
      ],
      "text": "3.1\nReducing Data Transfer"
    },
    {
      "page_no": 3,
      "bbox": [
        305.7489929199219,
        290.33453369140625,
        526.2252807617188,
        775.4959716796875
      ],
      "text": "Our method leverages the inherent activation spar-\nsity found in Feed-Forward Network (FFN) mod-\nels, as documented in preceding research. The OPT\n6.7B model, for instance, exhibits a notable 97%\nsparsity within its FFN layer. Similarly, the Falcon\n7B model has been adapted through fine-tuning,\nwhich involves swapping their activation functions\nto ReLU, resulting in 95% sparsity while being\nsimilar in accuracy (Mirzadeh et al., 2023). Replac-\ning activations of Llama 2 model (Touvron et al.,\n2023b) by FATReLU and finetuning can achieve\n90% sparsity(Song et al., 2024). In light of this\ninformation, our approach involves the iterative\ntransfer of only the essential, dynamic subset of the\nweights from flash memory to DRAM for process-\ning during inference.\nSelective Persistence Strategy. We opt to re-\ntain the embeddings and matrices within the at-\ntention mechanism of the transformer constantly\nin DRAM. For the Feed-Forward Network (FFN)\nportions, only the non-sparse segments are dynam-\nically loaded into DRAM as needed. Keeping at-\ntention weights, which constitute approximately\none-third of the model’s size, in memory, allows\nfor more efficient computation and quicker access,\nthereby enhancing inference performance without\nthe need for full model loading.\nAnticipating ReLU Sparsity.\nThe ReLU\nactivation function naturally induces over 90%\nsparsity in the FFN’s intermediate outputs, which\nreduces the memory footprint for subsequent\nlayers that utilize these sparse outputs. However,\nthe preceding layer, namely the up project, must\nbe fully present in memory.\nTo avoid loading the entire up projection ma-\ntrix, we follow Liu et al. (2023b), and employ a"
    },
    {
      "page_no": 4,
      "bbox": [
        78.00325012207031,
        190.78311157226562,
        244.9365234375,
        204.60220336914062
      ],
      "text": "−4\n−3\n−2\n−1\n0\n1\n2\nOutput Magnitude (before ReLU)"
    },
    {
      "page_no": 4,
      "bbox": [
        73.22920227050781,
        125.07955932617188,
        80.27802276611328,
        141.6876220703125
      ],
      "text": "Count"
    },
    {
      "page_no": 4,
      "bbox": [
        190.31869506835938,
        146.94549560546875,
        230.0336151123047,
        153.9943084716797
      ],
      "text": "False Negative"
    },
    {
      "page_no": 4,
      "bbox": [
        206.9801788330078,
        82.99652099609375,
        238.5229949951172,
        96.46726989746094
      ],
      "text": "Up Projection\nPredictor"
    },
    {
      "page_no": 4,
      "bbox": [
        123.67900085449219,
        213.55502319335938,
        194.63909912109375,
        222.52142333984375
      ],
      "text": "(a) predictor vs relu"
    },
    {
      "page_no": 4,
      "bbox": [
        414.3583679199219,
        76.062744140625,
        436.984130859375,
        84.20305633544922
      ],
      "text": "M = dffn"
    },
    {
      "page_no": 4,
      "bbox": [
        376.0589904785156,
        213.55502319335938,
        454.7660217285156,
        222.52142333984375
      ],
      "text": "(b) low rank predictor"
    },
    {
      "page_no": 4,
      "bbox": [
        70.86599731445312,
        233.95240783691406,
        524.4115600585938,
        279.9471130371094
      ],
      "text": "Figure 3: (a) Preactivations of tokens in one sequence in OPT 6.7B. The blue graph shows the preactivation of\nelements that the predictor detected as positive while the green graph is for up projection. As it can be seen most\nof the False Positives are close to 0 and False Negatives constitute a small portion of the elements. (b) A small\nlow-rank predictor finds out which intermediate neurons are going to be activated."
    },
    {
      "page_no": 4,
      "bbox": [
        70.55699920654297,
        301.8919677734375,
        290.7828674316406,
        335.73809814453125
      ],
      "text": "Table 1: The low-rank predictor has a marginal impact\non zero-shot metrics as the predictor of each layer accu-\nrately identifies sparsity."
    },
    {
      "page_no": 4,
      "bbox": [
        88.05107116699219,
        351.421875,
        271.61871337890625,
        360.99517822265625
      ],
      "text": "Zero-Shot Task\nOPT 6.7B\nwith Predictor"
    },
    {
      "page_no": 4,
      "bbox": [
        88.05107116699219,
        368.4012756347656,
        250.4330596923828,
        401.7544250488281
      ],
      "text": "Arc Easy\n66.1\n66.2\nArc Challenge\n30.6\n30.6\nHellaSwag\n50.3\n49.8"
    },
    {
      "page_no": 4,
      "bbox": [
        70.0479965209961,
        424.13983154296875,
        291.0458068847656,
        775.4432373046875
      ],
      "text": "low-rank predictor to identify the elements zeroed\nby ReLU (see Figure 3b). We used a balanced\nloss over negative and positive samples of each\nlayer. In contrast to their work, our predictor needs\nonly the output of the current layer’s attention mod-\nule and not the previous layer’s FFN module. We\nhave observed that postponing the prediction to\nthe current layer is sufficient for hardware-aware\nweight-loading algorithm design but leads to more\naccurate outcomes due to deferred inputs. We used\n10000 samples from the C4 training dataset to do\nthe training for 2 epochs. It took 4 hours on an\nA100 GPU to train each predictor.\nWe thereby only load elements indicated by the\npredictor, as shown in Figure 3a. Furthermore, as\ndemonstrated in Table 1, using predictors does not\nadversely affect the model’s performance in 0-shot\ntasks. For more details please refer to Appendix B.\nThe Sliding Window Technique. In our study,\nwe define an active neuron as one that yields a\npositive output in our low-rank predictor model.\nOur approach focuses on managing neuron data by\nemploying a Sliding Window Technique. This tech-\nnique entails maintaining a DRAM cache of only\nthe weight rows that were predicted to be required\nby the recent subset of input tokens. The key aspect"
    },
    {
      "page_no": 4,
      "bbox": [
        305.7489929199219,
        303.7874450683594,
        526.2220458984375,
        626.4037475585938
      ],
      "text": "of this technique is the incremental loading of neu-\nron data that differs between the current input token\nand its immediate predecessors. This strategy al-\nlows for efficient memory utilization, as it frees up\nmemory resources previously allocated to cached\nweights required by tokens that are no longer within\nthe sliding window (as depicted in Figure 4b).\nFrom a mathematical standpoint, let sagg(k)\ndenote the cumulative use of neuron data across\na sequence of k input tokens. Our memory archi-\ntecture is designed to store an average of sagg(k)\nin DRAM. As we process each new token, the\nincremental neuron data, which is mathematically\nrepresented as sagg(k + 1) −sagg(k), is loaded\nfrom flash memory into DRAM. This practice\nis grounded in the observed trend of decreasing\naggregated neuron usage over time. Consequently,\nlarger values of k result in a lesser volume of data\nbeing loaded for each new token (refer to Figure\n4a), while smaller values of k can help conserve\nDRAM that is used to store the cached weights. In\ndetermining the size of the sliding window, the aim\nis to maximize it within the constraints imposed\nby the available memory capacity."
    },
    {
      "page_no": 4,
      "bbox": [
        306.1419982910156,
        638.216064453125,
        482.2257385253906,
        649.1251220703125
      ],
      "text": "3.2\nIncreasing Transfer Throughput"
    },
    {
      "page_no": 4,
      "bbox": [
        305.8039855957031,
        656.0375366210938,
        526.2173461914062,
        775.4432373046875
      ],
      "text": "To increase data throughput from flash memory, it\nis crucial to read data in larger chunks, preferably\nsized as the multiples of the block size of the un-\nderlying storage pool. In this section, we detail the\nstrategy we have employed to augment the chunk\nsizes for more efficient flash memory reads.\nBundling Columns and Rows. Note that in the\nFFN layer, the usage of the ith column from the up\nprojection and the ith row from the down projection"
    },
    {
      "page_no": 5,
      "bbox": [
        95.86256408691406,
        194.26519775390625,
        240.19833374023438,
        208.2985076904297
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nWindow size (k)"
    },
    {
      "page_no": 5,
      "bbox": [
        84.4515609741211,
        184.38973999023438,
        87.17925262451172,
        189.97341918945312
      ],
      "text": "0"
    },
    {
      "page_no": 5,
      "bbox": [
        81.72064208984375,
        163.9588165283203,
        87.176025390625,
        169.54249572753906
      ],
      "text": "10"
    },
    {
      "page_no": 5,
      "bbox": [
        81.72064208984375,
        143.5279083251953,
        87.176025390625,
        149.11158752441406
      ],
      "text": "20"
    },
    {
      "page_no": 5,
      "bbox": [
        81.72064208984375,
        123.09699249267578,
        87.176025390625,
        128.68067932128906
      ],
      "text": "30"
    },
    {
      "page_no": 5,
      "bbox": [
        81.72064208984375,
        102.66607666015625,
        87.176025390625,
        108.249755859375
      ],
      "text": "40"
    },
    {
      "page_no": 5,
      "bbox": [
        81.72064208984375,
        82.23515319824219,
        87.176025390625,
        87.81883239746094
      ],
      "text": "50"
    },
    {
      "page_no": 5,
      "bbox": [
        73.28968811035156,
        119.84151458740234,
        80.51890563964844,
        150.70108032226562
      ],
      "text": "Percentage"
    },
    {
      "page_no": 5,
      "bbox": [
        181.37551879882812,
        102.95870971679688,
        238.06065368652344,
        110.99118041992188
      ],
      "text": "Aggregated Usage"
    },
    {
      "page_no": 5,
      "bbox": [
        172.0258026123047,
        174.4669189453125,
        236.27830505371094,
        182.49937438964844
      ],
      "text": "Incremental Transfer"
    },
    {
      "page_no": 5,
      "bbox": [
        104.81057739257812,
        82.56681060791016,
        123.62691497802734,
        89.84821319580078
      ],
      "text": "sagg(k)"
    },
    {
      "page_no": 5,
      "bbox": [
        104.81057739257812,
        92.80877685546875,
        161.71530151367188,
        104.56063842773438
      ],
      "text": "sagg(k + 1) −sagg(k)"
    },
    {
      "page_no": 5,
      "bbox": [
        110.85099792480469,
        217.32400512695312,
        212.00096130371094,
        226.2904052734375
      ],
      "text": "(a) aggregated neuron usage"
    },
    {
      "page_no": 5,
      "bbox": [
        281.3438720703125,
        117.14311981201172,
        510.31304931640625,
        120.91570281982422
      ],
      "text": "0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
    },
    {
      "page_no": 5,
      "bbox": [
        281.31878662109375,
        178.4764404296875,
        510.2879943847656,
        182.24903869628906
      ],
      "text": "0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
    },
    {
      "page_no": 5,
      "bbox": [
        301.033203125,
        201.82122802734375,
        512.8656616210938,
        208.41592407226562
      ],
      "text": "Neurons to be deleted\nNew Neurons\nNeurons from initial window"
    },
    {
      "page_no": 5,
      "bbox": [
        358.67156982421875,
        107.88504028320312,
        445.19940185546875,
        114.17268371582031
      ],
      "text": "Active neurons in the initial window"
    },
    {
      "page_no": 5,
      "bbox": [
        362.69573974609375,
        169.20697021484375,
        446.7599792480469,
        175.49461364746094
      ],
      "text": "Active neurons in the new window"
    },
    {
      "page_no": 5,
      "bbox": [
        378.96502685546875,
        73.45297241210938,
        421.38763427734375,
        80.9981460571289
      ],
      "text": "Initial Window"
    },
    {
      "page_no": 5,
      "bbox": [
        296.34503173828125,
        90.00144958496094,
        493.1665954589844,
        95.03157043457031
      ],
      "text": "Once\nUpon\nA\nTime\nThere\nWas\nA\nKid\nWho\nHad\nA\nDream"
    },
    {
      "page_no": 5,
      "bbox": [
        377.61383056640625,
        135.7394256591797,
        425.94976806640625,
        143.2845916748047
      ],
      "text": "Sliding Window"
    },
    {
      "page_no": 5,
      "bbox": [
        296.34503173828125,
        153.0078125,
        493.1665954589844,
        158.0379180908203
      ],
      "text": "Once\nUpon\nA\nTime\nThere\nWas\nA\nKid\nWho\nHad\nA\nDream"
    },
    {
      "page_no": 5,
      "bbox": [
        365.5270080566406,
        217.32400512695312,
        433.5461730957031,
        226.2904052734375
      ],
      "text": "(b) sliding window"
    },
    {
      "page_no": 5,
      "bbox": [
        70.86599731445312,
        237.72145080566406,
        524.4120483398438,
        283.7160949707031
      ],
      "text": "Figure 4: (a) Aggregated neuron usage of the tenth layer of Falcon 7B: the slope of aggregated neuron usage is\ndecreasing. Other layers exhibit the same pattern. (b) Rather than deleting neurons that were brought to DRAM we\nkeep the active neurons of past k tokens (we use k = 5): when the new token \"Was\" is being processed only a small\nfraction of new weights need to be loaded."
    },
    {
      "page_no": 5,
      "bbox": [
        70.4729995727539,
        307.25421142578125,
        290.94921875,
        631.708740234375
      ],
      "text": "coincides with the activation of the ith intermediate\nneuron. Consequently, by storing these correspond-\ning columns and rows together in flash memory,\nwe can consolidate the data into larger chunks for\nreading. Refer to Figure 5 for an illustration of this\nbundling approach. If each element of weights of\nthe network is stored in num_bytes such bundling\ndoubles the chunk size from dmodel×num_bytes to\n2dmodel×num_bytes as shown in Figure 5. Our\nanalysis and experiment show this increases the\nthroughput of the model.\nBundling Based on Co-activation. We hypoth-\nesized that neurons might exhibit highly correlated\nactivity patterns, enabling bundling. By analyzing\nactivations on the C4 validation dataset, we found\na power law distribution of coactivations. However,\nbundling neurons with their highest coactivated\nneuron (closest friend) led to multiple loadings of\nhighly active neurons, counteracting our goal. This\nresult suggests that very active neurons are the clos-\nest friends of many others. We present this negative\nresult to inspire future research on effective neu-\nron bundling for efficient inference. Please refer to\nAppendix D for details."
    },
    {
      "page_no": 5,
      "bbox": [
        70.86599731445312,
        648.841064453125,
        282.0552673339844,
        659.7501220703125
      ],
      "text": "3.3\nOptimized Data Management in DRAM"
    },
    {
      "page_no": 5,
      "bbox": [
        70.4729995727539,
        669.6343383789062,
        290.9490051269531,
        775.4959716796875
      ],
      "text": "Although data transfer within DRAM is more ef-\nficient compared to accessing flash memory, it still\nincurs a non-negligible cost. When introducing\ndata for new neurons, reallocating the matrix and\nappending new matrices can lead to significant\noverhead due to the need for rewriting existing\nneuron data in DRAM. This is particularly costly\nwhen a substantial portion (approximately 25%)"
    },
    {
      "page_no": 5,
      "bbox": [
        306.1419982910156,
        455.0343017578125,
        524.4085083007812,
        488.9820861816406
      ],
      "text": "Figure 5: By bundling columns of the up project and\nrows of the down project layer, we can load 2x chunks\ninstead of reading columns or rows separately."
    },
    {
      "page_no": 5,
      "bbox": [
        304.33099365234375,
        517.9798583984375,
        526.3216552734375,
        775.4432373046875
      ],
      "text": "of the Feed-Forward Networks (FFNs) in DRAM\nneeds to be rewritten. To address this issue, we\nadopt an alternative memory management strategy.\nThis involves the preallocation of all necessary\nmemory and the establishment of a corresponding\ndata structure for efficient management. The data\nstructure comprises elements such as pointers,\nmatrix, bias, num_used, and last_k_active\nshown in Figure 6.\nEach row in the matrix represents the concate-\nnated row of the ‘up project’ and the column of\nthe ‘down project’ of a neuron. The pointer vec-\ntor indicates the original neuron index correspond-\ning to each row in the matrix. The bias for the\n‘up project’ in the original model is represented in\nthe corresponding bias element. The num_used\nparameter tracks the number of rows currently\nutilized in the matrix, initially set to zero. The\nmatrix for the ith layer is pre-allocated with a size"
    },
    {
      "page_no": 6,
      "bbox": [
        206.66493225097656,
        86.63031768798828,
        209.4481658935547,
        94.90478515625
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        204.5078887939453,
        97.02127838134766,
        211.60409545898438,
        105.29574584960938
      ],
      "text": "10"
    },
    {
      "page_no": 6,
      "bbox": [
        204.58636474609375,
        107.41221618652344,
        211.52528381347656,
        115.68668365478516
      ],
      "text": "15"
    },
    {
      "page_no": 6,
      "bbox": [
        205.97708129882812,
        117.80316925048828,
        210.13482666015625,
        126.07763671875
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        128.19412231445312,
        211.02085876464844,
        136.4685821533203
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        138.5850830078125,
        211.02085876464844,
        146.8595428466797
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        148.9759979248047,
        211.02085876464844,
        157.25045776367188
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        159.366943359375,
        211.02085876464844,
        167.6414031982422
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        169.75790405273438,
        211.02085876464844,
        178.03236389160156
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        205.09222412109375,
        180.1488494873047,
        211.02085876464844,
        188.42330932617188
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        118.20175170898438,
        95.34768676757812,
        130.17544555664062,
        100.47648620605469
      ],
      "text": "Copy"
    },
    {
      "page_no": 6,
      "bbox": [
        326.79296875,
        73.9649429321289,
        343.9349670410156,
        80.17079162597656
      ],
      "text": "Pointer"
    },
    {
      "page_no": 6,
      "bbox": [
        335.63226318359375,
        86.63031768798828,
        338.4154968261719,
        94.90478515625
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.9443664550781,
        97.04310607910156,
        339.10211181640625,
        105.31757354736328
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        333.5535888671875,
        107.45588684082031,
        340.4925231933594,
        115.73035430908203
      ],
      "text": "15"
    },
    {
      "page_no": 6,
      "bbox": [
        334.9443664550781,
        117.86864471435547,
        339.10211181640625,
        126.14311218261719
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        128.28143310546875,
        339.9880676269531,
        136.55589294433594
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        138.6942138671875,
        339.9880676269531,
        146.9686737060547
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        149.1069793701172,
        339.9880676269531,
        157.38143920898438
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        159.51974487304688,
        339.9880676269531,
        167.79420471191406
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        169.9325408935547,
        339.9880676269531,
        178.20700073242188
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        334.0594482421875,
        180.34530639648438,
        339.9880676269531,
        188.61976623535156
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        348.59552001953125,
        86.63031768798828,
        358.76422119140625,
        94.90478515625
      ],
      "text": "0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        348.4168701171875,
        97.04310607910156,
        358.947998046875,
        105.31757354736328
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        348.5403747558594,
        107.45588684082031,
        358.8253173828125,
        115.73035430908203
      ],
      "text": "0.2"
    },
    {
      "page_no": 6,
      "bbox": [
        348.4168701171875,
        117.86864471435547,
        358.947998046875,
        126.14311218261719
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        128.28143310546875,
        356.6461486816406,
        136.55589294433594
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        138.6942138671875,
        356.6461486816406,
        146.9686737060547
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        149.1069793701172,
        356.6461486816406,
        157.38143920898438
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        159.51974487304688,
        356.6461486816406,
        167.79420471191406
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        169.9325408935547,
        356.6461486816406,
        178.20700073242188
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        350.717529296875,
        180.34530639648438,
        356.6461486816406,
        188.61976623535156
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        346.7115478515625,
        73.9649429321289,
        361.91845703125,
        80.17079162597656
      ],
      "text": "Scalar"
    },
    {
      "page_no": 6,
      "bbox": [
        94.28626251220703,
        107.54090881347656,
        125.36305236816406,
        113.74675750732422
      ],
      "text": "num_rows ="
    },
    {
      "page_no": 6,
      "bbox": [
        106.7260971069336,
        113.73820495605469,
        110.03929901123047,
        119.94405364990234
      ],
      "text": "4"
    },
    {
      "page_no": 6,
      "bbox": [
        471.86663818359375,
        86.8440170288086,
        474.6498718261719,
        95.11848449707031
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        471.17877197265625,
        97.30046844482422,
        475.3365173339844,
        105.57493591308594
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        469.78802490234375,
        107.75689697265625,
        476.7269592285156,
        116.03136444091797
      ],
      "text": "15"
    },
    {
      "page_no": 6,
      "bbox": [
        471.32232666015625,
        118.21332550048828,
        475.19287109375,
        126.48779296875
      ],
      "text": "7"
    },
    {
      "page_no": 6,
      "bbox": [
        471.1370849609375,
        128.66976928710938,
        475.37689208984375,
        136.94422912597656
      ],
      "text": "9"
    },
    {
      "page_no": 6,
      "bbox": [
        470.29400634765625,
        139.12619018554688,
        476.2226257324219,
        147.40065002441406
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        470.29400634765625,
        149.5826416015625,
        476.2226257324219,
        157.8571014404297
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        470.29400634765625,
        160.0390625,
        476.2226257324219,
        168.3135223388672
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        470.29400634765625,
        170.4954833984375,
        476.2226257324219,
        178.7699432373047
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        470.29400634765625,
        180.951904296875,
        476.2226257324219,
        189.2263641357422
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        486.85406494140625,
        86.8440170288086,
        497.02276611328125,
        95.11848449707031
      ],
      "text": "0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        486.6754150390625,
        97.30046844482422,
        497.20654296875,
        105.57493591308594
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        486.7989501953125,
        107.75689697265625,
        497.0838928222656,
        116.03136444091797
      ],
      "text": "0.2"
    },
    {
      "page_no": 6,
      "bbox": [
        486.6754150390625,
        118.21332550048828,
        497.20654296875,
        126.48779296875
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        486.8641357421875,
        128.66976928710938,
        497.0191650390625,
        136.94422912597656
      ],
      "text": "0.3"
    },
    {
      "page_no": 6,
      "bbox": [
        488.97607421875,
        139.12619018554688,
        494.9046936035156,
        147.40065002441406
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        488.97607421875,
        149.5826416015625,
        494.9046936035156,
        157.8571014404297
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        488.97607421875,
        160.0390625,
        494.9046936035156,
        168.3135223388672
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        488.97607421875,
        170.4954833984375,
        494.9046936035156,
        178.7699432373047
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        488.97607421875,
        180.951904296875,
        494.9046936035156,
        189.2263641357422
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        361.3927001953125,
        114.87225341796875,
        393.9122314453125,
        121.0781021118164
      ],
      "text": "num_rows ="
    },
    {
      "page_no": 6,
      "bbox": [
        375.372802734375,
        121.06954956054688,
        378.4911193847656,
        127.27539825439453
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        150.32301330566406,
        216.93492126464844,
        427.55731201171875,
        224.17507934570312
      ],
      "text": "To be deleted\nRemaining\nNew"
    },
    {
      "page_no": 6,
      "bbox": [
        197.52293395996094,
        74.00505828857422,
        231.52809143066406,
        80.21090698242188
      ],
      "text": "Pointer Scalar"
    },
    {
      "page_no": 6,
      "bbox": [
        217.597412109375,
        86.63031768798828,
        227.76611328125,
        94.90478515625
      ],
      "text": "0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        217.74099731445312,
        97.04310607910156,
        227.6224822998047,
        105.31757354736328
      ],
      "text": "0.7"
    },
    {
      "page_no": 6,
      "bbox": [
        217.54232788085938,
        107.45588684082031,
        227.8272705078125,
        115.73035430908203
      ],
      "text": "0.2"
    },
    {
      "page_no": 6,
      "bbox": [
        217.41876220703125,
        117.86864471435547,
        227.94989013671875,
        126.14311218261719
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        128.28143310546875,
        225.6480255126953,
        136.55589294433594
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        138.6942138671875,
        225.6480255126953,
        146.9686737060547
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        149.1069793701172,
        225.6480255126953,
        157.38143920898438
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        159.51974487304688,
        225.6480255126953,
        167.79420471191406
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        169.9325408935547,
        225.6480255126953,
        178.20700073242188
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        219.71939086914062,
        180.34530639648438,
        225.6480255126953,
        188.61976623535156
      ],
      "text": "-1"
    },
    {
      "page_no": 6,
      "bbox": [
        462.6005859375,
        74.00505828857422,
        499.22198486328125,
        80.21090698242188
      ],
      "text": "Pointer\nScalar"
    },
    {
      "page_no": 6,
      "bbox": [
        132.03065490722656,
        195.45196533203125,
        456.14422607421875,
        204.94122314453125
      ],
      "text": "1. Start deletion\n2. Deletion complete\n3. Insertion complete"
    },
    {
      "page_no": 6,
      "bbox": [
        229.3771514892578,
        94.9105453491211,
        261.89666748046875,
        101.11639404296875
      ],
      "text": "num_rows ="
    },
    {
      "page_no": 6,
      "bbox": [
        243.33236694335938,
        101.10784149169922,
        246.50196838378906,
        107.31369018554688
      ],
      "text": "3"
    },
    {
      "page_no": 6,
      "bbox": [
        70.86599731445312,
        239.81736755371094,
        524.4126586914062,
        261.67291259765625
      ],
      "text": "Figure 6: Memory management; First we replace elements to be deleted by last elements to maintain a consecutive\noccupation of memory. Then the new weights are stacked at the end. This reduces the unnecessary data movements."
    },
    {
      "page_no": 6,
      "bbox": [
        70.4729995727539,
        285.13336181640625,
        290.94110107421875,
        404.7897644042969
      ],
      "text": "of Reqi × 2dmodel, where Reqi denotes the maxi-\nmum number of neurons required for the specified\nwindow size in a subset of C4 validation set. By al-\nlocating enough memory for each layer in advance,\nwe minimize the need for reallocation. Finally, the\nlast_k_active component identifies the neurons\nfrom the original model that were most recently\nactivated using the last k tokens. The following\noperations can be done as depicted in Figure 6:"
    },
    {
      "page_no": 6,
      "bbox": [
        70.86598205566406,
        412.15283203125,
        293.9747314453125,
        775.4697265625
      ],
      "text": "1. Deleting Neurons: Neurons that are no longer\nrequired are identified efficiently in linear\ntime, utilizing the associated last_k_active\nvalue and the current prediction. The matrix,\npointer, and scalars of these redundant neu-\nrons are replaced with the most recent elements,\nand their count is subtracted from num_rows.\nFor O(c) neurons to be deleted, a memory\nrewrite of the order O(c × dmodel) is required.\n2. Bringing in New Neurons:\nThe required\nweights are retrieved from flash memory. The\ncorresponding pointers and scalars are read from\nDRAM, and these rows are then inserted into\nthe matrix, extending from num_row to num_row\n+ num_new. This approach eliminates the need\nfor reallocating memory in DRAM and copying\nexisting data, reducing inference latency.\n3. Inference\nProcess:\nFor\nthe\ninfer-\nence\noperation,\nthe\nfirst\nhalf\nof\nthe\nmatrix[:num_rows,:d_model] is used as the\n‘up project’, and the transposed second half,\nmatrix[:num_rows,d_model:].transpose(),\nserves as the ’down project’. This configuration\nis possible because the order of neurons in the\nintermediate output of the FFN does not alter\nthe final output, allowing for a streamlined\ninference process."
    },
    {
      "page_no": 6,
      "bbox": [
        306.1419982910156,
        285.57080078125,
        526.2684326171875,
        323.4775695800781
      ],
      "text": "These steps collectively ensure efficient memory\nmanagement during inference, optimizing the neu-\nral network’s performance and resource utilization."
    },
    {
      "page_no": 6,
      "bbox": [
        306.1419982910156,
        335.8981628417969,
        451.6009216308594,
        347.8533630371094
      ],
      "text": "4\nExperiments and Results"
    },
    {
      "page_no": 6,
      "bbox": [
        305.6289978027344,
        357.95233154296875,
        526.3173828125,
        463.7877502441406
      ],
      "text": "We start this section by briefly discussing our\nexperimental setup and implementation details.\nNext, we show that the techniques introduced\nin Section 3 can improve the inference latency\nsignificantly across different models and runtime\nplatforms. We postpone the some details to the\nappendix sections as follows: performance of our\ntrained low-rank predictor (Appendix B)."
    },
    {
      "page_no": 6,
      "bbox": [
        306.1419982910156,
        475.6140441894531,
        423.7201843261719,
        486.5231628417969
      ],
      "text": "4.1\nExperimental Setup"
    },
    {
      "page_no": 6,
      "bbox": [
        306.1419982910156,
        493.4725341796875,
        526.2252197265625,
        775.4959716796875
      ],
      "text": "Our work is mainly motivated by optimizing infer-\nence efficiency on personal devices. To this end,\nin our experiments, we process sequences individ-\nually, running only one sequence at a time. This\napproach allows us to allocate a specific portion of\nDRAM for the Key-Value (KV) cache while pri-\nmarily focusing on the model size. For the imple-\nmentation of our inference process, we utilize Hug-\ngingFace Transformers library (Wolf et al., 2019)\nand PyTorch (Paszke et al., 2019). This setup is\ntested under the condition that approximately half\nof the model size is available in DRAM. While with\na different level of sparsity or employing quantiza-\ntion, one can work with smaller available DRAM\ncapacity, these optimizations are orthogonal to our\nproposed method.\nModels. We mainly use OPT 6.7B (Zhang et al.,\n2022b) and the sparsified Falcon 7B (Mirzadeh\net al., 2023) model for our evaluations, but we ad-\nditionally report results on Phi-2 (Gunasekar et al.,\n2023), Persimmon 8B (Elsen et al., 2023) and a"
    },
    {
      "page_no": 7,
      "bbox": [
        70.55699920654297,
        71.60427856445312,
        524.4099731445312,
        117.50711059570312
      ],
      "text": "Table 2: The I/O latency of OPT 6.7B 16 bit on M1 Max when half the memory is available. By employing the\nactivation predictor and windowing, we can reduce the data transfer from flash memory to DRAM. While this\nreduces the throughput, the bundling technique can alleviate this by doubling the data transfer chunk size and hence\nthe throughput which leads to reducing the overall latency to half."
    },
    {
      "page_no": 7,
      "bbox": [
        132.68124389648438,
        132.57470703125,
        417.1249694824219,
        140.8487091064453
      ],
      "text": "Configuration\nPerformance Metrics"
    },
    {
      "page_no": 7,
      "bbox": [
        75.39399719238281,
        147.1121826171875,
        519.8831787109375,
        161.4675750732422
      ],
      "text": "Hybrid\nPredictor\nWindowing\nBundling\nDRAM (GB)\nFlash→DRAM (GB)\nThroughput (GB/s)\nI/O Latency (ms)"
    },
    {
      "page_no": 7,
      "bbox": [
        85.14460754394531,
        159.84996032714844,
        503.9114990234375,
        211.22879028320312
      ],
      "text": "✗\n✗\n✗\n✗\n0\n13.4 GB\n6.10 GB/s\n2196 ms\n✓\n✗\n✗\n✗\n6.7\n6.7 GB\n6.10 GB/s\n1090 ms\n✓\n✓\n✗\n✗\n4.8\n0.9 GB\n1.25 GB/s\n738 ms\n✓\n✓\n✓\n✗\n6.5\n0.2 GB\n1.25 GB/s\n164 ms\n✓\n✓\n✓\n✓\n6.5\n0.2 GB\n2.25 GB/s\n87 ms"
    },
    {
      "page_no": 7,
      "bbox": [
        70.86599731445312,
        237.1578369140625,
        289.5174865722656,
        288.63177490234375
      ],
      "text": "Llama 2 (Touvron et al., 2023b) which is sparsified\nusing FATReLU (Song et al., 2024). Note that\nthe techniques introduced in this work are mostly\nindependent of architecture."
    },
    {
      "page_no": 7,
      "bbox": [
        70.86599731445312,
        295.2947692871094,
        289.1376953125,
        347.03375244140625
      ],
      "text": "Data. We use a small subset of C4 validation\ndataset for our latency measurements. We take the\nfirst 128 tokens of each example as the prompt, and\ngenerate 256 new tokens."
    },
    {
      "page_no": 7,
      "bbox": [
        70.4729995727539,
        353.8642272949219,
        290.948974609375,
        500.28076171875
      ],
      "text": "Hardware Configuration. Our models are eval-\nuated across three hardware setups. The first in-\ncludes an Apple M1 Max with a 1TB SSD. The\nsecond features an Apple M2 Ultra with a 2TB\nSSD. On MacBooks we run the model on the CPU\nwith float32 or GPU with Metal and float16. The\nthird setup uses a Linux machine with a 24GB\nNVIDIA RTX 4090, where GPU computations uti-\nlize bfloat16 models. Across all setups, we assume\nnearly half of the total memory (DRAM and GPU)\nis allocated for model computations."
    },
    {
      "page_no": 7,
      "bbox": [
        70.35299682617188,
        507.0213623046875,
        291.0415344238281,
        775.4959716796875
      ],
      "text": "Baselines. We compare our models with a naive\nbaseline of loading the model on demand when\ndoing the forward pass. We additionally compare\nwith our hybrid loading approach as a secondary\nbaseline when half of the model is persisted in\nmemory and the other half is loaded on demand at\ngeneration of every token without use of sparsity.\nWe used best theoretical possible numbers for IO\nlatency for each of the methods to make a fair\ncomparison, the real number might be higher. For\nmethods not employing sparsity or weight sharing,\nat least half of the model must be transferred\nfrom flash memory during the forward pass. This\nnecessity arises because, initially, only half of the\nmodel is available in DRAM, but as the forward\npass progresses, the entire model capacity is\nutilized. Consequently, any data not present at the\nstart must be transferred at least once. Thus, the\nmost efficient theoretical baseline involves loading\nhalf of the model size from the flash memory"
    },
    {
      "page_no": 7,
      "bbox": [
        306.1419982910156,
        236.9923553466797,
        526.2218627929688,
        438.03375244140625
      ],
      "text": "into DRAM. This optimal I/O scenario serves\nas our primary baseline. Given the nature of our\nsetup (i.e., the limited available DRAM or GPU\nmemory), we are not aware of any other method\nthat can surpass this theoretical I/O efficiency.\nImplementation Details. To optimize data load-\ning from flash memory, our system employs reads\nparallelized over 32 threads. This multithreaded\napproach is intended to both better amortize\nlatency to the first byte by not waiting for each read\nsequentially, and maximize read throughput by\nreading multiple streams at once (Figure 2b). To\nbetter assess the actual throughput, we conducted\nbenchmarks without the aid of operating system\ncaching leading to a more accurate measurement."
    },
    {
      "page_no": 7,
      "bbox": [
        306.1419982910156,
        447.8250427246094,
        443.8475646972656,
        458.7341613769531
      ],
      "text": "4.2\nFaster Load From Flash"
    },
    {
      "page_no": 7,
      "bbox": [
        305.7489929199219,
        466.0435485839844,
        526.22509765625,
        775.4959716796875
      ],
      "text": "Our first result in Table 2 demonstrates the effec-\ntiveness of techniques we introduced in Section 3,\nwhere the I/O latency depends on how much data\nis being transferred from flash to DRAM, and the\nchunk size which determines the throughput. For\ninstance, by using a low-rank predictor, we reduce\nthe data transfer significantly, and the amount of\nthis traffic can be further reduced using our pro-\nposed windowing technique. Compared to a long,\ncontiguous read, scattered reads will necessarily re-\nsult in lower throughput (e.g. 1.25 GiB/s sparse vs\n6.1 GiB/s dense), but this is partially mitigated\nby bundling up-projection and down-projection\nweights. The overall effect of sparse reads is still\nstrongly favorable, because only a small subset of\nthe overall weights is loaded incrementally in each\niteration, and the load of just the required subset of\nweights takes less time and less DRAM.\nAdditionally, we examine end-to-end latencies\nunder various setups in Table 3. We allocate ap-\nproximately 50 % of the model size for OPT, Fal-\ncon, and Persimmon and Llama 2. For the sig-\nnificantly smaller Phi-2 model, we observed less"
    },
    {
      "page_no": 8,
      "bbox": [
        70.55699920654297,
        71.7097396850586,
        290.78753662109375,
        117.50711059570312
      ],
      "text": "Table 3: The end-to-end inference latency across differ-\nent setups. Our efficient implementation (referred as All)\nthat employs the predictor, windowing, and bundling\ncan lead to significant latency reduction."
    },
    {
      "page_no": 8,
      "bbox": [
        211.10707092285156,
        132.8831787109375,
        280.2405090332031,
        139.88900756835938
      ],
      "text": "Inference Latency (ms)"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        145.24490356445312,
        295.36749267578125,
        152.250732421875
      ],
      "text": "Model\nMethod\nBackend\nI/O\nMem\nCompute\nTotal"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        157.67123413085938,
        294.716064453125,
        235.05447387695312
      ],
      "text": "OPT 6.7B\nNaive\nCPU\n2196\n0\n986\n3182\nOPT 6.7B\nAll\nCPU\n105\n58\n506\n669\nOPT 6.7B\nNaive\nMetal M1\n2196\n0\n193\n2389\nOPT 6.7B\nAll\nMetal M1\n92\n35\n438\n565\nOPT 6.7B\nNaive\nMetal M2\n2145\n0\n125\n2270\nOPT 6.7B\nAll\nMetal M2\n26\n8\n271\n305\nOPT 6.7B\nNaive\nGPU\n2196\n0\n22\n2218\nOPT 6.7B\nAll\nGPU\n30\n34\n20\n84\nOPT 6.7B\nSpeculative\nGPU\n38.5\n9.5\n12\n60"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        240.4110107421875,
        294.716064453125,
        264.8191833496094
      ],
      "text": "Falcon 7B\nNaive\nCPU\n2295\n0\n800\n3095\nFalcon 7B\nHybrid\nCPU\n1147\n0\n800\n1947\nFalcon 7B\nAll\nCPU\n161\n92\n453\n706"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        270.1750183105469,
        294.71649169921875,
        294.5832214355469
      ],
      "text": "Persimmon 8B\nNaive\nCPU\n2622\n0\n1184\n3806\nPersimmon 8B\nHybrid\nCPU\n1311\n0\n1184\n2495\nPersimmon 8B\nAll\nCPU\n283\n98\n660\n1041"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        299.939697265625,
        294.7160339355469,
        324.347900390625
      ],
      "text": "Phi-2 2.7B\nNaive\nCPU\n885\n0\n402\n1287\nPhi-2 2.7B\nHybrid\nCPU\n309\n0\n402\n711\nPhi-2 2.7B\nAll\nCPU\n211\n76\n259\n546"
    },
    {
      "page_no": 8,
      "bbox": [
        74.70507049560547,
        329.7037658691406,
        294.716064453125,
        354.1119384765625
      ],
      "text": "Llama 2 7B\nNaive\nCPU\n2166\n0\n929\n3095\nLlama 2 7B\nHybrid\nCPU\n974\n0\n929\n1903\nLlama 2 7B\nAll\nCPU\n279\n152\n563\n994"
    },
    {
      "page_no": 8,
      "bbox": [
        70.35299682617188,
        379.4088439941406,
        291.04547119140625,
        457.9807434082031
      ],
      "text": "sparsity rates, prompting us to set this limit at 65%.\nWe observe a significant improvement in loading\nefficiency over both naive and hybrid approaches\nacross all models. Moreover we showed the GPU\nbackend outcomes further improve when combined\nwith speculative decoding."
    },
    {
      "page_no": 8,
      "bbox": [
        70.86599731445312,
        472.3050537109375,
        240.6334228515625,
        483.21417236328125
      ],
      "text": "4.3\nThe Memory-Latency Tradeoff"
    },
    {
      "page_no": 8,
      "bbox": [
        70.86599731445312,
        491.5252990722656,
        291.04156494140625,
        638.8201293945312
      ],
      "text": "So far, we have mainly worked under the assump-\ntion that the available DRAM is roughly half of\nour model size. However, we note that this is not a\nhard constraint and we can relax this constraint.\nTo this end, we study the impact of window size\non memory usage, and consequently on latency.\nBy increasing the window size, we increase the\npercentage of model parameters that we keep in\nDRAM. As a result, we need to bring fewer param-\neters, and hence the latency can be reduced at the\ncost of using higher DRAM as shown in Figure 7."
    },
    {
      "page_no": 8,
      "bbox": [
        70.86599731445312,
        653.7241821289062,
        176.824951171875,
        665.6793823242188
      ],
      "text": "5\nAblation analysis"
    },
    {
      "page_no": 8,
      "bbox": [
        70.86599731445312,
        677.549072265625,
        253.58251953125,
        688.4581298828125
      ],
      "text": "5.1\nThe Impact of Longer Generation"
    },
    {
      "page_no": 8,
      "bbox": [
        70.86599731445312,
        696.7323608398438,
        290.9413757324219,
        775.4959716796875
      ],
      "text": "In our previous results, we have used short to\nmedium-length (256 tokens) generations for our\nbenchmarks. It is possible that for longer genera-\ntion of tokens, the ssd enable thermal throttling and\nlower the performance. However, Figure 8 shows\nthat this is not the case, even when we generate"
    },
    {
      "page_no": 8,
      "bbox": [
        344.0912780761719,
        194.46438598632812,
        504.1313171386719,
        210.3673553466797
      ],
      "text": "35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n% of Model in DRAM"
    },
    {
      "page_no": 8,
      "bbox": [
        335.7066345214844,
        186.97470092773438,
        339.15570068359375,
        194.50537109375
      ],
      "text": "0"
    },
    {
      "page_no": 8,
      "bbox": [
        332.2529296875,
        162.78363037109375,
        339.15106201171875,
        170.31430053710938
      ],
      "text": "10"
    },
    {
      "page_no": 8,
      "bbox": [
        332.2529296875,
        138.59255981445312,
        339.15106201171875,
        146.12322998046875
      ],
      "text": "20"
    },
    {
      "page_no": 8,
      "bbox": [
        332.2529296875,
        114.40150451660156,
        339.15106201171875,
        121.93218231201172
      ],
      "text": "30"
    },
    {
      "page_no": 8,
      "bbox": [
        332.2529296875,
        90.21044158935547,
        339.15106201171875,
        97.74111938476562
      ],
      "text": "40"
    },
    {
      "page_no": 8,
      "bbox": [
        322.9885559082031,
        113.38854217529297,
        330.5794677734375,
        152.49884033203125
      ],
      "text": "Latency (ms)"
    },
    {
      "page_no": 8,
      "bbox": [
        428.5400390625,
        147.0435028076172,
        455.1760559082031,
        154.63442993164062
      ],
      "text": "Compute"
    },
    {
      "page_no": 8,
      "bbox": [
        428.5400390625,
        163.9772491455078,
        493.0202941894531,
        171.56817626953125
      ],
      "text": "Memory Management"
    },
    {
      "page_no": 8,
      "bbox": [
        428.5400390625,
        180.4271697998047,
        478.46697998046875,
        188.01809692382812
      ],
      "text": "Load From Flash"
    },
    {
      "page_no": 8,
      "bbox": [
        306.1419982910156,
        223.67330932617188,
        525.0792846679688,
        257.6210632324219
      ],
      "text": "Figure 7: By bringing more of our model (OPT-6.7B)\nparameters into DRAM, the latency can be reduced on\nthe GPU machine."
    },
    {
      "page_no": 8,
      "bbox": [
        305.3240051269531,
        279.3013610839844,
        526.22265625,
        481.8017578125
      ],
      "text": "1000 tokens for OPT 6.7B model on GPU. More-\nover, we show that the average flash latency doesn’t\nincrease as we go further in generation. In contrast,\nthe flash latency for the the first few tokens is higher\nsince the allocated memory in DRAM is empty and\nneeds to be filled in with neurons and for first few\ntokens we need more data transfer.\nAlso it is possible to argue that the non-greedy\nsampling methods such as the Nucleus sam-\npling (Holtzman et al., 2020) method can result\nin more diverse activation, and hence less favor-\nable towards our method. We found out this is not\nthe case either for long token generations. Nucleus\nsampling doesn’t lead to lower performance in long\ngeneration in neither cpu or gpu."
    },
    {
      "page_no": 8,
      "bbox": [
        306.1419982910156,
        499.48504638671875,
        430.1674499511719,
        510.3941650390625
      ],
      "text": "5.2\nSpeculative Decoding"
    },
    {
      "page_no": 8,
      "bbox": [
        305.7489929199219,
        520.5933227539062,
        526.3167724609375,
        775.4697265625
      ],
      "text": "To further showcase the strength of our method\nand adaptability to other decoding strategies we\nhave applied speculative decoding on the OPT 6.7B\nmodel. The challenge for doing speculative decod-\ning is the limited memory available within DRAM.\nGiven λ tokens from the draft model, the big model\nverifies them and will keep a window of size k for\neach layer. The model should decide neurons of\nwhich tokens to keep in memory before verifica-\ntions are done. If the model keeps the last k tokens\nout of λ + 1 tokens in memory and most of them\nget rejected, there will be very few neuron reuse\nfor the next forward pass. We conjecture that if\nthe ratio of the acceptance is α keeping the last k\ntokens ending with α(λ + 1)th token is optimal in\nDRAM. We used λ = 4 and were able to improve\nthe speed of decoding by 1.4x as shown in table\n5, this is close to the original 1.58x speedup of\nspeculative decoding."
    },
    {
      "page_no": 9,
      "bbox": [
        114.52621459960938,
        196.04348754882812,
        270.6469421386719,
        210.3673553466797
      ],
      "text": "0\n200\n400\n600\n800\n1000\nGeneration Length"
    },
    {
      "page_no": 9,
      "bbox": [
        102.7120361328125,
        185.474609375,
        105.57620239257812,
        191.33766174316406
      ],
      "text": "0"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        166.82275390625,
        105.56942749023438,
        172.68580627441406
      ],
      "text": "100"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        148.1708984375,
        105.56942749023438,
        154.03395080566406
      ],
      "text": "200"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        129.51905822753906,
        105.56942749023438,
        135.38211059570312
      ],
      "text": "300"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        110.86719512939453,
        105.56942749023438,
        116.7302474975586
      ],
      "text": "400"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        92.21533966064453,
        105.56942749023438,
        98.0783920288086
      ],
      "text": "500"
    },
    {
      "page_no": 9,
      "bbox": [
        96.97692108154297,
        73.5634765625,
        105.56942749023438,
        79.42652893066406
      ],
      "text": "600"
    },
    {
      "page_no": 9,
      "bbox": [
        87.58451843261719,
        87.5071029663086,
        100.6502456665039,
        181.9304962158203
      ],
      "text": "DRAM →Flash\nLatency (ms)"
    },
    {
      "page_no": 9,
      "bbox": [
        223.2150421142578,
        154.781982421875,
        264.47906494140625,
        161.53616333007812
      ],
      "text": "CPU (Nucleus)"
    },
    {
      "page_no": 9,
      "bbox": [
        223.2150421142578,
        173.433837890625,
        264.6597900390625,
        180.18801879882812
      ],
      "text": "GPU (Nucleus)"
    },
    {
      "page_no": 9,
      "bbox": [
        223.2150421142578,
        184.62493896484375,
        262.236572265625,
        191.37911987304688
      ],
      "text": "GPU (Greedy)"
    },
    {
      "page_no": 9,
      "bbox": [
        70.86599731445312,
        223.67330932617188,
        289.1324768066406,
        245.66610717773438
      ],
      "text": "Figure 8: Weight loading latency of OPT 6.7B with\nincreasing generation length."
    },
    {
      "page_no": 9,
      "bbox": [
        70.86599731445312,
        268.9380798339844,
        239.92434692382812,
        279.8471984863281
      ],
      "text": "5.3\nA Note on Power Consumption"
    },
    {
      "page_no": 9,
      "bbox": [
        70.4729995727539,
        286.59783935546875,
        290.9492492675781,
        473.562744140625
      ],
      "text": "In evaluating the efficiency of our method, we com-\npared the power consumption of our sparse model\napproach with that of generating tokens using a\ndense model of similar size. While the power us-\nage (energy per unit of time) of the sparse model\nwas lower than that of the dense model, the ex-\ntended duration required for token generation re-\nsulted in the sparse model having a higher total\nenergy consumption. This is going to be reflected\nin the greater area under the curve when plotting\npower over time for the sparse model compared\nto the dense model. A systematic and quantitative\nevaluation of the exact power usage pattern is left\nas a future work."
    },
    {
      "page_no": 9,
      "bbox": [
        70.86599731445312,
        485.4141540527344,
        164.60671997070312,
        497.3693542480469
      ],
      "text": "6\nRelated Works"
    },
    {
      "page_no": 9,
      "bbox": [
        70.4729995727539,
        507.0443420410156,
        290.9489440917969,
        775.4959716796875
      ],
      "text": "As LLMs grow in size, reducing their computa-\ntional and memory requirements for inference has\nbecome an active area of research. Approaches\nbroadly fall into two categories: model compres-\nsion techniques such as pruning and quantization\n(Han et al., 2016b; Sun et al., 2023; Jaiswal et al.,\n2023; Xia et al., 2023; Zhang et al., 2022a; Xu et al.,\n2023; Shao et al., 2023; Lin et al., 2023; Hoang\net al., 2023; Zhao et al., 2023; Ahmadian et al.,\n2023; Li et al., 2023), and selective execution such\nas sparse activation (Liu et al., 2023b; Mirzadeh\net al., 2023), or conditional computation (Graves,\n2016; Baykal et al., 2023). Our work is orthogonal\nto these directions, focusing mainly on minimizing\ndata transfer from flash memory during inference.\nPerhaps most related to our work is the literature\non selective weight loading. Dejavu (Liu et al.,\n2023b) exploits activation sparsity to load a subset\nof weights for each layer. However, it still requires\nloading from GPU memory.\nFlexGen (Sheng"
    },
    {
      "page_no": 9,
      "bbox": [
        305.7489929199219,
        73.46934509277344,
        526.3173217773438,
        301.24774169921875
      ],
      "text": "et al., 2023) offloads the weights and KV-cache\nfrom GPU memory to DRAM and DRAM to flash\nmemory. In contrast, we consider only the cases\nwhere the full model can’t reside in the whole\nDRAM and GPU memory on the edge devices.\nNotably, FlexGen is still theoretically bound by\nthe slow throughput of flash to DRAM in such\nscenarios. An expanded discusion of related works\nis deferred to Appendix E.\nOverall, the primary assumption in the literature\nis that the model can fully reside in the GPU mem-\nory or system DRAM. However, considering the\nlimited resources available on personal devices, we\ndo not share this assumption in this work. Instead,\nwe concentrate on exploring how to store and load\nparameters on flash memory more efficiently, aim-\ning to enhance inference efficiency."
    },
    {
      "page_no": 9,
      "bbox": [
        306.1419982910156,
        313.2021484375,
        377.8851623535156,
        325.1573486328125
      ],
      "text": "7\nDiscussion"
    },
    {
      "page_no": 9,
      "bbox": [
        305.7489929199219,
        334.9599609375,
        526.3174438476562,
        752.4027709960938
      ],
      "text": "In this study, we have tackled the significant chal-\nlenge of running large language models (LLMs)\non devices with constrained memory capacities.\nOur approach, deeply rooted in the understand-\ning of flash memory and DRAM characteristics,\nrepresents a novel convergence of hardware-aware\nstrategies and machine learning. By developing an\ninference cost model that aligns with these hard-\nware constraints, we have introduced two new tech-\nniques: ‘windowing’ and ‘row-column bundling’.\nThe practical outcomes of our research are\nnoteworthy. We have demonstrated the ability to\nrun LLMs up to twice the size of available DRAM.\nFor example, on OPT model, we achieve an\nacceleration in inference speed of 4-5x compared\nto traditional loading methods in CPU, and 20-25x\nin GPU. This is particularly crucial for deploying\nLLMs in resource-limited environments, thereby\nexpanding their applicability and accessibility.\nWhile in this work we have studied the previ-\nously unexplored problem of serving LLMs from\nflash, we note that this work is only a first step in\nthis direction, and has several limitations that we\ndiscuss in the next section, and we believe there\nare several interesting problems left to be explored\nin future works. For instance, from the algorithmic\nperspective, more optimized techniques of weight\nbundling and data structures can be crafted, while\nfrom the engineering perspective, the character-\nistics of specific hardware platforms can inform\nworks on building more efficient inference stacks."
    },
    {
      "page_no": 10,
      "bbox": [
        70.86599731445312,
        72.64916229248047,
        147.91726684570312,
        84.60436248779297
      ],
      "text": "8\nLimitations"
    },
    {
      "page_no": 10,
      "bbox": [
        70.4729995727539,
        94.85032653808594,
        290.9499816894531,
        621.2767333984375
      ],
      "text": "Our study represents an initial endeavor in the\npursuit of democratizing Large Language Model\n(LLM) inference, making it accessible to a wider\narray of individuals and devices. We recognize that\nthis early effort has its limitations, which, in turn,\nopen up compelling avenues for future research. A\ncritical aspect for future exploration is the system-\natic analysis of power consumption and thermal\nlimitations inherent in the methods we propose,\nparticularly for on-device deployment.\nCurrently, our study is limited to single-batch\ninference. We provide some preliminary results\non combining our proposed idea with specula-\ntive decoding, however, expanding this to include\nmore complex scenarios like prompt processing\nand multi-batch inference are valuable areas for\nfurther investigation.\nIn our initial proof of concept, we operated under\nthe assumption of memory availability being half\nthe size of the model. Exploring the dynamics of\nworking with varying memory sizes—both larger\nand smaller—introduces a fascinating balance be-\ntween latency and accuracy, and is a compelling\narea for future exploration.\nIn conclusion, our methodology is constructed\non the foundation of sparsified networks. Nonethe-\nless, the underlying concept holds potential for\nbroader applications. It can be adapted to selec-\ntively load weights in non-sparse networks or to\ndynamically retrieve model weights from flash stor-\nage. This adaptation would be contingent on the\nspecific requirements of the input prompt or the\ncontextual parameters provided. Such an approach\nsuggests a versatile strategy for managing model\nweights, and optimizing performance based on the\nnature of the input, thereby enhancing the effi-\nciency, usefulness, and applicability of the pro-\nposed scheme in various scenarios dealing with\nLarge Language Models (LLMs)."
    },
    {
      "page_no": 10,
      "bbox": [
        70.86599731445312,
        633.8842163085938,
        169.6996307373047,
        645.8394165039062
      ],
      "text": "Acknowledgements"
    },
    {
      "page_no": 10,
      "bbox": [
        70.35299682617188,
        656.0853881835938,
        290.94927978515625,
        775.4697265625
      ],
      "text": "We would like to thank Itay Sagron, Lailin Chen,\nChenfan (Frank) Sun, Hanieh Hashemi, Mahyar\nNajibi, Qichen Fu, Moin Nabi, Peter Zatloukal, Ar-\nsalan Farooq, Sachin Mehta, Mohammad Samragh,\nMatt Johnson, Etai Zaltsman, Lin Chang, Dominic\nGiampaolo, Tal Uliel, Hadi Pouransari, Fartash\nFaghri, Oncel Tuzel, Samy Bengio, Ruoming Pang,\nChong Wang, Ronan Collobert, David Grangier,\nand Aftab Munshi for the valuable discussions."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        72.64922332763672,
        361.68585205078125,
        84.60442352294922
      ],
      "text": "References"
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        91.6451416015625,
        526.0648193359375,
        134.44308471679688
      ],
      "text": "Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat\nVenkitesh, Stephen Gou, Phil Blunsom, A. Ustun,\nand Sara Hooker. 2023. Intriguing properties of quan-\ntization at scale. ArXiv, abs/2305.19268."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        143.609619140625,
        526.1515502929688,
        219.34506225585938
      ],
      "text": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMaitha Alhammadi, Mazzotta Daniele, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. The falcon series of language models: To-\nwards open frontier models."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        228.45529174804688,
        526.0650024414062,
        315.2051086425781
      ],
      "text": "Reza Yazdani Aminabadi, Samyam Rajbhandari, Am-\nmar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,\nOlatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff\nRasley, et al. 2022. Deepspeed-inference: enabling\nefficient inference of transformer models at unprece-\ndented scale. In SC22: International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, pages 1–15. IEEE."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        324.3163146972656,
        526.0662841796875,
        378.1890869140625
      ],
      "text": "Sangmin Bae, Jongwoo Ko, Hwanjun Song, and\nSe-Young Yun. 2023.\nFast and robust early-\nexiting framework for autoregressive language mod-\nels with synchronized parallel decoding.\nArXiv,\nabs/2310.05424."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        387.30029296875,
        526.0601806640625,
        430.2140808105469
      ],
      "text": "Cenk Baykal, Dylan Cutler, Nishanth Dikkala, Nikhil\nGhosh, Rina Panigrahy, and Xin Wang. 2023. Al-\nternating updates for efficient transformers. ArXiv,\nabs/2301.13310."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        439.3253173828125,
        524.4105224609375,
        504.1570739746094
      ],
      "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        513.268310546875,
        526.0650024414062,
        556.1820678710938
      ],
      "text": "Yew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-\njanya Poria. 2023. Instructeval: Towards holistic\nevaluation of instruction-tuned large language mod-\nels."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        565.3082885742188,
        525.6578369140625,
        630.1250610351562
      ],
      "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        639.3864135742188,
        525.6514282226562,
        693.1090698242188
      ],
      "text": "Han Dai, Yi Zhang, Ziyu Gong, Nanqing Yang, Wei Dai,\nEric Song, and Qiankun Xie. 2021. Spatten: Efficient\nsparse attention architecture with cascade token and\nhead pruning. In Advances in Neural Information\nProcessing Systems, volume 34."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        702.1202392578125,
        526.0628662109375,
        745.134033203125
      ],
      "text": "Erich Elsen, Augustus Odena, Maxwell Nye, Sa˘g-\nnak Ta¸sırlar, Tri Dao, Curtis Hawthorne, Deepak\nMoparthi, and Arushi Somani. 2023.\nReleasing\nPersimmon-8B."
    },
    {
      "page_no": 10,
      "bbox": [
        306.1419982910156,
        754.2442626953125,
        524.4085083007812,
        775.2410278320312
      ],
      "text": "Trevor Gale, Matei Zaharia, Cliff Young, and Erich\nElsen. 2020. Sparse gpu kernels for deep learning."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        74.19430541992188,
        290.7904052734375,
        149.98611450195312
      ],
      "text": "Mingyu Gao, Jie Yu, Wentai Li, Michael C Dai,\nNam Sung Kim, and Krste Asanovic. 2022. com-\nputedram: In-memory compute using off-the-shelf\ndram. In Proceedings of the 27th ACM International\nConference on Architectural Support for Program-\nming Languages and Operating Systems, pages 1065–\n1079."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        159.92733764648438,
        289.1324157714844,
        191.88308715820312
      ],
      "text": "Google Gemini Team. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        201.8402557373047,
        290.786865234375,
        233.78109741210938
      ],
      "text": "Alex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. In International Conference\non Machine Learning, pages 3500–3509. PMLR."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        243.72232055664062,
        290.3788146972656,
        330.4731140136719
      ],
      "text": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nHarkirat Singh Behl, Xin Wang, Sébastien Bubeck,\nRonen Eldan, Adam Tauman Kalai, Yin Tat Lee, and\nYuanzhi Li. 2023. Textbooks are all you need. CoRR,\nabs/2306.11644."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        340.414306640625,
        290.8766174316406,
        405.2471008300781
      ],
      "text": "Jongmin Ham, Jinha Kim, Jinwoong Choi, Cheolwoo\nCho, Seulki Hong, Kyeongsu Han, and Taejoo Chung.\n2016. Graphssd: a high performance flash-based stor-\nage system for large-scale graph processing. In 2016\nUSENIX Annual Technical Conference (USENIXATC\n16), pages 243–256."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        415.33941650390625,
        290.8767395019531,
        458.10308837890625
      ],
      "text": "Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pe-\ndram, Mark A Horowitz, and William J Dally. 2016a.\nEie: efficient inference engine on compressed deep\nneural network. arXiv preprint arXiv:1602.01528."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        468.0453186035156,
        290.8802795410156,
        521.9180297851562
      ],
      "text": "Song Han, Huizi Mao, and William J Dally. 2016b.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. In International Conference on Learn-\ning Representations (ICLR)."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        531.8602905273438,
        290.38238525390625,
        563.8161010742188
      ],
      "text": "Awni Hannun, Jagrit Digani, Angelos Katharopoulos,\nand Ronan Collobert. 2023.\nMLX: Efficient and\nflexible machine learning on apple silicon."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        573.75830078125,
        290.3823547363281,
        605.7130737304688
      ],
      "text": "Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,\nand Di He. 2023. Rest: Retrieval-based speculative\ndecoding. ArXiv, abs/2311.08252."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        615.6552734375,
        290.8818054199219,
        680.487060546875
      ],
      "text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        690.5613403320312,
        290.788818359375,
        733.3440551757812
      ],
      "text": "Duc Nien Hoang, Minsik Cho, Thomas Merth, Moham-\nmad Rastegari, and Zhangyang Wang. 2023. (dy-\nnamic) prompting might be all you need to repair\ncompressed llms. ArXiv, abs/2310.00867."
    },
    {
      "page_no": 11,
      "bbox": [
        70.86599731445312,
        743.2852783203125,
        289.13250732421875,
        775.2650146484375
      ],
      "text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on"
    },
    {
      "page_no": 11,
      "bbox": [
        316.74200439453125,
        74.11883544921875,
        526.1574096679688,
        95.19107055664062
      ],
      "text": "Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        106.03530883789062,
        526.151611328125,
        148.95010375976562
      ],
      "text": "Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang,\nZhangyang Wang, and Yinfei Yang. 2023. Compress-\ning llms: The truth is rarely pure and never simple.\nArXiv, abs/2310.01382."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        159.79330444335938,
        526.0650024414062,
        246.54409790039062
      ],
      "text": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nthée Lacroix, and William El Sayed. 2023. Mistral\n7b. CoRR, abs/2310.06825."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        257.3872985839844,
        526.1563110351562,
        289.3431091308594
      ],
      "text": "Yaniv Leviathan, Matan Kalman, and Yossi Matias.\n2022. Fast inference from transformers via spec-\nulative decoding."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        300.1872863769531,
        526.0635375976562,
        343.1020812988281
      ],
      "text": "Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang\nChu. 2023. Norm tweaking: High-performance low-\nbit quantization of large language models. ArXiv,\nabs/2309.02784."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        353.9453125,
        526.0670776367188,
        396.8600769042969
      ],
      "text": "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, and Song Han. 2023. Awq: Activation-\naware weight quantization for llm compression and\nacceleration. ArXiv, abs/2306.00978."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        407.7043151855469,
        526.1515502929688,
        461.57708740234375
      ],
      "text": "Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\nRaghuraman Krishnamoorthi, and Vikas Chandra.\n2023a. Llm-qat: Data-free quantization aware train-\ning for large language models. CoRR."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        472.5533447265625,
        526.0651245117188,
        537.2540893554688
      ],
      "text": "Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher Re, et al. 2023b. Deja\nvu: Contextual sparsity for efficient llms at infer-\nence time. In International Conference on Machine\nLearning, pages 22137–22176. PMLR."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        548.0972900390625,
        526.0681762695312,
        612.9300537109375
      ],
      "text": "Moinuddin K Meswani, Sergey Blagodurov, David\nRoberts, John Slice, Mike Ignatowski, and Gabriel\nLoh. 2015. Neural cache: Bit-serial in-cache acceler-\nation of deep neural networks. In 2015 48th Annual\nIEEE/ACM International Symposium on Microarchi-\ntecture (MICRO), pages 383–394. IEEE."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        623.8759765625,
        526.151611328125,
        677.6470336914062
      ],
      "text": "Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo\nC Del Mundo, Oncel Tuzel, Golnoosh Samei, Mo-\nhammad Rastegari, and Mehrdad Farajtabar. 2023.\nRelu strikes back: Exploiting activation sparsity in\nlarge language models."
    },
    {
      "page_no": 11,
      "bbox": [
        306.1419982910156,
        688.4912719726562,
        526.1539306640625,
        775.2410278320312
      ],
      "text": "Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,\nAntonio Puglielli, Rangharajan Venkatesan, Brucek\nKhailany, Joel Emer, Stephen W Keckler, and\nWilliam J Dally. 2017. Timeloop: A systematic ap-\nproach to dnn accelerator evaluation. In 2017 IEEE\nInternational Symposium on Performance Analysis\nof Systems and Software (ISPASS), pages 241–251.\nIEEE."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        74.19430541992188,
        290.8812255859375,
        215.73910522460938
      ],
      "text": "Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        227.70431518554688,
        290.51434326171875,
        292.5370788574219
      ],
      "text": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,\nShaden Smith, and Yuxiong He. 2021. Zero-infinity:\nBreaking the gpu memory wall for extreme scale\ndeep learning. In SC21: International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, pages 1–14."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        304.5022888183594,
        290.8731384277344,
        380.2940979003906
      ],
      "text": "Minsoo Rhu, Natalia Gimelshein, Jason Clemons,\nArslan Zulfiqar, and Stephen W Keckler. 2013.\nvdnn: Virtualized deep neural networks for scalable,\nmemory-efficient neural network design. In 2016\n49th Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO), page Article 13. IEEE\nComputer Society."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        392.29681396484375,
        290.5215148925781,
        446.132080078125
      ],
      "text": "Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng\nXu, Lirui Zhao, Zhiqiang Li, Kaipeng Zhang, Peng\nGao, Yu Jiao Qiao, and Ping Luo. 2023. Omniquant:\nOmnidirectionally calibrated quantization for large\nlanguage models. ArXiv, abs/2308.13137."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        458.0982971191406,
        290.8766174316406,
        533.8890991210938
      ],
      "text": "Yifan Shao, Mengjiao Li, Wenhao Cai, Qi Wang,\nDhananjay Narayanan,\nand Parthasarathy Ran-\nganathan. 2022. Hotpot: Warmed-up gigascale infer-\nence with tightly-coupled compute and reuse in flash.\nIn Proceedings of the 55th Annual IEEE/ACM In-\nternational Symposium on Microarchitecture, pages\n335–349."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        545.8804931640625,
        290.7889709472656,
        643.5640258789062
      ],
      "text": "Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christo-\npher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen:\nHigh-throughput generative inference of large lan-\nguage models with a single GPU. In International\nConference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pages\n31094–31116. PMLR."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        655.634765625,
        290.8755187988281,
        709.4020385742188
      ],
      "text": "Chenyang Song, Xu Han, Zhengyan Zhang, Shengding\nHu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu,\nGuangli Li, Tao Yang, and Maosong Sun. 2024.\nProsparse: Introducing and enhancing intrinsic acti-\nvation sparsity within large language models."
    },
    {
      "page_no": 12,
      "bbox": [
        70.86599731445312,
        721.4359130859375,
        290.7910461425781,
        775.2410278320312
      ],
      "text": "Vedant Subramani, Marios Savvides, Li Ping, and Sha-\nran Narang. 2022. Adapt: Parameter adaptive token-\nwise inference for vision transformers. In Proceed-\nings of the 55th Annual IEEE/ACM International\nSymposium on Microarchitecture."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        74.34542083740234,
        526.1492919921875,
        106.15011596679688
      ],
      "text": "Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.\n2023. A simple and effective pruning approach for\nlarge language models. ArXiv, abs/2306.11695."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        118.18060302734375,
        525.6544799804688,
        193.88107299804688
      ],
      "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        205.82131958007812,
        526.06494140625,
        270.6531066894531
      ],
      "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        282.5932922363281,
        526.0670776367188,
        347.4250793457031
      ],
      "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. CoRR,\nabs/1910.03771."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        359.42913818359375,
        525.6569213867188,
        424.1980895996094
      ],
      "text": "Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\nZhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and\nShuaiwen Leon Song. 2023. Flash-llm: Enabling\nlow-cost and highly-efficient large generative model\ninference with unstructured sparsity. Proc. VLDB\nEndow., 17:211–224."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        436.1485290527344,
        526.0675048828125,
        490.0110778808594
      ],
      "text": "Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue\nWang, Kaixiong Zhou, Xia Hu, and Anshumali Shri-\nvastava. 2023. Compress, then prompt: Improving\naccuracy-efficiency trade-off of llm inference with\ntransferable prompt. ArXiv, abs/2305.11186."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        501.9653015136719,
        526.0625,
        544.8650512695312
      ],
      "text": "Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shang-\nguang Wang, and Mengwei Xu. 2023. Edgemoe:\nFast on-device inference of moe-based large language\nmodels. ArXiv, abs/2308.14352."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        556.8052978515625,
        524.4138793945312,
        588.7610473632812
      ],
      "text": "Hengrui Zhang, August Ning, Rohan Prabhakar, and\nDavid Wentzlaff. 2023a.\nA hardware evaluation\nframework for large language model inference."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        600.7002563476562,
        526.1536254882812,
        654.5740356445312
      ],
      "text": "Jinchao Zhang, Jue Wang, Huan Li, Lidan Shou,\nKe Chen, Gang Chen, and Sharad Mehrotra. 2023b.\nDraft & verify: Lossless large language model ac-\nceleration via self-speculative decoding.\nArXiv,\nabs/2309.08168."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        666.5133056640625,
        526.06005859375,
        731.3460693359375
      ],
      "text": "Shizhao Zhang, Han Dai, Tian Sheng, Jiawei Zhang,\nXiaoyong Li, Qun Xu, Mengjia Dai, Yunsong Xiao,\nChao Ma, Rui Tang, et al. 2022a. Llm quantization:\nQuantization-aware training for large language mod-\nels. In Advances in Neural Information Processing\nSystems, volume 35."
    },
    {
      "page_no": 12,
      "bbox": [
        306.1419982910156,
        743.2852783203125,
        525.653564453125,
        775.2650146484375
      ],
      "text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,"
    },
    {
      "page_no": 13,
      "bbox": [
        81.46600341796875,
        74.28082275390625,
        290.8755187988281,
        128.06808471679688
      ],
      "text": "Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022b.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068."
    },
    {
      "page_no": 13,
      "bbox": [
        70.86599731445312,
        137.95529174804688,
        290.5185241699219,
        191.82907104492188
      ],
      "text": "Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han,\nYankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan\nLiu, Zeyu Mi, and Maosong Sun. 2024. Relu2 wins:\nDiscovering efficient activation functions for sparse\nllms."
    },
    {
      "page_no": 13,
      "bbox": [
        70.86599731445312,
        201.85214233398438,
        290.8766784667969,
        255.59011840820312
      ],
      "text": "Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn\nChen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,\nTianqi Chen, and Baris Kasikci. 2023. Atom: Low-\nbit quantization for efficient and accurate llm serving.\nArXiv, abs/2310.19102."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        72.64916229248047,
        192.8688201904297,
        84.60436248779297
      ],
      "text": "A\nAppendix Overview"
    },
    {
      "page_no": 14,
      "bbox": [
        70.52799987792969,
        94.36764526367188,
        236.59725952148438,
        105.27674865722656
      ],
      "text": "The appendix is structured as follows:"
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        116.38961791992188,
        291.0398864746094,
        195.0447540283203
      ],
      "text": "• In Appendix B, we provide additional details on\nthe low-rank predictor introduced in Section 3.\nWe evaluate our trained predictors from both ac-\ncuracy (i.e., their impact on the model’s accuracy)\nand efficiency perspectives (i.e., the additional\nneurons they predict to be activated)."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        206.45364379882812,
        289.3150939941406,
        244.4607696533203
      ],
      "text": "• Appendix C offers a more detailed description of\nour experimental setup and implementation for\nthe experiments conducted in Section 4."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        255.78733825683594,
        291.046875,
        348.04827880859375
      ],
      "text": "• In Appendix D, we discuss a negative result re-\ngarding the strategy of bundling neurons based\non co-activation as a potential method for increas-\ning chunk size (cf. Section 3.2). We intention-\nally include this negative result as we believe it\nmay inspire future research on effective neuron\nbundling and its utilization for efficient inference."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        359.4424133300781,
        289.5295715332031,
        383.9417419433594
      ],
      "text": "• In Appendix E, we delve deeper into the review\nof related works in the literature."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        395.35064697265625,
        289.1331787109375,
        419.8087463378906
      ],
      "text": "• In Appendix F, we go over implications of llm in\nflash when going to smaller devices."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        431.13531494140625,
        290.9397277832031,
        469.22576904296875
      ],
      "text": "• Finally, Appendix G compares the texts gener-\nated by the base model with those produced by\nour models that utilize the predictor."
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        481.0821838378906,
        256.2433166503906,
        506.9853820800781
      ],
      "text": "B\nLow-Rank Activation Predictor:\nAdditional Results"
    },
    {
      "page_no": 14,
      "bbox": [
        70.86599731445312,
        516.6491088867188,
        239.58616638183594,
        527.5581665039062
      ],
      "text": "B.1\nSparsity patterns of predictors"
    },
    {
      "page_no": 14,
      "bbox": [
        70.4729995727539,
        534.2040405273438,
        291.0399475097656,
        775.4432373046875
      ],
      "text": "The number of neurons predicted to be active will\ndetermine the efficiency of our algorithm, the less\nsparse the predicted activation the more weights\nwill have to be loaded from flash. We evaluated\nthe sparsity patterns over 100 random samples of\nthe C4 validation dataset. In Figure 9 we can see\nthe sparsity patterns of OPT, Persimmon, and Phi.\nIn OPT, the number of active neurons predicted by\nthe predictor is 3x the amount of actual sparsity\nobserved in the case of dense inference. In Persim-\nmon it is about the same - 3x the required neurons,\nand in Phi-2 it is roughly 2x the required neurons\nof the original model that are activated by the pre-\ndictor. The neurons that are activated by the model\nand not the predictor are the false negatives. The\ngap between the neurons active in both the predic-\ntor and the model, and the neurons active only in\nthe model is very narrow in all three models, hence"
    },
    {
      "page_no": 14,
      "bbox": [
        306.1419982910156,
        73.63481903076172,
        526.3187255859375,
        206.4037628173828
      ],
      "text": "false negatives constitute a small fraction of predic-\ntions. To reduce false negatives, the predictor has\nto \"over-predict\", which results in loading neurons\nthat are redundant, that is, will have zero activation\nand no effect on the outcome. An interesting future\ndirection of this work is improving the accuracy\nof the predictors to be able to load fewer neurons.\nOne observation we had in OPT and Persimmon\nis the later layers have more active neurons, which\ncan be seen in Figure 9d."
    },
    {
      "page_no": 14,
      "bbox": [
        306.1419982910156,
        220.0610809326172,
        501.9929504394531,
        230.9701690673828
      ],
      "text": "B.2\nAccuracy of models using predictors"
    },
    {
      "page_no": 14,
      "bbox": [
        305.3240051269531,
        238.8623504638672,
        526.2252197265625,
        507.28875732421875
      ],
      "text": "We evaluate the accuracy of models on public\nbenchmarks with predictors in place. In Table 4 it\ncan be seen zero shot accuracy of models doesn’t\ndrop. Also, we can see that increasing the predictor\nsize for the last 4 layers of Persimmon and Falcon\nimproves the zero-shot metrics. We evaluated mod-\nels on MMLU (Hendrycks et al., 2021) benchmark\nas well. We used Instruct Eval’s implementaion\n(Chia et al., 2023) for evaluating MMLU. In Figure\n10a we can see the MMLU of Persimmon doesn’t\ndrop when the last 4 layers use higher rank pre-\ndictors but this is not the case for lower ranked\nones. Phi2’s MMLU will drop 2.3 points from\nthe relufied model still keeping at 52 as shown in\nFigure 10b. By increasing the threshold of low-\nrank predictor we can reduce the amount of data\nload, this comes with a slight degradation in zero-\nshot metrics as seen in the Table 4 for different\nthresholds of the Persimmon model. We have used\nthreshold=0.7 for Persimmon."
    },
    {
      "page_no": 14,
      "bbox": [
        306.1419982910156,
        520.9461059570312,
        440.67291259765625,
        531.8551635742188
      ],
      "text": "B.3\nOverhead of predictors"
    },
    {
      "page_no": 14,
      "bbox": [
        305.8039855957031,
        539.7473754882812,
        526.2174072265625,
        672.6817626953125
      ],
      "text": "The average rank of predictors in the OPT-6.7B\nis 240, this will result in less than 2.4% of non-\nembedding weights and FLOPs. In M1 Max CPU\nexperiments this was comprising 2.75% and in\nRTX GPU it was 4.8% of inference time which\nis negligible. For Falocn 7B, predictors take 4%\nmodel size and CPU computation. For Persimmon\nit was taking 2.85% of inference time on CPU. For\nLlama 2 7B it was taking 3.92% of inference time\non CPU."
    },
    {
      "page_no": 14,
      "bbox": [
        306.1419982910156,
        686.9171752929688,
        415.40057373046875,
        698.8723754882812
      ],
      "text": "C\nExtended Results"
    },
    {
      "page_no": 14,
      "bbox": [
        305.8689880371094,
        710.3027954101562,
        526.2225952148438,
        775.4525756835938
      ],
      "text": "Experimental Setup: Our experiment is designed\nto optimize inference efficiency on personal de-\nvices. To this end, we process sequences individ-\nually, running only one sequence at a time. This\napproach allows us to allocate a specific portion of"
    },
    {
      "page_no": 15,
      "bbox": [
        91.46659088134766,
        134.41293334960938,
        175.80874633789062,
        146.22134399414062
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nLayer number"
    },
    {
      "page_no": 15,
      "bbox": [
        79.28178405761719,
        78.05418395996094,
        86.97537994384766,
        134.18765258789062
      ],
      "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0"
    },
    {
      "page_no": 15,
      "bbox": [
        71.82717895507812,
        93.45146179199219,
        78.88919830322266,
        118.28385162353516
      ],
      "text": "Non zero"
    },
    {
      "page_no": 15,
      "bbox": [
        91.181640625,
        80.3311996459961,
        126.47209167480469,
        85.03921508789062
      ],
      "text": "Non Sparse portion"
    },
    {
      "page_no": 15,
      "bbox": [
        106.8275375366211,
        86.86547088623047,
        122.79532623291016,
        104.64202880859375
      ],
      "text": "both\nmodel\npredictor"
    },
    {
      "page_no": 15,
      "bbox": [
        102.03600311279297,
        153.39102172851562,
        150.51731872558594,
        162.357421875
      ],
      "text": "(a) OPT-6.7B"
    },
    {
      "page_no": 15,
      "bbox": [
        205.60960388183594,
        134.41293334960938,
        292.65625,
        146.22134399414062
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\n35\nLayer number"
    },
    {
      "page_no": 15,
      "bbox": [
        193.42478942871094,
        78.04930114746094,
        201.11839294433594,
        133.08154296875
      ],
      "text": "2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5"
    },
    {
      "page_no": 15,
      "bbox": [
        185.97018432617188,
        93.60271453857422,
        193.03219604492188,
        118.43511199951172
      ],
      "text": "Non zero"
    },
    {
      "page_no": 15,
      "bbox": [
        207.29299926757812,
        153.39102172851562,
        273.54571533203125,
        162.357421875
      ],
      "text": "(b) Persimmon 8B"
    },
    {
      "page_no": 15,
      "bbox": [
        316.4942932128906,
        134.41293334960938,
        403.83856201171875,
        146.22134399414062
      ],
      "text": "0\n5\n10\n15\n20\n25\n30\nLayer number"
    },
    {
      "page_no": 15,
      "bbox": [
        309.7088317871094,
        130.57192993164062,
        311.8479919433594,
        135.24256896972656
      ],
      "text": "5"
    },
    {
      "page_no": 15,
      "bbox": [
        307.5667724609375,
        120.47988891601562,
        311.8450927734375,
        125.1505355834961
      ],
      "text": "10"
    },
    {
      "page_no": 15,
      "bbox": [
        307.5667724609375,
        110.38785552978516,
        311.8450927734375,
        115.05850219726562
      ],
      "text": "15"
    },
    {
      "page_no": 15,
      "bbox": [
        307.5667724609375,
        100.29582214355469,
        311.8450927734375,
        104.96646881103516
      ],
      "text": "20"
    },
    {
      "page_no": 15,
      "bbox": [
        307.5667724609375,
        90.20378112792969,
        311.8450927734375,
        94.87442779541016
      ],
      "text": "25"
    },
    {
      "page_no": 15,
      "bbox": [
        307.5667724609375,
        80.11174774169922,
        311.8450927734375,
        84.78239440917969
      ],
      "text": "30"
    },
    {
      "page_no": 15,
      "bbox": [
        300.1121826171875,
        93.40736389160156,
        307.1741943359375,
        118.23976135253906
      ],
      "text": "Non zero"
    },
    {
      "page_no": 15,
      "bbox": [
        338.7489929199219,
        153.39102172851562,
        370.37347412109375,
        162.357421875
      ],
      "text": "(c) Phi-2"
    },
    {
      "page_no": 15,
      "bbox": [
        431.6482238769531,
        133.35842895507812,
        520.3829345703125,
        146.13766479492188
      ],
      "text": "0\n50\n100\n150\n200\n250\nToken"
    },
    {
      "page_no": 15,
      "bbox": [
        424.7198486328125,
        128.04653930664062,
        427.0348815917969,
        133.10118103027344
      ],
      "text": "0"
    },
    {
      "page_no": 15,
      "bbox": [
        422.4017028808594,
        115.56542205810547,
        427.0317687988281,
        120.62006378173828
      ],
      "text": "10"
    },
    {
      "page_no": 15,
      "bbox": [
        422.4017028808594,
        103.08430480957031,
        427.0317687988281,
        108.13894653320312
      ],
      "text": "20"
    },
    {
      "page_no": 15,
      "bbox": [
        422.4017028808594,
        90.60318756103516,
        427.0317687988281,
        95.65782928466797
      ],
      "text": "30"
    },
    {
      "page_no": 15,
      "bbox": [
        422.4017028808594,
        78.1220703125,
        427.0317687988281,
        83.17671203613281
      ],
      "text": "40"
    },
    {
      "page_no": 15,
      "bbox": [
        414.3341979980469,
        75.91722106933594,
        421.976806640625,
        128.70639038085938
      ],
      "text": "Cached neurons%"
    },
    {
      "page_no": 15,
      "bbox": [
        510.5450744628906,
        74.95790100097656,
        520.1347045898438,
        90.74607849121094
      ],
      "text": "layer 8\nlayer 16\nlayer 24\nlayer 32"
    },
    {
      "page_no": 15,
      "bbox": [
        420.6440124511719,
        153.39102172851562,
        516.7638549804688,
        162.357421875
      ],
      "text": "(d) Rows cached over time"
    },
    {
      "page_no": 15,
      "bbox": [
        70.86599731445312,
        173.8415985107422,
        524.413330078125,
        243.69406127929688
      ],
      "text": "Figure 9: (a) The percentage of fired neurons in each layer’s FFN is less than 5%. In predictor, roughly 3x of this\namount will get activated. The narrow gap between neurons that are activated in both shows the model output will\nnot change abruptly. (b) Earlier layers of Persimmon have less active neurons, and later layers of Persimmon have\nhigher active neurons, so we trained larger predictors for them. (c) In Phi2 middle layers have a higher active neuron\nratio, so we trained larger predictors for those layers. (d) The number of neurons cached within a real scenario of\ninference, later layers have more cached rows because of their higher nonsparse ratio."
    },
    {
      "page_no": 15,
      "bbox": [
        93.69956970214844,
        385.08270263671875,
        255.8605194091797,
        399.6644287109375
      ],
      "text": "0.50\n0.55\n0.60\n0.65\n0.70\nThreshold"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        369.47357177734375,
        88.3003921508789,
        375.4421691894531
      ],
      "text": "35"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        354.2772216796875,
        88.3003921508789,
        360.2458190917969
      ],
      "text": "36"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        339.0808410644531,
        88.3003921508789,
        345.0494384765625
      ],
      "text": "37"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        323.8844909667969,
        88.3003921508789,
        329.85308837890625
      ],
      "text": "38"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        308.6881103515625,
        88.3003921508789,
        314.6567077636719
      ],
      "text": "39"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        293.49176025390625,
        88.3003921508789,
        299.4603576660156
      ],
      "text": "40"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        278.29541015625,
        88.3003921508789,
        284.2640075683594
      ],
      "text": "41"
    },
    {
      "page_no": 15,
      "bbox": [
        82.46893310546875,
        263.09906005859375,
        88.3003921508789,
        269.0676574707031
      ],
      "text": "42"
    },
    {
      "page_no": 15,
      "bbox": [
        73.4567642211914,
        296.07647705078125,
        81.1843490600586,
        347.1448974609375
      ],
      "text": "MMLU Accuracy"
    },
    {
      "page_no": 15,
      "bbox": [
        165.53564453125,
        283.45635986328125,
        254.34481811523438,
        320.2010192871094
      ],
      "text": "Persimmon-8B\nPredictor: (32 × 256) + (4 × 1152)\nOriginal\nPredictor: 36 × 256"
    },
    {
      "page_no": 15,
      "bbox": [
        160.98599243164062,
        408.4679870605469,
        170.93869018554688,
        417.43438720703125
      ],
      "text": "(a)"
    },
    {
      "page_no": 15,
      "bbox": [
        416.95196533203125,
        391.8778381347656,
        454.2489013671875,
        399.64935302734375
      ],
      "text": "Phi-2 Model"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        378.8954162597656,
        346.9225158691406,
        384.89794921875
      ],
      "text": "46"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        360.2205810546875,
        346.9225158691406,
        366.2231140136719
      ],
      "text": "48"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        341.5457458496094,
        346.9225158691406,
        347.54827880859375
      ],
      "text": "50"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        322.8708801269531,
        346.9225158691406,
        328.8734130859375
      ],
      "text": "52"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        304.196044921875,
        346.9225158691406,
        310.1985778808594
      ],
      "text": "54"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        285.5212097167969,
        346.9225158691406,
        291.52374267578125
      ],
      "text": "56"
    },
    {
      "page_no": 15,
      "bbox": [
        341.0578918457031,
        266.8463439941406,
        346.9225158691406,
        272.848876953125
      ],
      "text": "58"
    },
    {
      "page_no": 15,
      "bbox": [
        331.9945068359375,
        299.7431640625,
        339.7660217285156,
        350.9004821777344
      ],
      "text": "MMLU Accuarcy"
    },
    {
      "page_no": 15,
      "bbox": [
        360.73883056640625,
        267.403076171875,
        382.6895751953125,
        274.31793212890625
      ],
      "text": "Original"
    },
    {
      "page_no": 15,
      "bbox": [
        393.7974548339844,
        304.7527770996094,
        434.12420654296875,
        311.6676330566406
      ],
      "text": "+ Reluﬁcation"
    },
    {
      "page_no": 15,
      "bbox": [
        437.11602783203125,
        295.41534423828125,
        474.68096923828125,
        302.3302001953125
      ],
      "text": "+ Distillation"
    },
    {
      "page_no": 15,
      "bbox": [
        483.5980224609375,
        314.0902099609375,
        516.4260864257812,
        321.00506591796875
      ],
      "text": "+ Predictor"
    },
    {
      "page_no": 15,
      "bbox": [
        421.52398681640625,
        408.4679870605469,
        431.9787902832031,
        417.43438720703125
      ],
      "text": "(b)"
    },
    {
      "page_no": 15,
      "bbox": [
        70.86599731445312,
        428.8919677734375,
        524.57958984375,
        462.9060974121094
      ],
      "text": "Figure 10: (a) If we use larger predictors in the last 4 layers MMLU wouldn’t drop a lot in Persimmon. (b) Phi’s\nMMLU will drop in the relufication process due to lower quality data, using distillation can improve the results for\nthat. Using predictors will downgrade the results but still keep it at 52."
    },
    {
      "page_no": 15,
      "bbox": [
        70.86599731445312,
        486.61431884765625,
        290.9413146972656,
        538.2527465820312
      ],
      "text": "DRAM for the Key-Value (KV) cache while pri-\nmarily focusing on the model size. This strategy is\nparticularly effective when dealing with only one\nsequence/query at a time.2"
    },
    {
      "page_no": 15,
      "bbox": [
        70.4729995727539,
        542.3787841796875,
        291.0406494140625,
        730.7496948242188
      ],
      "text": "For the implementation of our inference process,\nwe utilize the HuggingFace Transformers and KV\ncaching. This setup is tested under the condition\nthat approximately half of the model size is avail-\nable in DRAM. We select this amount as a show-\ncase of the idea of hosting the LLM in Flash. With\na different level of sparsity or employing quantiza-\ntion, one can work with smaller available DRAM\ncapacity as well, or alternatively use larger models.\nSuch a configuration demonstrates the practicality\nof executing inference with lower memory foot-\nprints.\nSystems performance optimization: The pri-\nmary target of our experiments was the Apple ma-"
    },
    {
      "page_no": 15,
      "bbox": [
        70.86599731445312,
        744.5667114257812,
        289.13653564453125,
        775.0003662109375
      ],
      "text": "2For OPT 6.7 B model with context length 2048 KV-cache\nrequires 2048 × 2dmodel elements which is only 8% of model\nsize. Also, the KV cache itself can be held in flash memory."
    },
    {
      "page_no": 15,
      "bbox": [
        305.7489929199219,
        486.7215270996094,
        526.3215942382812,
        768.614990234375
      ],
      "text": "cOS 14.3 operating system. For high-performance\ninference, most of the existing deep learning frame-\nworks require that the shape of the weights and\nthe intermediate results in the computation remain\nstatic throughout. In particular, the Metal Perfor-\nmance Shaders (MPS) backend for PyTorch demon-\nstrates rather steep performance cliffs when any\nshape dynamism is present in the computational\ngraph. In order to build a high-performance imple-\nmentation, we chose to borrow custom, dynamism-\nfriendly Metal kernels from Apple’s open-source\nMLX deep learning framework (Hannun et al.,\n2023). In addition, we made use of the unified\nmemory architecture available on Apple systems,\nwhich we exploit to maintain the weights cache\nusing the GPU allocator, by creating the tensors us-\ning the MTLStorageModeShared allocation mode.\nThis mode allows both the CPU and the GPU to\naccess the same memory buffer directly, without\nredundant copies. We observe that the inputs to\nand the outputs from the feed-forward network"
    },
    {
      "page_no": 16,
      "bbox": [
        157.9980010986328,
        71.67945861816406,
        436.9706726074219,
        81.64205932617188
      ],
      "text": "Table 4: Model performance on zero-shot tasks when using predictors"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        97.47775268554688,
        499.17572021484375,
        107.37263488769531
      ],
      "text": "Model\nPredictor Parameters\nZero-Shot Metrics"
    },
    {
      "page_no": 16,
      "bbox": [
        225.67877197265625,
        114.93716430664062,
        524.4161987304688,
        137.1214141845703
      ],
      "text": "Rank for\nSensitives *\nRank for\nOther Layers\nThreshold\nArcEasy\nArc\nChallenge\nHella\nSwag"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        144.68685913085938,
        519.1907348632812,
        166.96145629882812
      ],
      "text": "OPT 6.7B\n-\n-\n-\n66.1\n30.6\n50.3\nOPT 6.7B with predictors\n1024\n128\n0.5\n66.2\n30.6\n49.8"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        174.43563842773438,
        524.1381225585938,
        233.57919311523438
      ],
      "text": "Falcon 7B\n-\n-\n-\n74.62\n40.05\n57.77\nFalcon 7B relufied\n-\n-\n-\n72.52\n38.23\n54.17\nFalcon 7B relufied with predictors\n128\n128\n0.50\n70.20\n35.41\n50.74\nFalcon 7B relufied with predictors\n1152\n128\n0.50\n71.51\n34.22\n52.28\nFalcon 7B relufied with predictors\n1152\n256\n0.50\n72.35\n36.35\n53.16"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        241.05331420898438,
        524.1381225585938,
        312.4862365722656
      ],
      "text": "Persimmon 8B\n-\n-\n-\n67.80\n34.64\n50.70\nPersimmon 8B with predictors\n256\n256\n0.5\n67.26\n33.87\n50.51\nPersimmon 8B with predictors\n256\n256\n0.55\n66.71\n34.73\n50.54\nPersimmon 8B with predictors\n256\n256\n0.60\n66.67\n34.04\n50.59\nPersimmon 8B with predictors\n256\n256\n0.65\n66.41\n34.22\n50.42\nPersimmon 8B with predictors\n1152\n256\n0.70\n66.30\n34.40\n52.70"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        319.9604187011719,
        524.1381225585938,
        366.813720703125
      ],
      "text": "Phi-2\n-\n-\n-\n79.62\n51.49\n55.17\nPhi-2 relufied\n-\n-\n-\n80.60\n50.12\n54.30\nPhi-2 relufied with predictors\n800\nmix 160, 480\n0.40\n79.96\n49.57\n53.50\nPhi-2 relufied with predictors\n800\nmix 160, 480\n0.55\n78.90\n47.90\n52.75"
    },
    {
      "page_no": 16,
      "bbox": [
        70.86599731445312,
        371.9996643066406,
        506.1273498535156,
        386.8677673339844
      ],
      "text": "* For OPT, Falcon, and Persimmon sensitive layers are the last 4 layers. For Phi-2 it is the middle 8."
    },
    {
      "page_no": 16,
      "bbox": [
        70.4729995727539,
        409.1223449707031,
        291.0454406738281,
        775.4959716796875
      ],
      "text": "have a static shape, so by hiding the dynamism\ninside a binary extension and handling the shape\ndynamism and memory management internally, we\nwere able to achieve a level of performance that is\nnot achievable with PyTorch MPS backend alone\nwhile leaving the rest of the model intact. Over\nthe course of our work, we were able to eliminate\nnearly all redundant data movement, improving\ninference performance.\nCaching Considerations for Data Loading\nfrom Flash Memory. When data is read from flash\nmemory, the operating system typically caches the\nblocks in the block cache, anticipating future reuse.\nHowever, this caching mechanism consumes ad-\nditional memory in DRAM beyond what is allo-\ncated for the model. To accurately assess the real\nthroughput of flash memory under limited DRAM\nconditions, benchmarks should be conducted with-\nout relying on caching. Practical systems may or\nmay not rely on filesystem cache, depending on\nrequirements.\nFor the purpose of our hardware benchmarking\nin this study, we deliberately and significantly\npessimize our NVMe throughput measurements.\nOn macOS and iOS, we employ the F_NOCACHE\nflag with the fcntl() function, while on Linux,\nwe use DirectIO. Additionally, on macOS, we"
    },
    {
      "page_no": 16,
      "bbox": [
        305.7820129394531,
        409.1223449707031,
        526.3174438476562,
        705.2457275390625
      ],
      "text": "clear any resident buffers before initiating the\nbenchmark using the purge command.\nThis\napproach provides a conservative lower bound\nof throughput in scenarios where no caching is\npermitted and makes the benchmarks repeatable.\nIt’s worth noting that these figures can improve if\neither the inference code or the operating system\nis allowed to cache some part of the weights.\nWhile OS-level buffer caching is advantageous\nfor general-purpose applications with high cache\nhit rates, it lacks fine-grained control over cache\nusage per process or buffer eviction at the appli-\ncation level. In the context of on-device memory\nconstraints and large model sizes, this could lead to\na situation where the file system level cache does\nnot help because in order to evaluate later layers\nearlier layers must be evicted in a rolling pattern,\nso the effective cache hit rate is close to zero. Aside\nfrom being inefficient, this can cause coexistence\nissues with other processes due to memory allo-\ncation pressure and Translation Lookaside Buffer\n(TLB) churn."
    },
    {
      "page_no": 16,
      "bbox": [
        306.1419982910156,
        718.6970825195312,
        464.651123046875,
        729.6061401367188
      ],
      "text": "C.1\nResults for OPT 6.7B Model"
    },
    {
      "page_no": 16,
      "bbox": [
        305.8039855957031,
        737.38037109375,
        524.7443237304688,
        775.4959716796875
      ],
      "text": "This section presents the outcomes for the OPT\n6.7B model, specifically under conditions where\nthe memory allocated for the model in DRAM is"
    },
    {
      "page_no": 17,
      "bbox": [
        70.55699920654297,
        71.60427856445312,
        526.0621337890625,
        105.55209350585938
      ],
      "text": "Table 5: The end-to-end inference latency across different setups with standard deviation. Our efficient imple-\nmentation (referred to as All) that employs the predictor, windowing, and bundling can lead to significant latency\nreduction."
    },
    {
      "page_no": 17,
      "bbox": [
        333.2696533203125,
        122.26461791992188,
        424.2185363769531,
        131.48117065429688
      ],
      "text": "Inference Latency (ms)"
    },
    {
      "page_no": 17,
      "bbox": [
        75.91651153564453,
        138.52719116210938,
        501.3341064453125,
        147.74374389648438
      ],
      "text": "Model\nMethod\nBackend\nI/O\nMem\nCompute\nTotal"
    },
    {
      "page_no": 17,
      "bbox": [
        75.9164810180664,
        154.72076416015625,
        522.1097412109375,
        193.94236755371094
      ],
      "text": "OPT 6.7B\nAll\nCPU\n104.90 (± 18.46)\n57.79 (± 9.63)\n506.50 (±17.33)\n669.20 (± 39.74)\nOPT 6.7B\nAll\nGPU\n30.55 (±3.09)\n34.11 (±2.38)\n19.97 (±0.86)\n84.64 (±6.16)\nOPT 6.7B\nSpeculative\nGPU\n38.53 (±10.0)\n9.45 (±1.7)\n12.18 (±2.0)\n60.16 (±13.4)"
    },
    {
      "page_no": 17,
      "bbox": [
        75.91651153564453,
        194.214111328125,
        524.4136962890625,
        210.20481872558594
      ],
      "text": "Persimmon 8B\nAll\nCPU\n310.52 (±41.12)\n155.80 (±21.30)\n623.74 (± 24.76)\n1090.08 (±79.08)"
    },
    {
      "page_no": 17,
      "bbox": [
        75.91651153564453,
        210.41647338867188,
        522.1096801757812,
        226.46739196777344
      ],
      "text": "Phi-2\nAll\nCPU\n211.08 (±24.81)\n76.87 (±7.18)\n258.74 (±20.90)\n546.69 (±31.98)"
    },
    {
      "page_no": 17,
      "bbox": [
        70.0479965209961,
        246.06063842773438,
        291.0453796386719,
        775.4959716796875
      ],
      "text": "approximately half of its baseline requirement.\nPredictors. For the initial 28 layers of the OPT\n6.7B model, we train predictors with a rank of\nr = 128. To reduce the occurrence of false nega-\ntives, the final four layers employ predictors with a\nhigher rank of r = 1024. These predictors achieve\nan average of 5% false negatives and 7% false posi-\ntives in the OPT 6.7B model. As depicted in Figure\n3a, our predictor accurately identifies most acti-\nvated neurons, while occasionally misidentifying\ninactive ones with values near zero. Notably, these\nfalse negatives, being close to zero, do not signifi-\ncantly alter the final output when they are excluded.\nFurthermore, as demonstrated in Table 1, this level\nof prediction accuracy does not adversely affect the\nmodel’s performance in 0-shot tasks.\nWindowing in the OPT 6.7B Model. Utilizing\na windowing method with k = 4 in the OPT 6.7B\nmodel significantly reduces the necessity for fresh\ndata loading. Using active neurons of predictor\nwould require about 10% of the DRAM memory\ncapacity on average; however, with our method,\nit drops to 2.4%. This process involves reserving\nDRAM memory for a window of the past 5 tokens,\nwhich, in turn, increases the DRAM requirement\nfor the Feed Forward Network (FFN) to 24%.\nThe overall memory retained in DRAM for the\nmodel comprises several components: Embed-\ndings, the Attention Model, the Predictor, and the\nLoaded Feed Forward layer.\nThe Predictor ac-\ncounts for 1.25% of the model size, while Em-\nbeddings constitute 3%. The Attention Model’s\nweights make up 32.3%, and the FFN occupies\n15.5% (calculated as 0.24×64.62). Summing these\nup, the total DRAM memory usage amounts to\n52.1% of the model’s size.\nLatency Analysis: Using a window size of 4,\neach token requires access to 2.4% of the Feed\nForward Network (FFN) neurons. For a 32-bit"
    },
    {
      "page_no": 17,
      "bbox": [
        305.76007080078125,
        245.8081817626953,
        526.2258911132812,
        528.8887939453125
      ],
      "text": "model, the data chunk size per read is 2dmodel ×\n4 bytes = 32 KiB, as it involves concatenated rows\nand columns. On an M1 Max, this results in the\naverage latency of 105ms per token for loading\nfrom flash and 57ms for memory management (in-\nvolving neuron deletion and addition). Thus, the\ntotal memory-related latency is less than 162ms\nper token (refer to Figure 1). In contrast, the base-\nline approach, which requires loading 13.4GB of\ndata at a speed of 6.1GB/s, leads to a latency of\napproximately 2196ms per token. Therefore, our\nmethod represents a substantial improvement over\nthe baseline.\nFor a 16-bit model on a GPU machine, the flash\nload time is reduced to 30.5ms, and memory man-\nagement takes 35ms, slightly higher due to the\nadditional overhead of transferring data from CPU\nto GPU. Nevertheless, the baseline method’s I/O\ntime remains above 2000 milliseconds.\nDetailed comparisons of how each method im-\npacts performance are provided in Table 2."
    },
    {
      "page_no": 17,
      "bbox": [
        306.1419982910156,
        541.8840942382812,
        465.2838439941406,
        552.7931518554688
      ],
      "text": "C.2\nResults for Falcon 7B Model"
    },
    {
      "page_no": 17,
      "bbox": [
        305.8039855957031,
        560.4052124023438,
        526.3228759765625,
        775.4959716796875
      ],
      "text": "To verify that our findings generalize beyond OPT\nmodels we also apply the idea of LLM in flash\nto Falcon model (Almazrouei et al., 2023). Since\nthe original Falcon model is not sparse, we used a\nsparsified (relufied) version with almost the same\nperformance as that of the base version (Mirzadeh\net al., 2023). Similar to the previous section, we\npresent the results obtained under the condition that\napproximately half of the model size is available\nfor use in DRAM.\nPredictors. In the Falcon 7B model, predictors\nof rank r = 256 are used for the initial 28 layers,\nand r = 1152 for the last four layers.\nWindow configuration. Our model reserves\nmemory for a window containing the last 4 tokens.\nThis setup utilizes 33% of the Feed Forward Net-"
    },
    {
      "page_no": 18,
      "bbox": [
        107.38025665283203,
        191.06219482421875,
        129.33099365234375,
        197.97706604003906
      ],
      "text": "Original"
    },
    {
      "page_no": 18,
      "bbox": [
        113.8897705078125,
        205.14926147460938,
        142.91561889648438,
        212.0641326904297
      ],
      "text": "OPT 6.7B"
    },
    {
      "page_no": 18,
      "bbox": [
        131.84768676757812,
        191.06219482421875,
        179.58636474609375,
        197.97706604003906
      ],
      "text": "Ours\nOriginal"
    },
    {
      "page_no": 18,
      "bbox": [
        163.22186279296875,
        205.14926147460938,
        194.10093688964844,
        212.0641326904297
      ],
      "text": "Persimmon"
    },
    {
      "page_no": 18,
      "bbox": [
        182.10305786132812,
        191.06219482421875,
        229.84173583984375,
        197.97706604003906
      ],
      "text": "Ours\nOriginal"
    },
    {
      "page_no": 18,
      "bbox": [
        230.64883422851562,
        205.14926147460938,
        243.2792510986328,
        212.0641326904297
      ],
      "text": "Phi2"
    },
    {
      "page_no": 18,
      "bbox": [
        97.09465789794922,
        186.07296752929688,
        270.2376403808594,
        197.97706604003906
      ],
      "text": "Ours Reluﬁed\n0"
    },
    {
      "page_no": 18,
      "bbox": [
        97.09465789794922,
        169.48654174804688,
        100.02696228027344,
        175.48907470703125
      ],
      "text": "2"
    },
    {
      "page_no": 18,
      "bbox": [
        97.09465789794922,
        152.90011596679688,
        100.02696228027344,
        158.90264892578125
      ],
      "text": "4"
    },
    {
      "page_no": 18,
      "bbox": [
        97.09465789794922,
        136.31369018554688,
        100.02696228027344,
        142.31622314453125
      ],
      "text": "6"
    },
    {
      "page_no": 18,
      "bbox": [
        97.09465789794922,
        119.72727966308594,
        100.02696228027344,
        125.72980499267578
      ],
      "text": "8"
    },
    {
      "page_no": 18,
      "bbox": [
        94.15888214111328,
        103.14085388183594,
        100.02349090576172,
        109.14337921142578
      ],
      "text": "10"
    },
    {
      "page_no": 18,
      "bbox": [
        94.15888214111328,
        86.55442810058594,
        100.02349090576172,
        92.55695343017578
      ],
      "text": "12"
    },
    {
      "page_no": 18,
      "bbox": [
        85.09549713134766,
        122.2040786743164,
        92.86700439453125,
        151.72312927246094
      ],
      "text": "Perplexity"
    },
    {
      "page_no": 18,
      "bbox": [
        70.86599731445312,
        225.47232055664062,
        289.1354064941406,
        259.4200744628906
      ],
      "text": "Figure 11: There is a slight drop in the perplexity of\nOPT and persimmon and more drop in phi-2 after using\npredictors."
    },
    {
      "page_no": 18,
      "bbox": [
        70.4729995727539,
        277.4653625488281,
        291.0454406738281,
        532.3457641601562
      ],
      "text": "work (FFN). In terms of memory allocation, em-\nbeddings take 4.2% of the model size, attention\nweights account for 19.4%, and predictors require\n4%. The active portion of the FFN, given our win-\ndow size, is 25.3% (calculated as 0.33 × 76.8).\nOverall, this amounts to 52.93% of the model’s\ntotal size.\nLatency Analysis. Using a window size of 4\nin our model requires accessing 3.1% of the Feed\nForward Network (FFN) neurons for each token. In\na 32-bit model, this equates to a data chunk size of\n35.5 KiB per read (calculated as 2dmodel ×4 bytes).\nOn an M1 Max device, the time taken to load this\ndata from flash memory is approximately 161ms,\nand the memory management process adds another\n90ms, leading to a total latency of 250ms per token.\nIn comparison, the baseline latency is around 2196\nmilliseconds, making our method approximately 9\nto 10 times faster."
    },
    {
      "page_no": 18,
      "bbox": [
        70.86599731445312,
        543.734130859375,
        166.39698791503906,
        554.6431884765625
      ],
      "text": "C.3\nPersimmon 8B"
    },
    {
      "page_no": 18,
      "bbox": [
        70.35299682617188,
        561.275390625,
        291.04156494140625,
        775.4959716796875
      ],
      "text": "We have applied LLM in Flash for Persimmon 8b\nmodels. Since Persimmon is already using squared\nReLU activation we didn’t need to finetune it fur-\nther.\nPredictors. In the Persimmon 8B base model,\npredictors of rank r=256 are used for the initial\n32 layers and r = 1152 for the last four layers.s.\nPersimmon’s sparsity is less than OPT and Falcon\nso we changed the sigmoid threshold to 0.7. In\nfigure 10a you can see that the MMLU of the model\ndoesn’t drop with this setting. This wouldn’t be the\ncase if all the predictors had a rank of 256. Also in\nFigure 11 you can see that perplexity on wikitext2\nwith doesn’t drop abruptly. Qualitative evaluations\ncan be found in Section G.\nWindow configuration. Our model reserves"
    },
    {
      "page_no": 18,
      "bbox": [
        305.3240051269531,
        73.56822967529297,
        526.2173461914062,
        220.05177307128906
      ],
      "text": "memory for a window containing the last 4 tokens\nand also reduces window size dynamically when-\never the whole memory usage passes the 25% of\nFFN size threshold.\nLatency analysis. Since we have fixed the mem-\nory budget we won’t exceed the 25% limit in the\nFFN which will be 50% of the total model size. We\nused nucleus sampling «cite nucleus» with p=0.9\nto have a broader analysis. As it can be seen in\nFigure 1 it takes 310ms for loading from flash and\n155ms for memory management."
    },
    {
      "page_no": 18,
      "bbox": [
        306.1419982910156,
        231.77110290527344,
        357.960205078125,
        242.68019104003906
      ],
      "text": "C.4\nPhi-2"
    },
    {
      "page_no": 18,
      "bbox": [
        305.6289978027344,
        249.4613494873047,
        526.3174438476562,
        626.5767822265625
      ],
      "text": "We have applied LLM in Flash for Phi-2 models.\nWe first relufied the model then trained the pre-\ndictor and applied inference. Since the model is\nalready small, we gave it 65% of its memory for\nrunning the inference. During the inference, we\nmodified the window size to make sure it will never\nexceed the limit.\nRelufication. We finetuned the model using\na refined-web dataset following Mirzadeh et al.\n(2023). We found that adding a distillation loss\nas suggested by (Liu et al., 2023a) improves the\nresults as can be seen in 10b. MMLU metric drops\nfrom 57 to 54.3 after relufication with distillation.\nPredictors. The sparsity pattern of Phi-2 is dif-\nferent than other models. As you can see in figure\n9c the sparsity of the middle layers is less than other\nlayers for a random sample of the C4 dataset. As a\ngeneral rule of thumb, we trained larger predictors\nfor the less sparse layers. If layers are grouped by\n4, we will have 8 groups of layers. For the last\ngroup, we didn’t use any predictors. For the first,\nsecond, and seventh groups, we trained a predic-\ntor of size 160. For the third and sixth groups we\ntrained predictors of size 480 and for groups in the\nmiddle we trained predictors of size 800.\nLatency analysis. Phi-2 gets 2.35x speedup\nover naive baseline as it can be seen in table 3. It\nalso improves our hybrid-only approach."
    },
    {
      "page_no": 18,
      "bbox": [
        306.1419982910156,
        638.2960815429688,
        371.5965881347656,
        649.2051391601562
      ],
      "text": "C.5\nLlama 2"
    },
    {
      "page_no": 18,
      "bbox": [
        305.7489929199219,
        655.9863891601562,
        524.681640625,
        775.4959716796875
      ],
      "text": "To further validate our result we tried running\nLlama2 (Touvron et al., 2023b) on flash. We used\nthe sparisified Llama2 (Song et al., 2024) as the\nbase model and run our experiments on M1 Max\nCPU. We used window size of 2. We didn’t cache\nweights when the total memory was growing over\n55% of model size.\nSparse models. The sparsified model (Song\net al., 2024) uses FATReLU function to ensure"
    },
    {
      "page_no": 19,
      "bbox": [
        70.4729995727539,
        73.46934509277344,
        291.04150390625,
        548.989013671875
      ],
      "text": "sparsity of llama is above 90%. For models that\nhave used Swi-GLU activation function (having a\ngated linear unit, a down project and an up project),\nreplacing Swish with ReLU within the FFN doesn’t\nensure high amount of sparsity (Mirzadeh et al.,\n2023). The FATReLU function activates neurons\nwith gated value greater than a threshold. This will\nensure only a small portion of neurons are activated\nwhich are the most informative.\nPredictors. We used predictors of size 1024\nin 4 middle layers and predictor of size 256 in all\nother layers. The reason we used larger predictors\nin the middle layers is higher neuron activation in\nmiddle layers (similar to Phi2). The reason why in\nsome networks middle layers are more active and\nin some networks later layers are more active is\nsubject to follow up research.\nLatency analysis. LLM in flash gets 3x speed\nup over naive baseline (Table 3). It is also perform-\ning better than hybrid model which is the theoret-\nical lower bound for approaches that doesn’t use\nsparsity.\nAccuracy analysis. When doing MMLU eval-\nuation using InstructEval repo (Chia et al., 2023)\nwe got MMLU of 41.8 for Llama 2, 38.96 for spar-\nsified model by (Song et al., 2024) and 38.63 after\ntraining our predictors. We noted a difference be-\ntween reported numbers and our evaluations. Using\npredictors on top of the sparse models didn’t hurt\nthe MMLU results.\nAlternative approaches. Since Llama 2’s gate\nproject with FATReLU provides sparse neurons,\nwe can directly use gate project as predictor. This\ncompletely matches with the sparse base model.\nSince gate projects take 1"
    },
    {
      "page_no": 19,
      "bbox": [
        181.28500366210938,
        535.8031005859375,
        275.5771179199219,
        552.7509765625
      ],
      "text": "3 of FFN layer and 5"
    },
    {
      "page_no": 19,
      "bbox": [
        70.4729995727539,
        537.9713745117188,
        289.5174560546875,
        616.708740234375
      ],
      "text": "9 of\neach transformer block, keeping them in memory\nwill occupy more space in DRAM than having\npredictors. In fact with window size of 1, this\napproach resulted in requiring 65% of model size\nin DRAM."
    },
    {
      "page_no": 19,
      "bbox": [
        70.86599731445312,
        631.9791870117188,
        259.6147155761719,
        643.9343872070312
      ],
      "text": "D\nBundling Based on Co-activation"
    },
    {
      "page_no": 19,
      "bbox": [
        70.86599731445312,
        656.1305541992188,
        291.0431823730469,
        775.4894409179688
      ],
      "text": "Given the high reuse of data in sparse models, we\nhypothesize that neurons may be highly correlated\nin their activity patterns, which may enable further\nbundling. To verify this we calculated the activa-\ntions of neurons over the C4 validation dataset. For\neach neuron, the coactivation of that neuron with\nother ones forms a power law distribution as de-\npicted in Figure 12a. Now, let’s call the neuron that\ncoactivates with a neuron the most closest friend."
    },
    {
      "page_no": 19,
      "bbox": [
        305.7489929199219,
        73.46934509277344,
        526.22509765625,
        260.60076904296875
      ],
      "text": "Indeed, the closest friend of each neuron coacti-\nvates with it very often. As Figure 12b demon-\nstrates, it is interesting to see each neuron and its\nclosest friend coactivate with each other at least\n95% of the time. The graphs for the 4th closest\nfriend and 8th closest friend are also drawn. Based\non this information we decided to put a bundle of\neach neuron and its closest friend in the flash mem-\nory; whenever a neuron is predicted to be active\nwe’ll bring its closest friend too. Unfortunately, this\nresulted in loading highly active neurons multiple\ntimes and the bundling worked against our original\nintention. It means the neurons that are very active\nare the ‘closest friends’ of almost everyone."
    },
    {
      "page_no": 19,
      "bbox": [
        306.1419982910156,
        277.30419921875,
        453.35833740234375,
        289.2593994140625
      ],
      "text": "E\nExtended Related Works"
    },
    {
      "page_no": 19,
      "bbox": [
        305.7489929199219,
        302.43902587890625,
        526.3214111328125,
        775.4591674804688
      ],
      "text": "Efficient Inference for Large Language Models.\nAs LLMs grow in size, reducing their computa-\ntional and memory requirements for inference has\nbecome an active area of research. Approaches\nbroadly fall into two categories: model compres-\nsion techniques like pruning and quantization (Han\net al., 2016b; Sun et al., 2023; Jaiswal et al., 2023;\nXia et al., 2023), (Zhang et al., 2022a; Xu et al.,\n2023; Shao et al., 2023; Lin et al., 2023; Hoang\net al., 2023; Zhao et al., 2023; Ahmadian et al.,\n2023; Liu et al., 2023a; Li et al., 2023), and se-\nlective execution like sparse activations (Liu et al.,\n2023b; Mirzadeh et al., 2023; Zhang et al., 2024)\nor conditional computation (Graves, 2016; Baykal\net al., 2023). Our work is complementary, focus-\ning on minimizing data transfer from flash memory\nduring inference.\nSelective Weight Loading. Most related to our\napproach is prior work on selective weight load-\ning. Dejavu (Liu et al., 2023b) exploits activa-\ntion sparsity to load a subset of weights for each\nlayer. However, it still requires loading from GPU\nmemory. Flexgen (Sheng et al., 2023) offloads\nthe weights and KV-cache from GPU memory to\nDRAM and DRAM to flash memory, in contrast,\nwe consider only the cases where the full model\ncan’t reside in the whole DRAM and GPU memory\non the edge devices. Flexgen is theoretically bound\nby the slow throughput of flash to DRAM in such\nscenarios. Similar techniques have been explored\nfor CNNs (Parashar et al., 2017), (Rhu et al., 2013).\nConcurrently, Adapt (Subramani et al., 2022) has\nproposed adaptive weight loading for vision trans-\nformers. We focus on transformer-based LLMs and\nintroduce techniques like neuron bundling tailored"
    },
    {
      "page_no": 20,
      "bbox": [
        87.80790710449219,
        142.45730590820312,
        182.8035430908203,
        147.2235870361328
      ],
      "text": "20 100 200 300 400 500 600 700 800 900 1000"
    },
    {
      "page_no": 20,
      "bbox": [
        96.55960083007812,
        147.30088806152344,
        169.97393798828125,
        154.50750732421875
      ],
      "text": "Top Co-activated Neurons"
    },
    {
      "page_no": 20,
      "bbox": [
        82.42152404785156,
        132.48611450195312,
        86.78743743896484,
        137.2523956298828
      ],
      "text": "10"
    },
    {
      "page_no": 20,
      "bbox": [
        82.42152404785156,
        119.38265228271484,
        86.78743743896484,
        124.14894104003906
      ],
      "text": "30"
    },
    {
      "page_no": 20,
      "bbox": [
        82.42152404785156,
        106.27918243408203,
        86.78743743896484,
        111.04547119140625
      ],
      "text": "50"
    },
    {
      "page_no": 20,
      "bbox": [
        82.42152404785156,
        93.17572021484375,
        86.78743743896484,
        97.94200897216797
      ],
      "text": "70"
    },
    {
      "page_no": 20,
      "bbox": [
        80.2356185913086,
        73.52052307128906,
        86.78743743896484,
        84.83853912353516
      ],
      "text": "90\n100"
    },
    {
      "page_no": 20,
      "bbox": [
        72.23761749267578,
        86.16583251953125,
        79.44424438476562,
        127.52772521972656
      ],
      "text": "Frequency (%)"
    },
    {
      "page_no": 20,
      "bbox": [
        83.05999755859375,
        161.69796752929688,
        171.76458740234375,
        170.66436767578125
      ],
      "text": "(a) coactivation intensity"
    },
    {
      "page_no": 20,
      "bbox": [
        210.71078491210938,
        142.45730590820312,
        295.3333435058594,
        154.50750732421875
      ],
      "text": "94\n95\n96\n97\n98\n99\n100\nPercentage of coactivation"
    },
    {
      "page_no": 20,
      "bbox": [
        201.583740234375,
        139.40696716308594,
        203.76669311523438,
        144.17324829101562
      ],
      "text": "0"
    },
    {
      "page_no": 20,
      "bbox": [
        195.02601623535156,
        131.1521759033203,
        203.75784301757812,
        135.91845703125
      ],
      "text": "2000"
    },
    {
      "page_no": 20,
      "bbox": [
        195.02601623535156,
        122.89737701416016,
        203.75784301757812,
        127.66366577148438
      ],
      "text": "4000"
    },
    {
      "page_no": 20,
      "bbox": [
        195.02601623535156,
        114.64258575439453,
        203.75784301757812,
        119.40887451171875
      ],
      "text": "6000"
    },
    {
      "page_no": 20,
      "bbox": [
        195.02601623535156,
        106.3877944946289,
        203.75784301757812,
        111.15408325195312
      ],
      "text": "8000"
    },
    {
      "page_no": 20,
      "bbox": [
        192.84011840820312,
        98.13300323486328,
        203.75489807128906,
        102.8992919921875
      ],
      "text": "10000"
    },
    {
      "page_no": 20,
      "bbox": [
        192.84011840820312,
        89.87821197509766,
        203.75489807128906,
        94.64450073242188
      ],
      "text": "12000"
    },
    {
      "page_no": 20,
      "bbox": [
        192.84011840820312,
        81.6234130859375,
        203.75489807128906,
        86.38970184326172
      ],
      "text": "14000"
    },
    {
      "page_no": 20,
      "bbox": [
        192.84011840820312,
        73.36862182617188,
        203.75489807128906,
        78.1349105834961
      ],
      "text": "16000"
    },
    {
      "page_no": 20,
      "bbox": [
        185.2328643798828,
        79.8999252319336,
        192.43948364257812,
        133.78550720214844
      ],
      "text": "Number of neurons"
    },
    {
      "page_no": 20,
      "bbox": [
        209.4199981689453,
        161.69796752929688,
        272.17584228515625,
        170.66436767578125
      ],
      "text": "(b) Closest friend"
    },
    {
      "page_no": 20,
      "bbox": [
        318.6429748535156,
        142.45730590820312,
        408.5206298828125,
        154.50750732421875
      ],
      "text": "60\n70\n80\n90\n100\nPercentage of coactivation"
    },
    {
      "page_no": 20,
      "bbox": [
        310.5979309082031,
        139.40696716308594,
        312.7808837890625,
        144.17324829101562
      ],
      "text": "0"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        130.042724609375,
        312.7749938964844,
        134.8090057373047
      ],
      "text": "100"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        120.67845916748047,
        312.7749938964844,
        125.44474792480469
      ],
      "text": "200"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        111.314208984375,
        312.7749938964844,
        116.08049774169922
      ],
      "text": "300"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        101.94995880126953,
        312.7749938964844,
        106.71624755859375
      ],
      "text": "400"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        92.58570861816406,
        312.7749938964844,
        97.35199737548828
      ],
      "text": "500"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        83.2214584350586,
        312.7749938964844,
        87.98774719238281
      ],
      "text": "600"
    },
    {
      "page_no": 20,
      "bbox": [
        306.2261047363281,
        73.85720825195312,
        312.7749938964844,
        78.62349700927734
      ],
      "text": "700"
    },
    {
      "page_no": 20,
      "bbox": [
        298.61883544921875,
        79.8999252319336,
        305.8254699707031,
        133.78550720214844
      ],
      "text": "Number of neurons"
    },
    {
      "page_no": 20,
      "bbox": [
        317.20599365234375,
        161.69796752929688,
        391.1607666015625,
        170.66436767578125
      ],
      "text": "(c) 4th closest friend"
    },
    {
      "page_no": 20,
      "bbox": [
        437.52874755859375,
        142.45730590820312,
        522.0059814453125,
        154.50750732421875
      ],
      "text": "50\n60\n70\n80\n90\n100\nPercentage of coactivation"
    },
    {
      "page_no": 20,
      "bbox": [
        426.1698303222656,
        139.40696716308594,
        428.352783203125,
        144.17324829101562
      ],
      "text": "0"
    },
    {
      "page_no": 20,
      "bbox": [
        421.7980041503906,
        129.2701873779297,
        428.34686279296875,
        134.03646850585938
      ],
      "text": "200"
    },
    {
      "page_no": 20,
      "bbox": [
        421.7980041503906,
        119.13339233398438,
        428.34686279296875,
        123.8996810913086
      ],
      "text": "400"
    },
    {
      "page_no": 20,
      "bbox": [
        421.7980041503906,
        108.99661254882812,
        428.34686279296875,
        113.76290130615234
      ],
      "text": "600"
    },
    {
      "page_no": 20,
      "bbox": [
        421.7980041503906,
        98.85982513427734,
        428.34686279296875,
        103.62611389160156
      ],
      "text": "800"
    },
    {
      "page_no": 20,
      "bbox": [
        419.6120910644531,
        88.7230453491211,
        428.34393310546875,
        93.48933410644531
      ],
      "text": "1000"
    },
    {
      "page_no": 20,
      "bbox": [
        419.6120910644531,
        78.58626556396484,
        428.34393310546875,
        83.35255432128906
      ],
      "text": "1200"
    },
    {
      "page_no": 20,
      "bbox": [
        412.00482177734375,
        79.8999252319336,
        419.2114562988281,
        133.78550720214844
      ],
      "text": "Number of neurons"
    },
    {
      "page_no": 20,
      "bbox": [
        430.34100341796875,
        161.69796752929688,
        504.79791259765625,
        170.66436767578125
      ],
      "text": "(d) 8th closest friend"
    },
    {
      "page_no": 20,
      "bbox": [
        70.86599731445312,
        182.24835205078125,
        524.411376953125,
        240.04507446289062
      ],
      "text": "Figure 12: (a) For a randomly selected neuron from the 10th layer of OPT 6.7B, there exists a group of neurons that\nare coactivated with high probability (b) The closest friend of a neuron is defined as the most coactivated neuron in\nthe same layer, and the closet friend of every neuron in OPT 6.7B almost always get coactivated. (c) The 3rd closest\nfriend gets co-activated with each neuron 86% of the time on average (d) The 7th closest friend seems to be less\nrelevant and doesn’t coactivate with the neuron very often."
    },
    {
      "page_no": 20,
      "bbox": [
        70.86599731445312,
        263.8356628417969,
        112.08056640625,
        274.7447509765625
      ],
      "text": "to LLMs."
    },
    {
      "page_no": 20,
      "bbox": [
        70.86599731445312,
        288.0233459472656,
        290.0404968261719,
        339.6617431640625
      ],
      "text": "To hide flash latency, we build on speculative\nexecution techniques like SpAtten (Dai et al., 2021;\nBae et al., 2023). But, we introduce lightweight\nspeculation tailored to adaptive weight loading."
    },
    {
      "page_no": 20,
      "bbox": [
        70.50599670410156,
        352.9650573730469,
        290.94744873046875,
        485.8737487792969
      ],
      "text": "Hardware Optimizations. There is a rich body\nof work on hardware optimizations for efficient\nLLM inference, including efficient memory archi-\ntectures (Gao et al., 2022), dataflow optimizations\n(Han et al., 2016a; Shao et al., 2022), hardware\nevaluation frameworks (Zhang et al., 2023a), faster\nsparse kernels (Gale et al., 2020) and flash op-\ntimizations (Ham et al., 2016), (Meswani et al.,\n2015). We focus on algorithmic improvements, but\nthese could provide additional speedups."
    },
    {
      "page_no": 20,
      "bbox": [
        70.50599670410156,
        499.0517578125,
        289.5174560546875,
        618.5377197265625
      ],
      "text": "Speculative Execution. Speculative decoding\n(Leviathan et al., 2022; Zhang et al., 2023b; He\net al., 2023) is a technique that uses a draft model\nfor generation and uses the larger model to verify\nthose tokens. This technique is orthogonal to us\nand can be used for further improvement. In the\ncase of speculative decoding, the window in our\nmethod is updated with multiple tokens rather than\none."
    },
    {
      "page_no": 20,
      "bbox": [
        70.50599670410156,
        631.7147827148438,
        289.3239440917969,
        683.4547729492188
      ],
      "text": "Mixture of Experts.\nMixture of Experts\n(Yi et al., 2023) have a sparse structure in their\nfeed-forward layer and can leverage our method\nfor enabling larger models on the device."
    },
    {
      "page_no": 20,
      "bbox": [
        70.86599731445312,
        696.8602905273438,
        290.94927978515625,
        775.4697265625
      ],
      "text": "In summary, we propose algorithmic techniques\nto minimize weight loading from flash memory dur-\ning LLM inference. By combining cost modeling,\nsparsity prediction, and hardware awareness, we\ndemonstrate 4-5x and 20-25x speedup on CPU and\nGPU, respectively."
    },
    {
      "page_no": 20,
      "bbox": [
        305.8330078125,
        261.8883361816406,
        524.4044189453125,
        283.8810729980469
      ],
      "text": "Table 6: Active neuron percentage in different layers\nof OPT 6.7B vs Quantized model over 100 sequences."
    },
    {
      "page_no": 20,
      "bbox": [
        355.265869140625,
        299.0888977050781,
        474.7041015625,
        307.6606140136719
      ],
      "text": "Layer\nOPT 6.7B\nQuantized"
    },
    {
      "page_no": 20,
      "bbox": [
        355.265869140625,
        314.2926330566406,
        466.72393798828125,
        355.1158447265625
      ],
      "text": "1\n1.56%\n1.42%\n16\n2.66%\n2.44%\n32\n5.36%\n5.45%\naverage\n3.30%\n3.27%"
    },
    {
      "page_no": 20,
      "bbox": [
        306.1419982910156,
        373.57415771484375,
        458.0567321777344,
        385.52935791015625
      ],
      "text": "F\nSmall Device Implications"
    },
    {
      "page_no": 20,
      "bbox": [
        305.6289978027344,
        395.0783386230469,
        526.321533203125,
        622.8567504882812
      ],
      "text": "We note that many of the hardware assumptions\n(e.g., limited DRAM capacity, characteristics of\nFlash such as bandwidth limitations and increased\nthroughput with larger chunks) are also applicable\nto smaller devices like smartphones. For exam-\nple, when running a 7B model on a smartphone,\nthe same technique can be employed; instead of\nloading 16-bit values, 4-bit values would be loaded.\nConsequently, rather than the baseline’s 3.5GB of\nrequired DRAM, our model would necessitate less\nthan 2GB of memory for operation.x We believe\nthat since quantization does not alter activation\nsparsity patterns, the same technique can be uti-\nlized here by loading 4-bit instead of 16-bit values\n(Table 6). However, actual implementation would\nrequire special 4-bit compute kernels on device,\nwhich falls outside the scope of this paper."
    },
    {
      "page_no": 20,
      "bbox": [
        306.1419982910156,
        634.5811767578125,
        447.6079406738281,
        646.536376953125
      ],
      "text": "G\nQualitative Evaluations"
    },
    {
      "page_no": 20,
      "bbox": [
        306.1419982910156,
        656.0853881835938,
        524.4148559570312,
        775.4697265625
      ],
      "text": "Here we present some of the examples generated\nfrom the model with a predictor. The purpose is to\nqualitatively demonstrate that the models’ output is\ncomparatively reasonable when the predictors are\napplied. Table 7 and 8 show the results where the\nprompt was \"once upon a time there was a\" and\nthe models are asked to complete it. Table 9 shows\nthe results after prompting the model to complete a\nrandom sample taken from C4 dataset."
    },
    {
      "page_no": 21,
      "bbox": [
        138.33200073242188,
        229.34449768066406,
        456.6369323730469,
        239.30709838867188
      ],
      "text": "Table 7: Comparison of the output of original model and model with predictors."
    },
    {
      "page_no": 21,
      "bbox": [
        76.84400177001953,
        255.53326416015625,
        248.69473266601562,
        266.63275146484375
      ],
      "text": "Prompt: Once upon a time there was a"
    },
    {
      "page_no": 21,
      "bbox": [
        76.84400177001953,
        286.7860412597656,
        435.3714904785156,
        297.6951599121094
      ],
      "text": "OPT 6.7B\nOPT 6.7B with predictors"
    },
    {
      "page_no": 21,
      "bbox": [
        76.84400177001953,
        305.9166564941406,
        279.4368896484375,
        438.768798828125
      ],
      "text": "little girl who was very sad.\nShe was sad because her daddy was gone.\nHe was in the army and he was in Iraq.\nShe was sad because she missed him so much.\nShe was sad because she was so lonely.\nShe was sad because she was so scared.\nShe was sad because she was so sad.\nShe was sad because she was sad.\nShe was sad because she was sad.\nShe was sad because she was sad."
    },
    {
      "page_no": 21,
      "bbox": [
        315.2320251464844,
        305.9167175292969,
        542.7208862304688,
        438.7687683105469
      ],
      "text": "man who was very rich.\nHe had a beautiful wife and two beautiful children.\nHe was a very good man and he loved his family very\nmuch.\nOne day he was walking down the street when he\nsaw a poor man who was begging for money.\nThe man was very poor and he had no money.\nThe man asked the man for some money.\nThe man gave the man some money.\nThe man was very happy and he thanked the man."
    },
    {
      "page_no": 21,
      "bbox": [
        76.84400177001953,
        458.92205810546875,
        457.8660583496094,
        469.8311767578125
      ],
      "text": "Persimmon 8B\nPersimmon 8B with predictors"
    },
    {
      "page_no": 21,
      "bbox": [
        76.84400177001953,
        478.05267333984375,
        303.6180419921875,
        610.9037475585938
      ],
      "text": "man named Bilbo Baggins.\nBilbo Baggins was a small hobbit, and he lived in a\nhole in the ground.\nBilbo Baggins had many adventures, and he met\nmany friends.\nBilbo Baggins was brave, and he was not afraid of\nanything.\nBilbo Baggins was happy, and he always smiled.\nBilbo Baggins was a kind person, and he always\nhelped others."
    },
    {
      "page_no": 21,
      "bbox": [
        315.5710144042969,
        478.0485534667969,
        542.7233276367188,
        610.9037475585938
      ],
      "text": "man named Bilbo Baggins. He lived in a hole in the\nground, with many other hobbits.\nBilbo Baggins was a very brave hobbit. He liked to\neat LOTS of cheese.\nOne day, Bilbo Baggins decided to leave his hobbit\nhole. He decided to travel the world.\nBilbo Baggins was very hungry.\nHe ate many\ncheeses.\nBilbo Baggins was very tired. He slept in a cave.\nBilbo Baggins was very cold. He"
    },
    {
      "page_no": 22,
      "bbox": [
        138.33200073242188,
        127.72547912597656,
        456.6369323730469,
        137.68807983398438
      ],
      "text": "Table 8: Comparison of the output of original model and model with predictors."
    },
    {
      "page_no": 22,
      "bbox": [
        76.84400177001953,
        153.91424560546875,
        248.69473266601562,
        165.0137481689453
      ],
      "text": "Prompt: Once upon a time there was a"
    },
    {
      "page_no": 22,
      "bbox": [
        76.84400177001953,
        185.16709899902344,
        455.6732482910156,
        196.07618713378906
      ],
      "text": "Phi-2\nRelufied Phi-2 with predictors"
    },
    {
      "page_no": 22,
      "bbox": [
        76.45099639892578,
        204.380859375,
        305.5193786621094,
        540.3877563476562
      ],
      "text": "young girl named Lily who loved to read. She would\nspend hours in her room with her nose buried in a\nbook, lost in the world of the characters she was\nreading about. Her parents were always happy to see\nher reading, knowing that it was good for her mind\nand her imagination.\nOne day, Lily’s parents decided to take her to the\nlibrary to pick out some new books to read. As\nthey walked through the aisles, Lily’s eyes widened\nwith excitement at all the different titles and covers.\nShe picked out a few books that caught her eye and\ncouldn’t wait to start reading them.\nAs they were leaving the library, Lily’s parents no-\nticed a sign advertising a book club for kids. They\nthought it would be a great opportunity for Lily to\nmeet other kids who loved to read and discuss books\ntogether. Lily was hesitant at first, but her parents\nconvinced her to give it a try.\nAt the first book club meeting, Lily was nervous but\nexcited to meet the other kids. They all sat in a circle\nand took turns sharing their thoughts on the book\nthey had read. Lily was surprised at how much she\nenjoyed hearing other people’s perspectives on the\nstory. She realized that reading wasn’t just about\nescaping into a world of her"
    },
    {
      "page_no": 22,
      "bbox": [
        315.1780090332031,
        204.380859375,
        543.7102661132812,
        499.73974609375
      ],
      "text": "young girl named Lily who loved to read. She would\nspend hours in her room with her nose buried in a\nbook, lost in the world of the characters she was\nreading about. Her favorite books were the ones\nthat had a happy ending, where the main character\novercame their obstacles and found true love.\nOne day, Lily’s parents surprised her with a trip to the\nlibrary. She was so excited to see all the new books\nshe could check out. As she walked through the\naisles, she noticed a book with a bright yellow cover\nthat caught her eye. It was called \"The Adventures\nof Lily and the Magic Book.\"\nLily eagerly checked out the book and took it home\nto read. As she read, she was transported to a magical\nworld where she met a talking cat, a brave knight,\nand a beautiful princess. The book was filled with\nadventure, magic, and romance.\nAs Lily read on, she noticed that the book was getting\ndarker and darker. The characters were facing more\nand more obstacles, and it seemed like they might\nnot make it out alive. Lily began to worry that the\nbook was going to end badly."
    },
    {
      "page_no": 22,
      "bbox": [
        76.84400177001953,
        560.5410766601562,
        462.0333251953125,
        571.4501342773438
      ],
      "text": "Sparse Llama 2\nSparse Llama 2 with predictors"
    },
    {
      "page_no": 22,
      "bbox": [
        76.33100128173828,
        579.7506103515625,
        305.5227355957031,
        712.5227661132812
      ],
      "text": "little girl who loved to play with her dolls. She loved\nto dress them up and make them do all sorts of things.\nOne day, she decided to make a dollhouse for her\ndolls.\nWrite a dialogue around the person in the article\nbased on the content in the previous article.\nMelanie: Hey, Mom! I was thinking about making a\ndollhouse for my dolls.\nMother: That sounds like a great idea, Melanie!\nWhat"
    },
    {
      "page_no": 22,
      "bbox": [
        315.5710144042969,
        579.7548217773438,
        544.1493530273438,
        712.5227661132812
      ],
      "text": "little girl named Lily. She loved to play with her dolls\nand stuffed animals. One day, she decided to make\na new doll for herself. Write a dialogue between\nthe person in the background and the person in the\nforeground.\nPerson in the background: Lily, what are you doing?\nPerson in the foreground: I’m making a doll for my-\nself.\nPerson in the background: Why do you need a doll?\nPerson in the foreground"
    },
    {
      "page_no": 23,
      "bbox": [
        138.33200073242188,
        209.0204620361328,
        456.6369323730469,
        218.98306274414062
      ],
      "text": "Table 9: Comparison of the output of original model and model with predictors."
    },
    {
      "page_no": 23,
      "bbox": [
        76.84400177001953,
        235.25172424316406,
        543.9972534179688,
        300.31536865234375
      ],
      "text": "Prompt: There is a lot of discussion right now about stress reduction. Today we see an increased work load\nof mental health professionals, increased occurrences of abuse or violence faced by social workers and other\ntherapists, and the terrible struggles facing our returning veterans. All of these things have increased our\nawareness of the stressful elements of our society and the impact those elements have on both our clients and\nthe professionals whose job it is to help. As a social worker I am"
    },
    {
      "page_no": 23,
      "bbox": [
        76.84400177001953,
        320.6590881347656,
        542.3467407226562,
        331.5682067871094
      ],
      "text": "Persimmon 8B (nucleus sampling)\nPersimmon 8B with predictors (nucleus sampling)"
    },
    {
      "page_no": 23,
      "bbox": [
        76.84400177001953,
        339.70733642578125,
        305.4318542480469,
        486.1907653808594
      ],
      "text": "constantly reminded of the importance of stress re-\nduction.\nam aware of this stress, and as a psychotherapist I\nhelp my clients learn techniques to deal with these\nstressful elements.\nI am writing to encourage all mental health profes-\nsionals to learn yoga. I strongly believe that yoga\nshould be part of your clinical routine, and that yoga\nclasses should be taken by clinicians for credit, just\nlike psychotherapy, case management and substance\nuse classes"
    },
    {
      "page_no": 23,
      "bbox": [
        315.5710144042969,
        339.70733642578125,
        542.3466186523438,
        418.44476318359375
      ],
      "text": "constantly reflecting on my own stress levels and\nhow I can remain more fit to handle the pressures of\nthe profession. In social work a social worker calls\nthemselves \"a problem solver.\" While this is the often\nexpected role of the professional, solving a problem\ncan often require complex actions to be successful."
    },
    {
      "page_no": 23,
      "bbox": [
        76.84400177001953,
        492.7950744628906,
        455.6732482910156,
        503.7041931152344
      ],
      "text": "Phi-2\nRelufied Phi-2 with predictors"
    },
    {
      "page_no": 23,
      "bbox": [
        76.45099639892578,
        511.8432922363281,
        305.5227355957031,
        631.227783203125
      ],
      "text": "constantly looking for ways to reduce stress in my\nlife and in the lives of my clients. One of the most\neffective ways to reduce stress is to take time to relax.\nThis is not always easy to do, especially when you\nare working with clients who are in crisis. However,\nit is important to take time to relax and recharge your\nbatteries. One way to do this is to take a break from\nwork and go for a walk. Walking is a great way to\nclear your mind and get some"
    },
    {
      "page_no": 23,
      "bbox": [
        314.75201416015625,
        511.93804931640625,
        542.3468627929688,
        631.227783203125
      ],
      "text": "often asked how I deal with stress. I have found that\nthere are a few things that I do that help me to manage\nstress.\n1. I make sure that I get enough sleep. I try to get\nat least 8 hours of sleep a night. I also try to get up\nearly and get out of bed by 7:00 a.m. This helps me\nto start my day feeling refreshed and ready to take on\nthe day.\n2. I make sure that I eat a healthy diet"
    }
  ],
  "pictures": [
    {
      "page_no": 2,
      "bbox": [
        222.0,
        72.0,
        246.0,
        151.0
      ],
      "xref": 0,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p2_blk1_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        70.0,
        166.0,
        178.0,
        179.0
      ],
      "xref": 1,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p2_blk2_crop.png"
    },
    {
      "page_no": 5,
      "bbox": [
        409.6671447753906,
        325.74810791015625,
        444.54534912109375,
        331.9949645996094
      ],
      "xref": 35,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p5_blk1_crop.png"
    },
    {
      "page_no": 5,
      "bbox": [
        452.7666320800781,
        325.74810791015625,
        487.64483642578125,
        331.9949645996094
      ],
      "xref": 36,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p5_blk2_crop.png"
    },
    {
      "page_no": 5,
      "bbox": [
        387.99285888671875,
        365.3598327636719,
        395.043701171875,
        396.5010681152344
      ],
      "xref": 37,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p5_blk3_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        410.86126708984375,
        178.31883239746094,
        449.3272705078125,
        184.85079956054688
      ],
      "xref": 73,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p6_blk1_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        277.244140625,
        179.5824737548828,
        315.71014404296875,
        186.11444091796875
      ],
      "xref": 74,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p6_blk2_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        148.8970184326172,
        179.5824737548828,
        187.36302185058594,
        186.11444091796875
      ],
      "xref": 75,
      "image_path": "../data/parsed_documents/2312.11514/images/2312.11514_p6_blk3_crop.png"
    }
  ],
  "tables": [
    {
      "page_no": 1,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p1_table1_lattice.csv"
    },
    {
      "page_no": 2,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p2_table1_lattice.csv"
    },
    {
      "page_no": 3,
      "index": 1,
      "flavor": "stream",
      "nrows": 61,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p3_table1_stream.csv"
    },
    {
      "page_no": 4,
      "index": 1,
      "flavor": "stream",
      "nrows": 56,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p4_table1_stream.csv"
    },
    {
      "page_no": 5,
      "index": 1,
      "flavor": "lattice",
      "nrows": 6,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p5_table1_lattice.csv"
    },
    {
      "page_no": 6,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p6_table1_lattice.csv"
    },
    {
      "page_no": 6,
      "index": 2,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p6_table2_lattice.csv"
    },
    {
      "page_no": 7,
      "index": 1,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p7_table1_lattice.csv"
    },
    {
      "page_no": 8,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p8_table1_lattice.csv"
    },
    {
      "page_no": 8,
      "index": 2,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p8_table2_lattice.csv"
    },
    {
      "page_no": 9,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p9_table1_lattice.csv"
    },
    {
      "page_no": 10,
      "index": 1,
      "flavor": "stream",
      "nrows": 90,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p10_table1_stream.csv"
    },
    {
      "page_no": 11,
      "index": 1,
      "flavor": "stream",
      "nrows": 88,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p11_table1_stream.csv"
    },
    {
      "page_no": 12,
      "index": 1,
      "flavor": "stream",
      "nrows": 64,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p12_table1_stream.csv"
    },
    {
      "page_no": 13,
      "index": 1,
      "flavor": "stream",
      "nrows": 15,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p13_table1_stream.csv"
    },
    {
      "page_no": 14,
      "index": 1,
      "flavor": "stream",
      "nrows": 83,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p14_table1_stream.csv"
    },
    {
      "page_no": 15,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p15_table1_lattice.csv"
    },
    {
      "page_no": 15,
      "index": 2,
      "flavor": "lattice",
      "nrows": 8,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p15_table2_lattice.csv"
    },
    {
      "page_no": 15,
      "index": 3,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 9,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p15_table3_lattice.csv"
    },
    {
      "page_no": 16,
      "index": 1,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p16_table1_lattice.csv"
    },
    {
      "page_no": 17,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p17_table1_lattice.csv"
    },
    {
      "page_no": 18,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 15,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p18_table1_lattice.csv"
    },
    {
      "page_no": 19,
      "index": 1,
      "flavor": "stream",
      "nrows": 65,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p19_table1_stream.csv"
    },
    {
      "page_no": 20,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p20_table1_lattice.csv"
    },
    {
      "page_no": 20,
      "index": 2,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p20_table2_lattice.csv"
    },
    {
      "page_no": 20,
      "index": 3,
      "flavor": "lattice",
      "nrows": 9,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p20_table3_lattice.csv"
    },
    {
      "page_no": 20,
      "index": 4,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 9,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p20_table4_lattice.csv"
    },
    {
      "page_no": 21,
      "index": 1,
      "flavor": "stream",
      "nrows": 24,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p21_table1_stream.csv"
    },
    {
      "page_no": 22,
      "index": 1,
      "flavor": "stream",
      "nrows": 39,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p22_table1_stream.csv"
    },
    {
      "page_no": 23,
      "index": 1,
      "flavor": "stream",
      "nrows": 28,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2312.11514/2312.11514_p23_table1_stream.csv"
    }
  ]
}