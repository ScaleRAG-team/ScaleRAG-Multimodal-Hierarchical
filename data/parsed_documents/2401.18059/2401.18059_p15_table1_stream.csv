"Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.
oLMpics– on what
language"
"Transactions of
model pre-training captures.
the Association for Computational Linguistics, 8:"
"743–758, 2020. URL https://arxiv.org/abs/1912.13283."
"Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,"
"Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models"
"with retrieval?
a comprehensive study.
arXiv preprint arXiv:2304.06762, 2023. URL https:"
"//arxiv.org/abs/2304.06762."
"Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe,
Jan Leike,
and Paul"
"Christiano.
Recursively Summarizing Books with Human Feedback, 2021.
URL https:"
"//arxiv.org/abs/2109.10862."
"Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,"
"and Quoc V. Le. QANet: Combining Local Convolution with Global Self-Attention for Read-"
"ing Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint"
"arXiv:1804.09541."
"Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang"
"Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are"
"strong context generators, 2022. URL https://arxiv.org/abs/2209.10063."
"Shiyue Zhang, David Wan,
and Mohit Bansal.
Extractive is not
faithful: An investigation of"
"broad unfaithfulness problems
in extractive summarization.
In Anna Rogers,
Jordan Boyd-"
"the 61st Annual Meeting of
the Association
Graber, and Naoaki Okazaki (eds.), Proceedings of"
"for Computational Linguistics (Volume 1: Long Papers), pp. 2153–2174, Toronto, Canada, July"
"2023. Association for Computational Linguistics.
doi:
10.18653/v1/2023.acl-long.120.
URL"
"https://aclanthology.org/2023.acl-long.120."
