"Fast Inference from Transformers via Speculative Decoding",""
"put, and not general approximation models, making it only","experiments we always performed the same standardization"
"suitable for the cases where the inputs and outputs are very","on the distributions generated by the approximation model"
"similar like grammatical error correction. In addition, simi-","as the desired one for the target model (Section 2.2), but fur-"
"larly to Blockwise Parallel Decoding, SAD does not support","ther improvements might be obtained by applying different"
"the general stochastic sampling setting.","transformations. We tested speculative decoding only in the"
"","text modality, but it might work well in other domains (e.g."
"After we initially published our work, an independent im-",""
"","images) which would be interesting to experiment with."
"plementation of speculative decoding (Chen et al., 2023)",""
"showed similar 2X-2.5X improvements on Chinchilla 70B.","Finally, we note that stochastic speculative execution and"
"","speculative sampling can be helpful outside the scope of"
"","speculative decoding from autoregressive models. For ex-"
"6. Discussion",""
"","ample, given two slow functions, f (x) and g(y) such that"
"We presented speculative sampling which enables efﬁcient","f (x) generates a distribution from which g’s input is sam-"
"stochastic speculative execution -
i.e.
speculative execu-","pled, we could use our method to run f and g in parallel."
"tion in the stochastic setting. We analyzed its impact on","This setup might arise e.g.
in physics simulations, or in rein-"
"decoding from autoregressive models like Transformers via","forcement learning where f is a large model that produces a"
"speculative decoding and have shown that given enough","distribution on actions, and g is the world simulation, which"
"compute resources, we get meaningful 2X-3X speedups in","would be interesting to explore."
"practice vs T5X, a popular optimized implementation.",""
"One limitation of speculative execution in general, and of","Acknowledgments"
"speculative decoding in particular,
is that
latency is
im-",""
"","We would like to extend a special thank you to YaGuang Li"
"proved through increased concurrency at the cost of an in-",""
"","for help with everything LaMDA related and for calculating"
"creased number of arithmetic operations. Thus, our method",""
"","the LaMDA ﬁgures in the paper, and to Blake Hechtman"
"is not helpful for conﬁgurations where additional compu-",""
"","for great insights and help with XLA. We would also like"
"tation resources are not available. However,
in common",""
"","to thank the reviewers for
insightful comments, as well"
"cases where additional computation resources are available",""
"","as Asaf Aharoni, Reiner Pope, Sasha Goldshtein, Nadav"
"(e.g. when memory bandwidth is the bottleneck) our method",""
"","Sherman, Eyal Segalis, Eyal Molad, Dani Valevski, Daniel"
"provides the speedup with signiﬁcant beneﬁts:
the model",""
"","Wasserman, Valerie Nygaard, Danny Vainstein, the LaMDA"
"architecture doesn’t change, retraining isn’t required, and",""
"","and Theta Labs teams at Google, and our families."
"most importantly, the output distribution is guaranteed to",""
"stay the same. Our method is easy to implement, and can",""
"be used to speedup inference using out-of-the-box models","References"
"without developing and evaluating custom schemes.",""
"","Bapna, A., Arivazhagan, N., and Firat, O.
Controlling"
"There are several directions for follow up research, impor-","computation versus quality for neural sequence models."
"tantly, further investigating the compatibility of speculative","ArXiv, abs/2002.07106, 2020."
"decoding with beam search (see Appendix A.4). Also, while",""
"","Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,"
"our method yields substantial speedups with existing off-the-",""
"","J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,"
"shelf approximation models, greater improvements might",""
"","Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,"
"be obtained via custom approximation models (Section 3.6),",""
"","Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,"
"such as those with custom architectures (e.g. custom sizes,",""
"","J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,"
"non-autoregressive models, or various heuristics) or with",""
"","Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,"
"custom training procedures (e.g. standard distillation with",""
"","S., Radford, A., Sutskever,
I., and Amodei, D.
Lan-"
"It
soft targets from Mp, or optimizing Mq for α directly).",""
"","guage models are few-shot learners.
In Proceedings of"
"could also be interesting to explore a hierarchical version",""
"","the 34th International Conference on Neural
Informa-"
"of the algorithm, where the approximation model is itself",""
"","tion Processing Systems, NIPS’20, Red Hook, NY, USA,"
"accelerated by an even faster model, which could allow",""
"","2020. Curran Associates Inc.
ISBN 9781713829546."
"for more capable approximation models.
In this work we",""
"ﬁxed the approximation model and the number of guesses",""
"","Burton, F. W.
Speculative computation, parallelism, and"
"γ throughout inference, but varying them during inference",""
"","IEEE Transactions on Comput-
functional programming."
"could yield additional improvements (Section 3.5). In our",""
"","ers, C-34(12):1190–1193, 1985. doi: 10.1109/TC.1985."
"","6312218."
"6Note that
the outputs from the LaMDA model always go",""
"through a T op40 ﬁlter. This has no effect on argmax, but does",""
"","Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,"
"have some effect on standard sampling.",""
"","Koehn, P. T., and Robinson, T. One billion word bench-"
