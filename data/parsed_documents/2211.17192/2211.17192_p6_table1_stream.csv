"","","Wall time","","","","","",""
"Figure 5. A simpliﬁed trace diagram for a full encoder-decoder Transformer stack. The top row shows speculative decoding with γ = 7","","","","","","","",""
"","so each of the calls to Mp (the purple blocks) is preceded by 7 calls to Mq (the blue blocks). The yellow block on the left is the call to the","","","","","","",""
"","encoder for Mp and the orange block is the call to the encoder for Mq. Likewise the middle row shows speculative decoding with γ = 3,","","","","","","",""
"and the bottom row shows standard decoding.","","","","","","","",""
"","copies tokens from the context in case we ﬁnd a matching","","","approximation models. As expected we see that α increases","","","",""
"","preﬁx, might yield high values of α. These parameter-less","","","with the size of the approximation model. Interestingly, α","","","",""
"","approximation models, have the additional advantage of","","","and walltime improvement are higher for argmax sampling","","","",""
"","being even simpler to deploy from a production standpoint.","","","(temp=0). We observe speedups of 2.6X (temp=1) and 3.4X","","","",""
"","","","","(temp=0) on the translation task and slightly lower speedups","","","",""
"","Another type of approximation models that can be used by","","","","","","",""
"","","","","of 2.3X (temp=1) and 3.1X (temp=0) for the summarization","","","",""
"","speculative decoding are non-autoregressive models,","like","","","","","",""
"","","","","task. These empirical results match well with the theoreti-","","","",""
"those from (Stern et al., 2018). Then,","instead of","the au-","","","","","",""
"","","","","cal predictions, with some variance due to implementation","","","",""
"togreressive loop in Algorithm 1 we’d just call","","the non-","","","","","",""
"","","","","details (see Appendix A.3).","","","",""
"autoregressive model once.","","","","","","","",""
"","A ﬁnal example, interesting mostly from a theoretical per-","","","","","","",""
"","","","","Table 2. Empirical results for speeding up inference from a T5-","","","",""
"","spective, is an approximation model which chooses tokens","","","","","","",""
"","","","XXL 11B model.","","","","",""
"","at random, which guarantees some improvement (although","","","","","","",""
"very small) for all models Mp.","","","","","","","",""
"","","","TASK","Mq","TEMP","γ","α","SPEED"
"","","","ENDE","T5-SMALL (cid:70)","0","7","0.75","3.4X"
"","","","ENDE","T5-BASE","0","7","0.8","2.8X"
"","","","ENDE","T5-LARGE","0","7","0.82","1.7X"
"","","","ENDE","T5-SMALL (cid:70)","1","7","0.62","2.6X"
"4. Experiments","","","ENDE","T5-BASE","1","5","0.68","2.4X"
"","","","ENDE","T5-LARGE","1","3","0.71","1.4X"
"4.1. Empirical Walltime Improvement","","","","","","","",""
"","","","CNNDM","T5-SMALL (cid:70)","0","5","0.65","3.1X"
"","","","CNNDM","T5-BASE","0","5","0.73","3.0X"
"","We implement our algorithm and compare it to the imple-","","","","","","",""
"","","","CNNDM","T5-LARGE","0","3","0.74","2.2X"
"","mentation in the T5X codebase for accelerating T5-XXL.","","","","","","",""
"","","","CNNDM","T5-SMALL (cid:70)","1","5","0.53","2.3X"
"","","","CNNDM","T5-BASE","1","3","0.55","2.2X"
"Setup","We test a standard encoder-decoder T5 version 1.1","","CNNDM","T5-LARGE","1","3","0.56","1.7X"
"","model (Raffel et al., 2020) on two tasks from the T5 paper:","","","","","","",""
