"Figure 1. Our technique illustrated in the case of unconditional language modeling. Each line represents one iteration of the algorithm.",""
"The green tokens are the suggestions made by the approximation model (here, a GPT-like Transformer decoder with 6M parameters",""
"trained on lm1b with 8k tokens) that the target model (here, a GPT-like Transformer decoder with 97M parameters in the same setting)",""
"accepted, while the red and blue tokens are the rejected suggestions and their corrections, respectively. For example, in the ﬁrst line the",""
"target model was run only once, and 5 tokens were generated.",""
"be accepted, while guaranteeing that the outputs from our","2. Speculative Decoding"
"system have the same distribution as those from the target",""
"","2.1. Overview"
"model alone. For example,
the sentence in Figure 1, con-",""
"sisting of 38 tokens, was generated by our method with",""
"","inference from which we’re
Let Mp be the target model,"
"only 9 serial runs of a larger target model (97M parameters)",""
"","trying to accelerate, and p(xt|x<t) the distribution we get"
"thanks to a smaller and more efﬁcient approximation model",""
"","from the model for a preﬁx x<t. Let Mq be a more efﬁ-"
"(6M parameters), while the probability of generating it
is",""
"","cient approximation model for the same task, and denote"
"unchanged.",""
"","by q(xt|x<t) the distribution we get from the model for a"
"","1. The core idea is to (1) use the more efﬁcient
preﬁx x<t"
"We analyze our method in a variety of
tasks and model",""
"","model Mq to generate γ ∈ Z+ completions (see Section 3.5"
"sizes: unconditional generation from a 97M parameter GPT-",""
"","for how to optimally choose this parameter),
then (2) use"
"like model trained on lm1b, English to German translation",""
"","the target model Mp to evaluate all of the guesses and their"
"and news article summarization with an 11B parameters",""
"","respective probabilities from Mq in parallel, accepting all"
"T5-XXL model, and a dialog task with a 137B parameter",""
"","those that can lead to an identical distribution, and (3) sam-"
"LaMDA model. We implement our method and compare",""
"","pling an additional token from an adjusted distribution to ﬁx"
"actual walltimes for T5-XXL to those of the robust T5X",""
"","the ﬁrst one that was rejected, or to add an additional one"
"implementation (Roberts et al., 2022), showing an out-of-",""
"","if they are all accepted. That way, each parallel run of the"
"the-box latency improvement of 2X-3X, without any change",""
"","target model Mp will produce at least one new token (so the"
"to the outputs (Section 4).",""
"","number of serial runs of the target model can never, even"
"Our method is easy to employ in actual production settings,",""
"","in the worst case, be larger than the simple autoregressive"
"doesn’t require training new models, and doesn’t change the",""
"","method), but it can potentially generate many new tokens,"
"outputs. Therefore,
in common situations where memory",""
"","up to γ + 1, depending on how well Mq approximates Mp."
"bandwidth is the bottleneck, and compute resources are",""
"available, it may be a good default to accelerate sampling",""
"","2.2. Standardized Sampling"
"from autoregressive models like Transformers.",""
"","First, note that while there are many methods and parame-"
"To summarize, our main contributions are: (1) A generaliza-",""
"","ters of sampling,
like argmax,
top-k, nucleus, and setting"
"tion of speculative execution to the stochastic setting, with",""
"","a temperature, and popular implementations usually treat"
"a novel sampling method we call speculative sampling, and",""
"","them differently at the logits level, they can all easily be cast"
"(2) A decoding mechanism we call speculative decoding that",""
"","into standard sampling from an adjusted probability distribu-"
"can accelerate decoding from autoregressive models, with-",""
"","tion. For example, argmax sampling is equivalent to zeroing"
"out any change to the model architectures, training regimes",""
"","out non-max elements of the distribution and normalizing."
"and output distributions.",""
"","We can therefore only deal with standard sampling from a"
"","1We’ll use p(x) to denote p(xt|x<t) whenever the preﬁx x<t"
"","is clear from the context, and similarly for q(x)."
