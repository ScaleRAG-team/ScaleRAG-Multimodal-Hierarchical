"ABSTRACT"
"In this
study, we
introduce
adaptive KV cache
compression,
a plug-and-play"
"method that reduces the memory footprint of generative inference for Large Lan-"
"guage Models (LLMs). Different from the conventional KV cache that retains key"
"and value vectors for all context
tokens, we conduct
targeted profiling to discern"
"the intrinsic structure of attention modules. Based on the recognized structure, we"
"propose FastGen, which constructs the KV cache in an adaptive manner:
evict-"
"ing long-range contexts on attention heads emphasizing local contexts, discarding"
"non-special
tokens on attention heads centered on special
tokens, and only em-"
"ploying the standard KV cache for attention heads that broadly attend to all tokens."
"Moreover, with the lightweight attention profiling used to guide the construction"
"of
the adaptive KV cache, FastGen can be deployed without
resource-intensive"
"fine-tuning or re-training. In our experiments across various asks, FastGen demon-"
"strates substantial reduction on GPU memory consumption with negligible gener-"
"ation quality loss."
