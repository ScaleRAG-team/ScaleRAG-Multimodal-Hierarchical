{
  "title": null,
  "authors": [],
  "source_path": "../data/pdf/2306.14048.pdf",
  "page_count": 49,
  "processed_pages": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49
  ],
  "counts": {
    "texts": 1566,
    "pictures": 56,
    "tables": 79
  },
  "stats_per_page": [
    {
      "page": 1,
      "text_blocks": 9,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 2,
      "text_blocks": 26,
      "layout_blocks": 9,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 9,
      "tables_found": 4
    },
    {
      "page": 3,
      "text_blocks": 9,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 4,
      "text_blocks": 21,
      "layout_blocks": 7,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 7,
      "tables_found": 1
    },
    {
      "page": 5,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 6,
      "text_blocks": 69,
      "layout_blocks": 29,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 29,
      "tables_found": 2
    },
    {
      "page": 7,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 8,
      "text_blocks": 20,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 9,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 10,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 11,
      "text_blocks": 19,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 12,
      "text_blocks": 2,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 13,
      "text_blocks": 18,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 14,
      "text_blocks": 19,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 15,
      "text_blocks": 21,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 16,
      "text_blocks": 2,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 17,
      "text_blocks": 2,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 18,
      "text_blocks": 2,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 19,
      "text_blocks": 7,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 20,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 21,
      "text_blocks": 13,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 22,
      "text_blocks": 5,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 23,
      "text_blocks": 18,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 24,
      "text_blocks": 25,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 25,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 26,
      "text_blocks": 15,
      "layout_blocks": 11,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 11,
      "tables_found": 2
    },
    {
      "page": 27,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 28,
      "text_blocks": 213,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 9
    },
    {
      "page": 29,
      "text_blocks": 239,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 30,
      "text_blocks": 9,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 31,
      "text_blocks": 197,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 32,
      "text_blocks": 10,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 33,
      "text_blocks": 28,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 34,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 35,
      "text_blocks": 57,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 36,
      "text_blocks": 30,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 37,
      "text_blocks": 13,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 38,
      "text_blocks": 26,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 39,
      "text_blocks": 26,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 40,
      "text_blocks": 25,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 41,
      "text_blocks": 31,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 42,
      "text_blocks": 26,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 43,
      "text_blocks": 24,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 44,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 45,
      "text_blocks": 35,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 46,
      "text_blocks": 33,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 47,
      "text_blocks": 31,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 48,
      "text_blocks": 25,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 49,
      "text_blocks": 6,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    }
  ],
  "texts": [
    {
      "page_no": 1,
      "bbox": [
        119.12899780273438,
        99.87686920166016,
        493.3041076660156,
        137.0172576904297
      ],
      "text": "H2O: Heavy-Hitter Oracle for Efficient Generative\nInference of Large Language Models"
    },
    {
      "page_no": 1,
      "bbox": [
        117.83200073242188,
        180.29530334472656,
        493.6693420410156,
        201.87640380859375
      ],
      "text": "Zhenyu Zhang1, Ying Sheng2, Tianyi Zhou3, Tianlong Chen1, Lianmin Zheng4, Ruisi Cai1,\nZhao Song5, Yuandong Tian6, Christopher Ré2, Clark Barrett2, Zhangyang Wang1, Beidi Chen6,7"
    },
    {
      "page_no": 1,
      "bbox": [
        114.38998413085938,
        202.16334533691406,
        497.6106262207031,
        256.6946716308594
      ],
      "text": "1University of Texas at Austin, 2Stanford University, 3University of California, San Diego,\n4University of California, Berkeley, 5Adobe Research, 6Meta AI (FAIR), 7Carnegie Mellon University\n{zhenyu.zhang,tianlong.chen,ruisi.cai,atlaswang}@utexas.edu, ying1123@stanford.edu,\n{chrismre,barrett}@cs.stanford.edu, t8zhou@ucsd.edu, lianminzheng@gmail.com,\nzsong@adobe.com, yuandong@meta.com, beidic@andrew.cmu.edu"
    },
    {
      "page_no": 1,
      "bbox": [
        283.7580261230469,
        286.31121826171875,
        328.2433166503906,
        298.26641845703125
      ],
      "text": "Abstract"
    },
    {
      "page_no": 1,
      "bbox": [
        143.34298706054688,
        313.3666076660156,
        470.7562561035156,
        563.5050659179688
      ],
      "text": "Large Language Models (LLMs), despite their recent impressive accomplishments,\nare notably cost-prohibitive to deploy, particularly for applications involving long-\ncontent generation, such as dialogue systems and story writing. Often, a large\namount of transient state information, referred to as the KV cache, is stored in GPU\nmemory in addition to model parameters, scaling linearly with the sequence length\nand batch size. In this paper, we introduce a novel approach for implementing the\nKV cache which significantly reduces its memory footprint. Our approach is based\non the noteworthy observation that a small portion of tokens contributes most of\nthe value when computing attention scores. We call these tokens Heavy Hitters\n(H2). Through a comprehensive investigation, we find that (i) the emergence of H2\nis natural and strongly correlates with the frequent co-occurrence of tokens in the\ntext, and (ii) removing them results in significant performance degradation. Based\non these insights, we propose Heavy Hitter Oracle (H2O), a KV cache eviction\npolicy that dynamically retains a balance of recent and H2 tokens. We formulate\nthe KV cache eviction as a dynamic submodular problem and prove (under mild\nassumptions) a theoretical guarantee for our novel eviction algorithm which could\nhelp guide future work. We validate the accuracy of our algorithm with OPT,\nLLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of\nH2O with 20% heavy hitters improves the throughput over three leading inference\nsystems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by\nup to 29×, 29×, and 3× on OPT-6.7B and OPT-30B. With the same batch size,\nH2O can reduce the latency by up to 1.9×. The code is available at https:\n//github.com/FMInference/H2O."
    },
    {
      "page_no": 1,
      "bbox": [
        108.0,
        588.2421264648438,
        190.8136749267578,
        600.1973266601562
      ],
      "text": "1\nIntroduction"
    },
    {
      "page_no": 1,
      "bbox": [
        107.64099884033203,
        614.4762573242188,
        505.7439270019531,
        711.7870483398438
      ],
      "text": "Large Language Models (LLMs) have demonstrated remarkable proficiency in a wide range of\nnatural language processing applications such as content creation, summarization, and dialogue\nsystems [1, 2, 3, 4]. However, their deployment is very costly. In addition to the widely-studied\nbottlenecks of model size and the quadratic cost of attention layers, the problem of the size of\nthe KV cache, which stores the intermediate attention key and values during generation to avoid\nre-computation, is becoming increasingly prominent [5]. For instance, a 30 billion-parameter model\nwith an input batch size of 128 and a sequence length of 1024 results in 180GB of KV cache. A\nnatural approach is to limit its maximum size as is done in classical software or hardware caches [6].\nHowever, it is challenging to reduce KV cache memory footprints in LLMs without accuracy drops."
    },
    {
      "page_no": 1,
      "bbox": [
        108.0,
        733.0769653320312,
        385.16937255859375,
        742.0433959960938
      ],
      "text": "37th Conference on Neural Information Processing Systems (NeurIPS 2023)."
    },
    {
      "page_no": 1,
      "bbox": [
        10.940000534057617,
        210.05999755859375,
        37.619998931884766,
        560.0
      ],
      "text": "arXiv:2306.14048v3  [cs.LG]  18 Dec 2023"
    },
    {
      "page_no": 2,
      "bbox": [
        283.96917724609375,
        205.50994873046875,
        290.0378112792969,
        210.88638305664062
      ],
      "text": "0.1"
    },
    {
      "page_no": 2,
      "bbox": [
        283.96917724609375,
        215.913330078125,
        300.4411926269531,
        221.28976440429688
      ],
      "text": "0.1\n0.5"
    },
    {
      "page_no": 2,
      "bbox": [
        151.12026977539062,
        163.4910888671875,
        348.7301940917969,
        179.4327850341797
      ],
      "text": "Children laughed\nand\nin\nthe\nsunny\npark\n...\n.\nplayed"
    },
    {
      "page_no": 2,
      "bbox": [
        283.96917724609375,
        226.3166961669922,
        300.4411926269531,
        231.69313049316406
      ],
      "text": "0.2\n0.1"
    },
    {
      "page_no": 2,
      "bbox": [
        285.789794921875,
        195.10658264160156,
        288.2172546386719,
        200.48301696777344
      ],
      "text": "1"
    },
    {
      "page_no": 2,
      "bbox": [
        294.37255859375,
        205.50994873046875,
        300.4411926269531,
        210.88638305664062
      ],
      "text": "0.9"
    },
    {
      "page_no": 2,
      "bbox": [
        315.1793212890625,
        226.3166961669922,
        321.2479553222656,
        231.69313049316406
      ],
      "text": "0.6"
    },
    {
      "page_no": 2,
      "bbox": [
        162.2497100830078,
        206.20350646972656,
        234.89993286132812,
        211.57994079589844
      ],
      "text": "0.2\n0.1\n0.1\n0.6"
    },
    {
      "page_no": 2,
      "bbox": [
        282.9288330078125,
        182.62254333496094,
        321.9414978027344,
        187.9989776611328
      ],
      "text": "1.4\n0.6\n1.5\n0.5"
    },
    {
      "page_no": 2,
      "bbox": [
        304.77593994140625,
        215.913330078125,
        310.8445739746094,
        221.28976440429688
      ],
      "text": "0.4"
    },
    {
      "page_no": 2,
      "bbox": [
        304.77593994140625,
        226.3166961669922,
        310.8445739746094,
        231.69313049316406
      ],
      "text": "0.1"
    },
    {
      "page_no": 2,
      "bbox": [
        305.1227111816406,
        216.6068878173828,
        311.19134521484375,
        221.9833221435547
      ],
      "text": "0.4"
    },
    {
      "page_no": 2,
      "bbox": [
        204.2966766357422,
        226.3166961669922,
        311.19134521484375,
        233.77381896972656
      ],
      "text": "0.1\nQuery"
    },
    {
      "page_no": 2,
      "bbox": [
        154.2954559326172,
        186.7838897705078,
        162.3837432861328,
        192.1603240966797
      ],
      "text": "Key"
    },
    {
      "page_no": 2,
      "bbox": [
        146.30870056152344,
        178.461181640625,
        157.9022216796875,
        183.83761596679688
      ],
      "text": "Value"
    },
    {
      "page_no": 2,
      "bbox": [
        150.04742431640625,
        68.41197967529297,
        457.5421142578125,
        76.7861557006836
      ],
      "text": "Static Sparsity (Strided)\nStatic Sparsity (Local)\nStatic Sparsity w. \nDynamic Sparsity"
    },
    {
      "page_no": 2,
      "bbox": [
        108.0,
        256.5724792480469,
        503.9993591308594,
        275.84490966796875
      ],
      "text": "Figure 1: Upper plots illustrate symbolic plots of an attention map deploying different KV cache policies in\nLLM generation. Lower right: contrasts their accuracy-memory trade-off. Left: the overview of H2O framework."
    },
    {
      "page_no": 2,
      "bbox": [
        107.25299835205078,
        293.6922912597656,
        505.2452392578125,
        391.0030822753906
      ],
      "text": "While there exists substantial literature on sparse attention approximation in training, they have\nnot seen wide adoption for alleviating KV cache bottleneck. First, most existing methods, e.g.,\nReformer [7] and Flash Attention [8], are designed to overcome the quadratic memory required by\nattention mechanisms when modeling long sequences but still require a large cache size. Second,\nvariants like sparse transformer [9], low-rank based transformers [10, 11] or multi-query attention [12,\n13, 5] can reduce the cache size, but directly applying them on pre-trained LLMs for generation\nresults in high miss rates and degrades the accuracy as shown in Figure 1. Finally, some recent\nadvances such as gisting tokens [14] can learn to compress the KV cache for documents, but their\nexpensive eviction policies are difficult to deploy during generation."
    },
    {
      "page_no": 2,
      "bbox": [
        107.69100189208984,
        397.1760559082031,
        504.665283203125,
        483.75506591796875
      ],
      "text": "Therefore, an ideal KV cache should have (i) a small cache size to reduce memory footprint, (ii) a\nlow miss rate to maintain the performance and long-content generation ability of LLMs, and (iii)\na low-cost eviction policy to reduce the wall-clock time during generation. However, there are\nthree technical challenges. First, it is not immediately clear whether the size of the KV cache can\nbe restricted—each decoding step might, in principle, require access to all previous attention keys\nand values. Second, identifying an optimal eviction policy that maintains generation accuracy is a\ncombinatorial problem1. Finally, even if an optimal policy can be brute-forced, it is infeasible for\ndeployment on real-world applications."
    },
    {
      "page_no": 2,
      "bbox": [
        108.0,
        490.1062927246094,
        503.9974060058594,
        511.0530700683594
      ],
      "text": "Fortunately, our preliminary exploration has yielded intriguing observations about the empirical\nproperties of LLMs. These findings pave the way for the potential design of an efficient KV cache."
    },
    {
      "page_no": 2,
      "bbox": [
        108.0,
        517.302001953125,
        503.99945068359375,
        571.0780639648438
      ],
      "text": "Sparsity for small cache size: We observe that even when trained densely, the attention matrices of\nLLMs are over 95% sparse at inference time (shown in Figure 2). This holds for a wide range of\npre-trained LLMs. Therefore, only 5% of the KV cache is sufficient for decoding the same output\ntoken at each generation step, which suggests it may be possible to have up to a 20× reduction in\nKV cache size without an accuracy drop."
    },
    {
      "page_no": 2,
      "bbox": [
        107.99999237060547,
        577.3270263671875,
        504.00201416015625,
        631.10302734375
      ],
      "text": "Heavy-Hitters for low miss rate: We discover that the accumulated attention scores of all tokens\nin attention blocks adhere to a power-law distribution. It suggests that there exists a small set of\ninfluential tokens that are critical during generation, named heavy-hitters (H2). H2 provides an\nopportunity to step away from the combinatorial search problem and identify an eviction policy that\nmaintains accuracy."
    },
    {
      "page_no": 2,
      "bbox": [
        108.0,
        637.2979125976562,
        504.0020751953125,
        669.3090209960938
      ],
      "text": "Greedy algorithm for low-cost policy: we surprisingly find that retaining the H2 based on local\nstatistics at each decoding step—summing the attention scores of only the preceding tokens—is as\neffective as considering the attention of future tokens (shown in Figure 2)."
    },
    {
      "page_no": 2,
      "bbox": [
        108.0,
        675.6602783203125,
        504.2476806640625,
        697.217529296875
      ],
      "text": "Based on the above, we first rigorously define the generative process of LLMs operating with a\nsize-constrained KV cache in Section 2.1. Then we propose Heavy-Hitter Oracle (H2O), a framework"
    },
    {
      "page_no": 2,
      "bbox": [
        120.65299987792969,
        711.6576538085938,
        425.3856201171875,
        722.1663818359375
      ],
      "text": "1Belady’s Algorithm is optimal for standard cache, but not necessarily for KV cache."
    },
    {
      "page_no": 2,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "2"
    },
    {
      "page_no": 3,
      "bbox": [
        108.0,
        74.4834213256836,
        504.3529968261719,
        95.27908325195312
      ],
      "text": "that exploits the properties of LLMs and uses simple, low-cost eviction policies that retrain the quality\nof LLMs throughout the generation process. Specifically,"
    },
    {
      "page_no": 3,
      "bbox": [
        108.0,
        101.47392272949219,
        505.73992919921875,
        202.73110961914062
      ],
      "text": "• In Section 3, we explore the emergence of H2 in attention, revealing their fundamental and critical\nroles: (i) H2 exhibit a strong correlation of frequently co-occurring words in textual data; and (ii)\nremoving H2 completely damages the model’s functionality. We demonstrate that H2 can largely\nlower the cache miss rate of the existing policies mentioned above. Theoretically, assuming the\nattention scheme is submodular, H2 corresponds to a greedy algorithm and is therefore near-optimal.\n• In Section 4, we present a greedy but low-cost variant of H2 which is dynamically determined\nby the accumulated attention score at each decoding step. We formulate the eviction policy with\ngreedy H2 as a variant of dynamic submodular maximization. The analysis shows that it results in\na similar generative process as the one using the H2 eviction policy."
    },
    {
      "page_no": 3,
      "bbox": [
        107.53199768066406,
        207.58731079101562,
        505.7433776855469,
        290.2370910644531
      ],
      "text": "We perform extensive experiments on OPT, LLaMA, and GPT-NeoX on a single NVIDIA A100\n(80GB) GPU to evaluate H2O across a range of tasks from lm-eval-harness [15] and HELM [16].\nWe implement H2O on top of FlexGen that can easily adapt different cache eviction techniques to\nproduce a system with high-throughput inference. Performance experiments show our framework\nachieves 29×, 29×, 3× higher throughputs compared to three leading inference systems, DeepSpeed\nZero-Inference [17], Hugging Face Accelerate [18], and FlexGen [19] respectively. With the same\nbatch size, H2O achieves up to 1.9× lower latency compare to FlexGen."
    },
    {
      "page_no": 3,
      "bbox": [
        107.99998474121094,
        300.35015869140625,
        304.8064880371094,
        312.30535888671875
      ],
      "text": "2\nRelated Work and Problem Setting"
    },
    {
      "page_no": 3,
      "bbox": [
        107.64099884033203,
        325.2962951660156,
        505.743408203125,
        455.32305908203125
      ],
      "text": "Efficient Inference of LLMs.\nThe substantial parameter counts of large language models (LLMs)\npresent significant challenges for inference. To overcome this limitation, previous efforts have\nemployed model compression techniques with specific designs to achieve efficient LLM inference,\nsuch as the method described in [20, 21, 22], which employs one-shot pruning on LLMs, resulting in\nnegligible performance degradation even without retraining. Additionally, alternative approaches\nexplore quantization methods specifically tailored to LLMs, as discussed in [23, 24, 25, 26, 27, 28].\nAlso, CoLT5 [29] employs a token-wise conditional computation strategy to reduce the overall\ncomputation cost. These methods address efficient inference from orthogonal perspectives and can be\norganically integrated. The techniques investigated in this study are closely associated with pruning or\nsparsity but focus on a distinct inference bottleneck, namely, KV cache. One closely related work[30]\nutilizes a learnable mechanism that determines necessary tokens during inference but requires an\nextra fine-tuning process, which makes it less practical."
    },
    {
      "page_no": 3,
      "bbox": [
        107.69100189208984,
        468.8655090332031,
        505.65386962890625,
        577.0780639648438
      ],
      "text": "Sparse, Low-rank Attention Approx.\nThe quadratic computational complexity of attention mod-\nules is one of the major bottlenecks of transformer inference [31]. Various efforts are devoted to\naddressing this challenge [7, 9, 10]. For example, Reformer [7] reduces the computational cost\nfrom quadratic to superlinear complexity via locality-sensitive hashing. Performer [10] employs\npositive orthogonal random features to approximate attention kernels. One relevant work, Sparse\nTransformer [9], introduces sparsity to reduce KV cache memory footprint and achieve an efficient\nattention mechanism, considered as our baseline in this paper. Moreover, SpAtten [32] utilizes\naccumulated attention scores to select important tokens for efficient attention inference while they\ndon’t consider the variance of token importance across attention heads and layers. Comparison with\nSpAtten is detailed in Appendix C.9."
    },
    {
      "page_no": 3,
      "bbox": [
        108.0,
        590.6214599609375,
        505.7432556152344,
        633.3548583984375
      ],
      "text": "Caching.\nCaching, which plays a pivotal role in optimizing system performance, entails the devel-\nopment of effective eviction policies to handle frequently accessed data. Conventional approaches\nsuch as Least Recently Used and Least Frequently Used [33, 34] prioritize the recency and frequency\nof data access. And the design of KV cache encounters many similar challenges as traditional caching."
    },
    {
      "page_no": 3,
      "bbox": [
        107.6709976196289,
        646.9714965820312,
        505.388916015625,
        722.4070434570312
      ],
      "text": "LLM Inference Breakdown.\nThe generative procedure of LLMs encompasses two distinct phases:\n(i) the prompt phase, in which an input sequence is utilized to produce the KV cache (consisting\nof the key and value embeddings), similar to the forward pass employed during LLM training; and\n(ii) the token generation phase, which leverages and updates the KV cache to generate new tokens\nincrementally. Each generation step relies on the previously generated tokens. The primary focus of\nthis paper is to enhance the efficiency of the KV cache in attention during the token generation phase,\nthereby accelerating LLM inference."
    },
    {
      "page_no": 3,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "3"
    },
    {
      "page_no": 4,
      "bbox": [
        157.45648193359375,
        145.57223510742188,
        461.5709533691406,
        153.70138549804688
      ],
      "text": "(a)\n(b)\n(c)\n(d)"
    },
    {
      "page_no": 4,
      "bbox": [
        489.84429931640625,
        98.52306365966797,
        502.8641662597656,
        103.94248962402344
      ],
      "text": "COPA"
    },
    {
      "page_no": 4,
      "bbox": [
        412.936279296875,
        66.56456756591797,
        489.1192932128906,
        74.21641540527344
      ],
      "text": "MathQA\nOpenBookQA"
    },
    {
      "page_no": 4,
      "bbox": [
        412.936279296875,
        98.52306365966797,
        424.05908203125,
        103.94248962402344
      ],
      "text": "PiQA"
    },
    {
      "page_no": 4,
      "bbox": [
        432.215576171875,
        126.83922576904297,
        498.610595703125,
        134.2560272216797
      ],
      "text": "RTE\nWinogrande"
    },
    {
      "page_no": 4,
      "bbox": [
        107.677001953125,
        160.78060913085938,
        505.5729064941406,
        219.54638671875
      ],
      "text": "Figure 2: (a) Attention Sparsity in pre-trained LLMs. (b) The distribution of accumulated attention scores with\nrespect to the corresponding word (red scatter) and the co-occurrence times of words in the data (gray curve).\nThe x-axis represents the word index in the vocabulary. (c) The performance comparison between the baseline\nmodel with full KV and the model w.o. heavy hitter. (d) Comparison between the baseline model with full KV,\nH2O with the local statistic, H2O with the global statistic, and the model with only the most recent KV (Local).\nApart from the baseline model, each model is evaluated with 20% KV cache budget."
    },
    {
      "page_no": 4,
      "bbox": [
        108.00003051757812,
        234.60150146484375,
        222.689453125,
        244.56410217285156
      ],
      "text": "2.1\nProblem Formulation"
    },
    {
      "page_no": 4,
      "bbox": [
        107.53199768066406,
        254.4369354248047,
        504.66790771484375,
        342.94708251953125
      ],
      "text": "We formally define the generative process with limited KV cache size. Denote attention query matrix\nas Q ∈Rn×d and key matrix as K ∈Rn×d. Qi,∗represents the i-th row of Q and K≤i,∗represents\nthe first i rows of K. Let k denote the budget of space and k < n. For simplicity, KSi,∗(∈Ri×d)\ndenotes a sub-matrix of K which selects Si rows from K. (For the non-selected rows [i]\\Si, we put\nall zeros in that row) Eviction policy is defined as:\nDefinition 2.1 (Eviction Policy, informal). Let Si−1 denote the source set. Let Si denote the target\nset. We defined the eviction policy g : Si−1 →Si such that"
    },
    {
      "page_no": 4,
      "bbox": [
        108.00004577636719,
        340.4909362792969,
        334.36676025390625,
        357.8410949707031
      ],
      "text": "• |Si| = k (KV cache size is not changing over the time)"
    },
    {
      "page_no": 4,
      "bbox": [
        108.00003814697266,
        356.97894287109375,
        501.1585693359375,
        375.8240966796875
      ],
      "text": "• |Si\\Si−1| ≤1 or equivalently |Si ∩Si−1| ≥k −1 (we can evict at most 1 KV in the KV cache)"
    },
    {
      "page_no": 4,
      "bbox": [
        107.6910400390625,
        376.4134826660156,
        503.9975891113281,
        418.16607666015625
      ],
      "text": "Then, we define the generative process with our eviction policy.\nDefinition 2.2 (The generative process with eviction policy, informal). Let k denote the size of the\nKV cache. For each i ∈[n], for the i-th token, we have"
    },
    {
      "page_no": 4,
      "bbox": [
        107.99998474121094,
        417.2049255371094,
        414.0530700683594,
        436.049072265625
      ],
      "text": "• Let Si ⊂[n] denote the tokens in KV cache when predicting the i-th token."
    },
    {
      "page_no": 4,
      "bbox": [
        108.0,
        433.09747314453125,
        504.6717834472656,
        452.3770751953125
      ],
      "text": "• The information we have is a length-i vector oi := D−1\ni\n·exp(Qi,∗(KSi,∗)⊤) (normalized attention)"
    },
    {
      "page_no": 4,
      "bbox": [
        126.43099975585938,
        453.2284851074219,
        505.6210632324219,
        489.2464904785156
      ],
      "text": "– scalar Di := (exp(Qi,∗(KSi,∗)⊤) −1[i]\\Si) · 1i (the evicted KV is set to 0, and we need to\nsubtract them when computing the normalization)\n– Replacing Si by [i] in the above definition of oi and Di leads to standard generative process."
    },
    {
      "page_no": 4,
      "bbox": [
        108.0,
        495.95379638671875,
        505.7478332519531,
        530.4755859375
      ],
      "text": "• The eviction policy (Definition 2.1) updates Si based on Si−1 and their corresponding information.\nRemark 2.3. Our goal is to find a KV cache eviction policy such that the output of the generative\nprocess is similar or comparable to the original one without limiting the cache size."
    },
    {
      "page_no": 4,
      "bbox": [
        107.99996948242188,
        544.0821533203125,
        193.44378662109375,
        556.037353515625
      ],
      "text": "3\nObservations"
    },
    {
      "page_no": 4,
      "bbox": [
        107.53196716308594,
        562.388916015625,
        467.5711669921875,
        573.1935424804688
      ],
      "text": "We present two key empirical insights of LLMs that inspire the design of H2O, as follows."
    },
    {
      "page_no": 4,
      "bbox": [
        107.99996948242188,
        580.79150390625,
        256.35302734375,
        590.7540893554688
      ],
      "text": "3.1\nSparsity for Small Cache Size"
    },
    {
      "page_no": 4,
      "bbox": [
        108.0,
        600.857421875,
        505.1664733886719,
        665.3660278320312
      ],
      "text": "Inspired by previous literature, which reveals the existence of attention sparsity in DistillBERT [35]\nand bounded-norm self-attention heads [36]. We first show an observation on the sparsity of attention\nin pre-trained LLMs. Then we discuss how it can potentially unlock the possibility of reducing\nKV cache size without an accuracy drop. Given the normalized attention score Softmax(QK⊤)\nmatrix that is calculated by the query matrix Q and the key matrix K, we set the threshold as one\npercent of the maximum value in each row and calculates the corresponding sparsity."
    },
    {
      "page_no": 4,
      "bbox": [
        108.0,
        672.164794921875,
        504.35333251953125,
        726.002685546875
      ],
      "text": "Observation.\nWe conduct zero-shot inference with the pre-trained OPT model on the validation\nset of Wiki-Text-103. We plot the layer-wise sparsity within attention blocks and visualize the\nnormalized attention score matrix. The results are presented in Figure 2 (a). We observe that although\nthe LLMs are densely trained, the resulting attention score matrices are highly sparse, with a sparsity\nover 95% in almost all layers."
    },
    {
      "page_no": 4,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "4"
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        74.28599548339844,
        505.6547546386719,
        106.18807983398438
      ],
      "text": "Insights.\nThe attention blocks’ sparsity suggests that access to all previous key and value embed-\ndings is unnecessary for generating the next token. This suggests it is possible to evict unessential\nKV embeddings and reduce the requirement of KV cache during generation."
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        120.30553436279297,
        271.4762878417969,
        130.2681427001953
      ],
      "text": "3.2\nHeavy-Hitters for Low Miss Rate"
    },
    {
      "page_no": 5,
      "bbox": [
        107.69100189208984,
        140.29629516601562,
        505.7405090332031,
        204.8548583984375
      ],
      "text": "The previous section showed the sparse nature of attention blocks in pre-trained LLMs, which\nprovides the opportunity for designing small KV cache size while still maintaining the performance\nof LLMs. However, determining the best eviction policy that preserves generation accuracy presents\na combinatorial challenge. Although Belady’s Algorithm [37] is optimal and easy to compute for\nstandard cache (offline), it is not applicable for KV cache design. Because once evicting important\nKVs, it could destroy the performance of LLMs due to the sequential dependency of LLM generation."
    },
    {
      "page_no": 5,
      "bbox": [
        107.53199768066406,
        211.9064483642578,
        505.74530029296875,
        298.3990783691406
      ],
      "text": "Observation.\nFortunately, in the early stage of our exploration, we find that the accumulated\nattention scores of all the tokens within attention blocks follow a power-law distribution, as shown\nin Figure 2. This suggests the existence of a small set of tokens that are critical during generation.\nWe denote those tokens as heavy-hitters (H2). In order to verify the importance of these tokens, we\ncompare the quality of LLM generation after masking heavy hitters with that of the original model.\nNot surprisingly, as shown in Figure 2, the accuracy drops drastically, confirming the importance of\nthose tokens. Additionally, we can see the accumulated attention score of each word (in red dots)\nhave a high correlation with their co-occurrences in the data (gray curve)."
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        305.3629150390625,
        505.65380859375,
        391.9200744628906
      ],
      "text": "Analysis.\nFirst, based on H2, we see an opportunity to side-step from the combinatorial search\nproblem and design a KV cache eviction policy that preserves the LLM generation quality. We\nconduct an empirical study implementing a KV cache eviction policy that retains only the H2 and\nthe recent KV embeddings in the cache. The intuition is that recent words typically exhibit stronger\ncorrelations with current tokens. We assess the effectiveness of this eviction policy through pre-\ntrained OPT-30B and six downstream tasks. The outcomes of these evaluations are illustrated in\nFigure 2. It is obvious that the H2 based eviction policy can largely reduce the KV cache size without\ndegrading the performance of OPT-30B."
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        398.11492919921875,
        504.003662109375,
        456.60858154296875
      ],
      "text": "Moreover, during the post analysis, inspired by [38], we find that H2 based policy is related to\nthe classical greedy algorithm (a polynomial-time algorithm with provable guarantees) under the\nassumption that the attention schema is submodular. We present details in Appendix D.\nLemma 3.1 (informal). Assuming the attention scheme is submodular, then greedily constructing the\nset Si (without cache size limitation) satisfies the near-optimal property in terms of submodular."
    },
    {
      "page_no": 5,
      "bbox": [
        108.00000762939453,
        469.49114990234375,
        230.8755340576172,
        481.44635009765625
      ],
      "text": "4\nHeavy-Hitter Oracle"
    },
    {
      "page_no": 5,
      "bbox": [
        107.69100189208984,
        488.03692626953125,
        505.2473449707031,
        552.7760620117188
      ],
      "text": "The goal of this section is to propose the greedy algorithm using the H2-based policy and to show\nthe provable guarantees. We first present the H2-based policy called H2O cache eviction policy and\nformulate its deployment in LLM generation as a variant of submodular maximization problem,\nnamed dynamic submodular. Then we present H2O in the generative process, followed by a practical\nexample of deploying our proposal. Finally, we provide theoretical guarantees for H2O and show our\nefficient system implementation."
    },
    {
      "page_no": 5,
      "bbox": [
        107.53199768066406,
        566.8935546875,
        505.2474060058594,
        645.7980346679688
      ],
      "text": "4.1\nGreedy Algorithm for Low-Cost Policy\nWe have shown a simple yet effective KV cache policy based on H2. However, it is impractical to\ndeploy such an algorithm because we do not have access to the future-generated tokens. Fortunately,\nwe empirically observe that local H2, which is calculated using local statistics at every decoding step\nby summing up the attention scores of the previous tokens, is equally effective as taking into account\nthe attention of future tokens (Figure 2). In the following, we formally define this dynamic attention\nscore computation (with space limitation) as a novel dynamic submodular type problem."
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        647.3999633789062,
        505.747314453125,
        680.299072265625
      ],
      "text": "Definition 4.1 (Dynamic submodular framework, informal). Define function F : 2[n] × 2[n] →R,\nthen for any set Z ⊂[n], we assume that F(Z, ·) : 2[n] →R is a submodular function w.r.t. to Z, i.e.,"
    },
    {
      "page_no": 5,
      "bbox": [
        108.0,
        679.3909912109375,
        310.441650390625,
        696.6870727539062
      ],
      "text": "• For all sets X, Y ⊂[n] satisfy that Z ⊂X ⊂Y ,"
    },
    {
      "page_no": 5,
      "bbox": [
        107.99998474121094,
        695.8790283203125,
        301.41162109375,
        713.1751098632812
      ],
      "text": "• For all element x ∈[n] satisfy that x ∈[n]\\Y ,"
    },
    {
      "page_no": 5,
      "bbox": [
        107.99995422363281,
        712.2680053710938,
        419.2395935058594,
        729.5640869140625
      ],
      "text": "we have f(X ∪{x}) −f(X) ≥f(Y ∪{x}) −f(Y ), where f(·) := F(Z, ·)."
    },
    {
      "page_no": 5,
      "bbox": [
        303.5089416503906,
        742.3324584960938,
        308.490234375,
        752.2950439453125
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        74.15408325195312,
        505.6723937988281,
        106.27165222167969
      ],
      "text": "Remark 4.2. We provide practical insights of Definition 4.1. X denotes the existing words in the\nKV cache. Y is any superset of X. x can be viewed as a “word” which is either newly added to KV\ncache or existing deleted from KV cache. An example f can be attention score, i.e., see Algorithm 1."
    },
    {
      "page_no": 6,
      "bbox": [
        107.64099884033203,
        112.36007690429688,
        503.99945068359375,
        144.2185821533203
      ],
      "text": "If we load the sequence of S1, S2, · · · , Sn (we promise that |Si| ≤k and |Si\\Si−1| ≤1) into\nDefinition 4.1, i.e., for each i ∈[n], we choose Z = Si, then it becomes a particular instance of the\ndynamic submodular problem."
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        156.60743713378906,
        430.2004089355469,
        166.57003784179688
      ],
      "text": "Next, we provide a formal description of our algorithm, followed by an example."
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        170.61297607421875,
        504.3000793457031,
        203.9170684814453
      ],
      "text": "Definition 4.3 (H2O Eviction Policy). Let Fscore : 2[n] →R denote certain score function. Let Si−1\ndenote the source set. Let Si denote the target set. We defined the eviction policy g : Si−1 →Si s.t."
    },
    {
      "page_no": 6,
      "bbox": [
        108.00006103515625,
        201.4608612060547,
        334.3667907714844,
        218.81105041503906
      ],
      "text": "• |Si| = k (KV cache size is not changing over the time)"
    },
    {
      "page_no": 6,
      "bbox": [
        108.00005340576172,
        217.94883728027344,
        501.1585693359375,
        236.7940216064453
      ],
      "text": "• |Si\\Si−1| ≤1 or equivalently |Si ∩Si−1| ≥k −1 (we can evict at most 1 KV in the KV cache)"
    },
    {
      "page_no": 6,
      "bbox": [
        108.00003051757812,
        234.49090576171875,
        484.6396179199219,
        253.28199768066406
      ],
      "text": "• We construct Si ←(Si−1 ∪{i})\\{u} as u ←arg maxv∈(Si−1∪{i}) Fscore(Si−1 ∪{i}\\{v}}"
    },
    {
      "page_no": 6,
      "bbox": [
        107.6910400390625,
        256.8433837890625,
        505.2457275390625,
        277.7149963378906
      ],
      "text": "To describe our algorithm (Algorithm 1), we choose a particular instantiation of the function Fscore,\ni.e., the summation of that sets in the attention matrix."
    },
    {
      "page_no": 6,
      "bbox": [
        107.64099884033203,
        308.5359191894531,
        253.3960418701172,
        320.22406005859375
      ],
      "text": "Algorithm 1 H2 Eviction Algorithm"
    },
    {
      "page_no": 6,
      "bbox": [
        107.99998474121094,
        323.74920654296875,
        315.1123046875,
        415.1253356933594
      ],
      "text": "1: procedure H2_EVICTION(Q, K ∈Rn×d, k ∈N)\n2:\nLet k denote the budget size of cache\n3:\nS0 ←∅\n4:\nfor i = 1 →n do\n5:\nif i ≤k then\n6:\nSi ←Si−1 ∪{i}\n7:\nelse\n8:\nDi ←(exp(Qi,∗(KSi−1,∗)⊤) −1[i]\\Si−1) · 1i\n9:\noi ←D−1\ni\n· (exp(Qi,∗(KSi−1,∗)⊤) −1[i]\\Si−1)\n10:\nFscore(T ) := P"
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        403.4477233886719,
        281.432373046875,
        435.08233642578125
      ],
      "text": "s∈T os\n11:\nGi ←Si−1 ∪{i}\n12:\nu ←arg max\nv∈Gi\nFscore(Si−1 ∪{i}\\{v}}"
    },
    {
      "page_no": 6,
      "bbox": [
        108.00001525878906,
        435.2650451660156,
        234.8496856689453,
        468.1424560546875
      ],
      "text": "13:\nSi ←(Si−1 ∪{i})\\{u}\n14:\nend if\n15:\nend for\n16: end procedure"
    },
    {
      "page_no": 6,
      "bbox": [
        438.2113342285156,
        366.61529541015625,
        442.51800537109375,
        369.713623046875
      ],
      "text": "0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        438.2113342285156,
        373.2545471191406,
        449.15728759765625,
        376.3528747558594
      ],
      "text": "0.1 0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        361.17510986328125,
        339.866943359375,
        489.0570373535156,
        349.67022705078125
      ],
      "text": "Children laughed\nand\nthe\nsunny\npark\n...\n.\nplayed"
    },
    {
      "page_no": 6,
      "bbox": [
        438.2113342285156,
        379.8938293457031,
        455.7965393066406,
        382.9921569824219
      ],
      "text": "0.2 0.1 0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        439.5046081542969,
        359.9760437011719,
        441.2272644042969,
        363.0743713378906
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        444.8506164550781,
        366.61529541015625,
        449.15728759765625,
        369.713623046875
      ],
      "text": "0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        451.4898681640625,
        373.2545471191406,
        455.7965393066406,
        376.3528747558594
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        437.353759765625,
        386.5330810546875,
        469.0750732421875,
        389.63140869140625
      ],
      "text": "0.03 0.02 0.2 0.05 0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        458.129150390625,
        379.8938293457031,
        462.435791015625,
        382.9921569824219
      ],
      "text": "0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        418.9367370605469,
        344.99444580078125,
        421.5207214355469,
        348.42559814453125
      ],
      "text": "in"
    },
    {
      "page_no": 6,
      "bbox": [
        436.994140625,
        351.63372802734375,
        469.30523681640625,
        355.06488037109375
      ],
      "text": "1.43\n0.65\n1.52\n0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        459.0143737792969,
        379.9612731933594,
        462.8872985839844,
        383.3924255371094
      ],
      "text": "0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        367.9457702636719,
        371.551513671875,
        427.9205017089844,
        374.982666015625
      ],
      "text": "0.03\n0.02\n0.05\n0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        444.6293029785156,
        424.6656494140625,
        448.5022277832031,
        428.0968017578125
      ],
      "text": "0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        444.6293029785156,
        431.3049011230469,
        455.1414794921875,
        434.7360534667969
      ],
      "text": "0.1\n0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        444.6293029785156,
        437.9441833496094,
        461.7807312011719,
        441.3753356933594
      ],
      "text": "0.2\n0.1\n0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        445.7911682128906,
        418.0263977050781,
        447.34033203125,
        421.4575500488281
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        451.2685546875,
        424.6656494140625,
        455.1414794921875,
        428.0968017578125
      ],
      "text": "0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        457.9078369140625,
        431.3049011230469,
        461.7807312011719,
        434.7360534667969
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        443.854736328125,
        444.5834655761719,
        475.05926513671875,
        448.0146179199219
      ],
      "text": "0.03 0.04 0.01 0.02 0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        464.547119140625,
        437.9441833496094,
        468.4200134277344,
        441.3753356933594
      ],
      "text": "0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        443.96539306640625,
        410.05926513671875,
        475.50189208984375,
        413.49041748046875
      ],
      "text": "1.6\n0.62\n1.8\n0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        457.35455322265625,
        444.5834655761719,
        462.7766418457031,
        448.0146179199219
      ],
      "text": "0.01"
    },
    {
      "page_no": 6,
      "bbox": [
        374.1424255371094,
        429.9770812988281,
        434.11712646484375,
        433.4082336425781
      ],
      "text": "0.03\n0.04\n0.02\n0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        402.9125671386719,
        429.9770812988281,
        408.33465576171875,
        433.4082336425781
      ],
      "text": "0.01"
    },
    {
      "page_no": 6,
      "bbox": [
        456.6906433105469,
        410.05926513671875,
        462.1127014160156,
        413.49041748046875
      ],
      "text": "0.51"
    },
    {
      "page_no": 6,
      "bbox": [
        457.6865234375,
        431.3049011230469,
        461.5594177246094,
        434.7360534667969
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        458.129150390625,
        437.9441833496094,
        462.0020446777344,
        441.3753356933594
      ],
      "text": "0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        447.2850036621094,
        314.8964538574219,
        451.1579284667969,
        318.3276062011719
      ],
      "text": "0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        447.2850036621094,
        321.53570556640625,
        457.79718017578125,
        324.96685791015625
      ],
      "text": "0.1\n0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        362.9455871582031,
        287.6380615234375,
        489.0570373535156,
        297.8117980957031
      ],
      "text": "Children laughed\nand\nin\nthe\nsunny\npark\n...\n.\nplayed"
    },
    {
      "page_no": 6,
      "bbox": [
        447.2850036621094,
        328.17498779296875,
        457.79718017578125,
        331.60614013671875
      ],
      "text": "0.2\n0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        448.4468994140625,
        308.2571716308594,
        449.9960632324219,
        311.6883239746094
      ],
      "text": "1"
    },
    {
      "page_no": 6,
      "bbox": [
        453.9242858886719,
        314.8964538574219,
        457.79718017578125,
        318.3276062011719
      ],
      "text": "0.9"
    },
    {
      "page_no": 6,
      "bbox": [
        467.20281982421875,
        328.17498779296875,
        471.0757141113281,
        331.60614013671875
      ],
      "text": "0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        369.6055908203125,
        315.3390808105469,
        416.41241455078125,
        318.7702331542969
      ],
      "text": "0.2\n0.1\n0.1\n0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        447.0636901855469,
        299.8474426269531,
        471.5183410644531,
        303.2785949707031
      ],
      "text": "1.4\n0.6\n1.5\n0.5"
    },
    {
      "page_no": 6,
      "bbox": [
        460.56353759765625,
        321.53570556640625,
        464.4364318847656,
        324.96685791015625
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        460.56353759765625,
        328.17498779296875,
        464.4364318847656,
        331.60614013671875
      ],
      "text": "0.1"
    },
    {
      "page_no": 6,
      "bbox": [
        461.22747802734375,
        321.53570556640625,
        465.1003723144531,
        324.96685791015625
      ],
      "text": "0.4"
    },
    {
      "page_no": 6,
      "bbox": [
        396.4393005371094,
        328.17498779296875,
        465.1003723144531,
        332.9339904785156
      ],
      "text": "0.1\nQuery"
    },
    {
      "page_no": 6,
      "bbox": [
        364.5293273925781,
        302.9457702636719,
        369.6911315917969,
        306.3769226074219
      ],
      "text": "Key"
    },
    {
      "page_no": 6,
      "bbox": [
        359.43231201171875,
        297.6343688964844,
        366.83111572265625,
        301.0655212402344
      ],
      "text": "Value"
    },
    {
      "page_no": 6,
      "bbox": [
        357.66876220703125,
        327.9733581542969,
        394.2590637207031,
        333.8553161621094
      ],
      "text": "Decoding Step 4"
    },
    {
      "page_no": 6,
      "bbox": [
        358.1044616699219,
        351.63372802734375,
        365.50323486328125,
        355.06488037109375
      ],
      "text": "Value"
    },
    {
      "page_no": 6,
      "bbox": [
        363.20147705078125,
        357.38775634765625,
        368.36328125,
        360.81890869140625
      ],
      "text": "Key"
    },
    {
      "page_no": 6,
      "bbox": [
        358.5539855957031,
        385.9562683105469,
        463.661865234375,
        391.8382263183594
      ],
      "text": "0.03\nQuery\nDecoding Step 5"
    },
    {
      "page_no": 6,
      "bbox": [
        464.7684020996094,
        437.9441833496094,
        468.6413269042969,
        441.3753356933594
      ],
      "text": "0.6"
    },
    {
      "page_no": 6,
      "bbox": [
        368.6373596191406,
        441.62969970703125,
        469.4158935546875,
        448.0146179199219
      ],
      "text": "0.02\nEviction w. Global Statistic"
    },
    {
      "page_no": 6,
      "bbox": [
        383.8869323730469,
        446.9411315917969,
        405.88641357421875,
        451.8427734375
      ],
      "text": "(infeasible)"
    },
    {
      "page_no": 6,
      "bbox": [
        339.2139892578125,
        468.5674743652344,
        509.4931335449219,
        487.4593811035156
      ],
      "text": "Figure 3: Illustration of Algorithm 1 during two\nconsecutive decoding steps."
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        493.8899230957031,
        505.2425537109375,
        536.81005859375
      ],
      "text": "Figure 3 presents an illustrative example of our H2 Eviction Algorithm. We assume that the budget\nsize of KV cache is 3. Following the completion of the fourth decoding step, the KV embeddings\nassociated with the third token are evicted based on the accumulated attention score. Consequently,\nthese evicted KV embeddings become inaccessible in the subsequent decoding steps."
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        555.2705078125,
        348.84588623046875,
        565.2330932617188
      ],
      "text": "4.2\nTheoretical Guarantee and System Implementation"
    },
    {
      "page_no": 6,
      "bbox": [
        107.53199768066406,
        576.95849609375,
        494.2901306152344,
        586.9210815429688
      ],
      "text": "We state a theoretical result as follows. The proofs and more details are provided in Appendix D."
    },
    {
      "page_no": 6,
      "bbox": [
        107.6709976196289,
        593.4857788085938,
        505.74346923828125,
        662.8945922851562
      ],
      "text": "Theorem 4.4 (informal). Under the mild assumption, let k denote the budget of space limitation. If\nfor each token, we greedily compute the attention score based on top-k choice, then we can show\nthe set eSi we generate each for token i satisfy that f(eSi) ≥(1 −α)(1 −1/e) max|S|=k f(S) −β,\nwhere α, β > 0 are parameters.\nRemark 4.5. We remark the above theorem provides a theoretical explanation of why can we hope\nour greedy algorithm (with cache limitation) can provide a good solution to the problem."
    },
    {
      "page_no": 6,
      "bbox": [
        108.0,
        680.1649169921875,
        505.6571960449219,
        723.0850219726562
      ],
      "text": "Implementation Details.\nWe provide a general framework that can support any KV cache eviction\nalgorithm and enhance throughput and reduce the latency of LLM generation with careful implemen-\ntation. For example, to ensure I/O efficiency, we do not swap memory when stored KV is evicted, but\ndirectly fill with newly-added KV. More details are included in Appendix A."
    },
    {
      "page_no": 6,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "6"
    },
    {
      "page_no": 7,
      "bbox": [
        139.11520385742188,
        247.97189331054688,
        476.4222717285156,
        254.32810974121094
      ],
      "text": "COPA, OPT-30B\nMathQA, OPT-30B\nRTE, OPT-66B"
    },
    {
      "page_no": 7,
      "bbox": [
        432.3846435546875,
        161.590087890625,
        490.73388671875,
        167.94630432128906
      ],
      "text": "OpenBookQA, OPT-66B"
    },
    {
      "page_no": 7,
      "bbox": [
        138.8013153076172,
        72.62545776367188,
        487.95928955078125,
        78.98168182373047
      ],
      "text": "XSUM, LLaMA-7B\nXSUM, LLaMA-13B\nXSUM, LLaMA-30B\nXSUM, GPT-NeoX-20B"
    },
    {
      "page_no": 7,
      "bbox": [
        124.85575103759766,
        161.590087890625,
        390.57696533203125,
        167.94630432128906
      ],
      "text": "CNN/Daily Mail, LLaMA-7B\nCNN/Daily Mail, GPT-NeoX-20B\nOpenBookQA, OPT-30B"
    },
    {
      "page_no": 7,
      "bbox": [
        244.15087890625,
        247.97189331054688,
        284.6498718261719,
        254.32810974121094
      ],
      "text": "COPA, OPT-66B"
    },
    {
      "page_no": 7,
      "bbox": [
        108.0,
        342.0484619140625,
        504.3138732910156,
        361.1853942871094
      ],
      "text": "Figure 4: Comparsion results between the baseline model with full cache, our H2O, and the \"Local\" strategy\nthat utilizes the most recent KV embeddings."
    },
    {
      "page_no": 7,
      "bbox": [
        107.99998474121094,
        371.57818603515625,
        235.75326538085938,
        383.53338623046875
      ],
      "text": "5\nEmpirical Evaluation"
    },
    {
      "page_no": 7,
      "bbox": [
        108.0,
        390.5439147949219,
        504.0042419433594,
        422.5560607910156
      ],
      "text": "In this section, our goal is to demonstrate that H2O, a remarkably simple KV cache eviction policy is\ncapable of enhancing end-to-end throughput and reducing latency in wall-clock while maintaining\ngeneration quality across a broad spectrum of domains and tasks."
    },
    {
      "page_no": 7,
      "bbox": [
        108.0,
        428.75091552734375,
        505.24566650390625,
        541.5070190429688
      ],
      "text": "• In Section 5.1, we show that H2O can reduce the memory footprint of KV cache by up to 5× without\naccuracy degradation on a wide range of model architectures (OPT, LLaMA, GPT-NeoX), sizes\n(from 6.7B to 175B) and evaluation benchmarks (HELM and lm-eval-harness). More importantly,\ncan enhance the performance of existing KV cache sparsification techniques.\n• In Section 5.2, we demonstrate that H2O can increase the inference throughput by up to 3×, 29×,\n29× compared to the state-of-the-art inference engine FlexGen, DeepSpeed and the widely used\nHugging Face Accelerate without compromising model quality.\n• In Section 5.3, we present extensive ablation studies to show the effectiveness of H2O under\ndifferent sequence lengths, especially the input with infinite sequence length and its compatibility\nwith quantization."
    },
    {
      "page_no": 7,
      "bbox": [
        107.64099884033203,
        547.9817504882812,
        505.74334716796875,
        557.87939453125
      ],
      "text": "All details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix A."
    },
    {
      "page_no": 7,
      "bbox": [
        108.0,
        569.6865234375,
        214.281005859375,
        579.6491088867188
      ],
      "text": "5.1\nEnd-to-End Results"
    },
    {
      "page_no": 7,
      "bbox": [
        107.53199768066406,
        589.6109008789062,
        505.65447998046875,
        610.7130737304688
      ],
      "text": "We demonstrate that H2O can reduce KV cache memory footprint by 5-10× while achieving compa-\nrable accuracy on a majority of tasks."
    },
    {
      "page_no": 7,
      "bbox": [
        107.64099884033203,
        618.2977905273438,
        505.2454833984375,
        683.0076293945312
      ],
      "text": "Setup.\nOur experiments are based on three representative model families of LLMs, including the\nOPT [39] with model sizes, LLaMA [40], and GPT-NeoX-20B [41]. We sample eight tasks from two\npopular evaluation frameworks (HELM [16] and lm-eval-harness [15]): COPA [42], MathQA [43],\nOpenBookQA [44], PiQA [45], RTE [46], Winogrande [47], XSUM [48], CNN/Daily Mail [49]. Also,\nwe evaluate our approach on recent generation benchmarks, AlpaceEval [50] and MT-bench [51],\nand the details are included in Appendix. We use NVIDIA A100 80GB GPU."
    },
    {
      "page_no": 7,
      "bbox": [
        108.0,
        690.3959350585938,
        504.16754150390625,
        722.4226684570312
      ],
      "text": "Baselines.\nSince H2O evenly assigns the caching budget to H2 and the most recent KV, except for\nfull KV cache, we consider the \"Local\" strategy as a baseline method. In addition, we also provide\ntwo different variants of Sparse Transformers (strided and fixed) as strong baselines. Also, the full"
    },
    {
      "page_no": 7,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "7"
    },
    {
      "page_no": 8,
      "bbox": [
        108.0,
        74.17692565917969,
        504.16888427734375,
        95.36265563964844
      ],
      "text": "KV cache with fewer shots (0/1-shot) prompts are considered as the baseline, which has a similar\nsequence length of the 5-shot tasks with 20% KV cache budget."
    },
    {
      "page_no": 8,
      "bbox": [
        305.72198486328125,
        117.06047058105469,
        504.0001220703125,
        136.1973876953125
      ],
      "text": "Table 1: Quantatively comparison between H2O with\nFull methods of different number of shots."
    },
    {
      "page_no": 8,
      "bbox": [
        315.6557312011719,
        146.86277770996094,
        497.5928955078125,
        154.40048217773438
      ],
      "text": "Methods\nPiQA\nCOPA\nOpenbookQA\nWinogrande"
    },
    {
      "page_no": 8,
      "bbox": [
        312.4068603515625,
        159.0546112060547,
        487.5906982421875,
        200.0699920654297
      ],
      "text": "Full\n80.09\n81.00\n44.80\n71.51\n0-shot Full\n78.89\n76.00\n41.40\n70.00\n1-shot Full\n79.11\n76.00\n43.60\n70.24\nLocal\n57.94\n56.00\n28.40\n51.30\nH2O\n79.22\n85.00\n43.80\n71.67"
    },
    {
      "page_no": 8,
      "bbox": [
        107.99998474121094,
        112.13291931152344,
        505.658203125,
        275.0540466308594
      ],
      "text": "Main Results.\nWe evaluate LLMs with KV\ncache budget ranging from 4% to 100% on 5-\nshot downstream tasks. Results are summarized\nin Figure 4 and Table 1& 2. The following ob-\nservations can be drawn: (1) With different KV\ncache budgets, our H2O demonstrates consistent\nand significant improvements against the \"Lo-\ncal\" strategy across various model sizes, model\ntypes, and downstream tasks. We can draw sim-\nilar conclusions comparing H2O with other baselines like Sparse Transformer; (2) Meanwhile, with\nless than 20% KV cache budget(i.e., more than 5× memory reduction), H2O achieves comparable\nperformance as the model with full KV embeddings; (3) H2O with 20% KV cache budget approxi-\nmately uses 1.2 samples per input and show consistent improvement over zero-shot and one-shot full\nmodel that use 1 and 2 samples, respectively. (4) Our H2O shows consistent effectiveness in the more\nchallenging long sequence generation tasks, XSUM, and CNN/Daily Mail."
    },
    {
      "page_no": 8,
      "bbox": [
        246.32400512695312,
        299.64892578125,
        505.5694274902344,
        318.806396484375
      ],
      "text": "Table 2: Results of different sparsification methods w. or w.o. H2.\nExperiments are conducted with OPT-30B with 20% KV cache budget."
    },
    {
      "page_no": 8,
      "bbox": [
        294.0441589355469,
        329.2777099609375,
        499.7842712402344,
        336.3060302734375
      ],
      "text": "Models\nCOPA\nOpenBookQA\nPiQA\nWinogrande"
    },
    {
      "page_no": 8,
      "bbox": [
        298.9217834472656,
        340.6463317871094,
        490.5589904785156,
        347.7336120605469
      ],
      "text": "Full\n85.00\n43.20\n78.51\n70.24"
    },
    {
      "page_no": 8,
      "bbox": [
        285.04730224609375,
        351.8516540527344,
        490.5589904785156,
        367.16998291015625
      ],
      "text": "Local w.o. H2\n48.00\n25.20\n55.82\n49.17\nLocal w. H2\n84.00\n43.00\n78.45\n69.06"
    },
    {
      "page_no": 8,
      "bbox": [
        252.57662963867188,
        370.916259765625,
        490.5589904785156,
        386.23388671875
      ],
      "text": "Sparse Transformer (strided) w.o. H2\n50.00\n24.60\n56.20\n47.59\nSparse Transformer (strided) w. H2\n83.00\n42.60\n78.24\n69.61"
    },
    {
      "page_no": 8,
      "bbox": [
        255.16641235351562,
        389.98016357421875,
        490.5589904785156,
        405.29852294921875
      ],
      "text": "Sparse Transformer (fixed) w.o. H2\n61.00\n23.80\n58.60\n49.88\nSparse Transformer (fixed) w. H2\n76.00\n41.40\n77.80\n64.96"
    },
    {
      "page_no": 8,
      "bbox": [
        107.50199890136719,
        291.9079284667969,
        504.4154357910156,
        477.257568359375
      ],
      "text": "Analysis.\nSince the evicted KV\nwill not be seen in the future\nsteps, dropping certain critical\nKV embeddings can cause a se-\nvere functional collapse, result-\ning in significant performance\ndegradation, e.g., in {LLaMA-\n13B, XSUM} {LLaMA-7B, CN-\nN/Daily Mail}, the \"Local\" strat-\negy collapses at 60% budgets\nwhile our H2O can still match\nthe full cache performance with\n20% budgets. In some tasks, our methods even surpass the baseline models, which demonstrates\na regularization effect of our H2O. For example, in {OPT-66B, RTE}, {OPT-30B, MathQA} and\n{GPT-NeoX-20B, XSUM}, our H2O achieves an extra performance improvement of 0.73%, 0.64%\nand 0.18 with 20% KV cache budget, respectively. These consistent results validate the effectiveness\nof our H2O framework."
    },
    {
      "page_no": 8,
      "bbox": [
        107.64099884033203,
        488.0252380371094,
        505.74798583984375,
        553.1815185546875
      ],
      "text": "Enhancing Baseline Techniques.\nImportantly, we observe other sparsification baselines fail under\nan extremely low cache budget while combining the most recent KV embeddings with the ones of\nheavy hitters successfully achieves comparable performance as using full KV embeddings. From\nTable 2, we can observe that both \"strided\" and \"fixed\" sparse attention fail under 20% KV cache\nbudgets, encountering a significant performance drop (up to 35% compared with the full cache).\nAfter combining with H2, both approaches reach a similar performance as using full KV embeddings."
    },
    {
      "page_no": 8,
      "bbox": [
        108.0,
        570.9095458984375,
        371.4809265136719,
        580.8721313476562
      ],
      "text": "5.2\nHeavy Hitter for High-Throughput Generative Inference"
    },
    {
      "page_no": 8,
      "bbox": [
        107.72200012207031,
        596.115966796875,
        505.5666198730469,
        634.9873657226562
      ],
      "text": "Table 3: Generation throughput (token/s) on a T4 GPU with different systems. In the sequence length row, we\nuse “512 + 32” to denote a prompt length of 512 and a generation length of 32. “OOM” means out-of-memory.\nThe gray text in the bracket denotes the effective batch size and the lowest level of the memory hierarchy that\nthe system needs for offloading, where “C” means CPU and “G” means GPU."
    },
    {
      "page_no": 8,
      "bbox": [
        115.9729232788086,
        646.5527954101562,
        461.5423889160156,
        655.250732421875
      ],
      "text": "Seq. length\n512+32\n512+512\n512+1024"
    },
    {
      "page_no": 8,
      "bbox": [
        115.9729232788086,
        661.0736083984375,
        459.8201904296875,
        669.7715454101562
      ],
      "text": "Model size\n6.7B\n30B\n6.7B\n30B\n6.7B\n30B"
    },
    {
      "page_no": 8,
      "bbox": [
        115.9729232788086,
        675.7874755859375,
        483.00909423828125,
        703.8138427734375
      ],
      "text": "Accelerate\n20.4 (2, G)\n0.6 (8, C)\n15.5 (1, G)\n0.6 (8, C)\n5.6 (16, C)\n0.6 (8, C)\nDeepSpeed\n10.2 (16, C)\n0.6 (4, C)\n9.6 (16, C)\n0.6 (4, C)\n10.1 (16, C)\n0.6 (4, C)\nFlexGen\n20.2 (2, G)\n8.1 (144, C)\n16.8 (1, G)\n8.5 (80, C)\n16.9 (1, G)\n7.1 (48, C)"
    },
    {
      "page_no": 8,
      "bbox": [
        115.9729232788086,
        709.6284790039062,
        496.05322265625,
        718.6592407226562
      ],
      "text": "H2O (20%)\n35.1 (4, G)\n12.7 (728, C)\n51.7 (4, G)\n18.83 (416, C)\n52.1 (4, G)\n13.82 (264, C)"
    },
    {
      "page_no": 8,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "8"
    },
    {
      "page_no": 9,
      "bbox": [
        107.72200012207031,
        73.54633331298828,
        505.59185791015625,
        92.40740966796875
      ],
      "text": "Table 5: Generation throughput and latency on an A100 GPU. In the sequence length row, we use “7000 + 1024”\nto denote a prompt length of 7000 and a generation length of 1024. “OOM” means out-of-memory."
    },
    {
      "page_no": 9,
      "bbox": [
        117.62582397460938,
        97.51801300048828,
        494.4183349609375,
        108.37336730957031
      ],
      "text": "Seq. length\nModel size\nBatch size\nMetric\nFlexGen\nH2O (20%)"
    },
    {
      "page_no": 9,
      "bbox": [
        117.58384704589844,
        115.44680786132812,
        494.41595458984375,
        149.13592529296875
      ],
      "text": "7000+1024\n30B\n1\nlatency (s)\n57.0\n50.4\n5000+5000\n13B\n4\nlatency (s)\n214.2\n155.4\n2048+2048\n6.7B\n24\nlatency (s)\n99.5\n53.5"
    },
    {
      "page_no": 9,
      "bbox": [
        117.58384704589844,
        156.36734008789062,
        494.4161071777344,
        178.44012451171875
      ],
      "text": "2048+2048\n6.7B\n24\nthroughput (token/s)\n494.1\n918.9\n2048+2048\n6.7B\n64\nthroughput (token/s)\nOOM\n1161.0"
    },
    {
      "page_no": 9,
      "bbox": [
        325.52301025390625,
        208.09230041503906,
        504.6033935546875,
        236.9154052734375
      ],
      "text": "Table 4: Results of generation throughput (token/s)\non a T4 GPU with different systems on real-world\ndatasets, XSUM."
    },
    {
      "page_no": 9,
      "bbox": [
        334.23614501953125,
        248.69935607910156,
        461.8340759277344,
        257.9015808105469
      ],
      "text": "Model size\n6.7B\n30B"
    },
    {
      "page_no": 9,
      "bbox": [
        334.23614501953125,
        264.2663269042969,
        490.96844482421875,
        293.9175720214844
      ],
      "text": "Accelerate\n11.98 (1, G)\n0.23 (2, C)\nDeepSpeed\n3.52 (6, C)\n0.31 (2, C)\nFlexGen\n10.80 (1, G)\n3.29 (44, C)"
    },
    {
      "page_no": 9,
      "bbox": [
        334.23614501953125,
        300.06927490234375,
        495.5734558105469,
        309.6235656738281
      ],
      "text": "H2O (20%)\n30.40 (1, G)\n6.70 (180, C)"
    },
    {
      "page_no": 9,
      "bbox": [
        107.64099884033203,
        206.3398895263672,
        505.74798583984375,
        460.0555419921875
      ],
      "text": "We implement our KV cache eviction policy in a\nstate-of-the-art inference engine, FlexGen [19], and\nreport the throughput and latency improvements.\nH2O is orthogonal to existing optimizations in Flex-\nGen, such as offloading and quantization, so they can\nbe combined to achieve better performance.\nSetup\nWe conducted experiments on two GPUs:\nan NVIDIA T4 (16GB) GPU and an NVIDIA A100\n(80GB) GPU. On the T4 GPU, we evaluate the gen-\neration throughput following the settings in the Flex-\nGen paper. The evaluated models are OPT-6.7B and\nOPT-30B. When the model and KV cache do not fit into a single GPU, we turn on CPU offloading.\nThe results of both pure GPU and GPU with CPU offloading are reported. All the speedup results are\ntested in an end-to-end setting, including both the pre-filling and generation phases. And it includes\nthe time for constructing the H2O KV cache. We use synthetic datasets where all prompts are padded\nto the same length. The system is then required to generate the same number of tokens for each\nprompt. We test different combinations of prompt and generation lengths. We also test our method on\nreal-world datasets (XSUM) for further assessment. The evaluation metric is generation throughput,\nwhich is the number of generated tokens / (prompt time + decoding time). We use DeepSpeed\nZeRO-Inference [17], Hugging Face Accelerate [18], and FlexGen [19] as baselines. On the A100\nGPU, with more GPU memory, we evaluate the performance of the systems with sequence lengths up\nto 10K. Although OPT is only trained on 2K sequence length, we benchmark the throughput and\nlatency performance to show the potential of H2O for better models in the future."
    },
    {
      "page_no": 9,
      "bbox": [
        107.64099884033203,
        468.741943359375,
        505.7451171875,
        566.2080688476562
      ],
      "text": "Results.\nTable 3& 4 shows the generation throughput of all systems on the T4 GPU. With our KV\ncache eviction policy, the memory usage is reduced, which brings two advantages: 1) we can use a\nmuch larger batch size; 2) we can make a setting from requiring offloading to not requiring offloading.\nAs shown in Table 3& 4, H2O with a 20% budget improves the generation throughput over FlexGen,\nDeepSpeed, and Accelerate by up to 3×, 29×, and 29×, respectively, across both synthetic and\nreal-world dataset. The results on the A100 GPU with sequence lengths from 4K to 10K are listed in\nTable 5. With the same batch size, H2O can reduce the latency by 1.1 −1.9× compared to FlexGen.\nAdditionally, H2O saves memory so it allows a larger batch size, which brings 2.3× improvement on\ngeneration throughput for OPT-6.7B."
    },
    {
      "page_no": 9,
      "bbox": [
        108.0,
        582.6585083007812,
        200.99090576171875,
        592.62109375
      ],
      "text": "5.3\nAblation Results"
    },
    {
      "page_no": 9,
      "bbox": [
        107.53199768066406,
        597.6439208984375,
        504.3481750488281,
        640.5640258789062
      ],
      "text": "We present extensive ablation studies of H2O on (1) infinite-length input, (2) different number of\nshots, (3) compatibility with quantization methods on KV cache, and (4) dissecting the effectiveness\nof different components. We find a surprising property of H2O – it not only improves the efficiency\nof LLMs, but also increases the diversity of the generated text."
    },
    {
      "page_no": 9,
      "bbox": [
        107.64099884033203,
        646.7589111328125,
        505.74530029296875,
        723.017578125
      ],
      "text": "Q1: Can H2O empower LLMs to process infinite-length inputs?\nA1: Effective genera-\ntion with sequence length up to four million tokens.\nSome recent works [52, 53] demon-\nstrate the possibility of handling infinite-length inputs, a notable challenge in current LLMs.\nThese methods employ an attention sink that retains the first few tokens and applies position\nrolling in the KV cache, empowering LLMs to process infinite-length inputs. Inspired by this\nprogress, we further implement our H2O for infinite-length inputs. Figure 5 showcases the pos-\nitive results of H2O, i.e., H2O can empower LLMs to tackle input with length up to four mil-"
    },
    {
      "page_no": 9,
      "bbox": [
        303.5090026855469,
        742.3324584960938,
        308.49029541015625,
        752.2950439453125
      ],
      "text": "9"
    },
    {
      "page_no": 10,
      "bbox": [
        108.0,
        74.33230590820312,
        503.9974060058594,
        95.30303192138672
      ],
      "text": "lion tokens, achieving a better performance (lower perplexity) than the original StreamLLM\nmethod [52] across various cache size. Further comparisons are reported in Appendix C.4."
    },
    {
      "page_no": 10,
      "bbox": [
        345.60198974609375,
        277.8464660644531,
        505.49139404296875,
        336.8333740234375
      ],
      "text": "Figure 5: (Upper) streaming with H2O to\nhandle inputs with sequence lengths of four\nmillion tokens. (Bottom) Perplexity compari-\nson between the original StreamLLM method\nand our H2O, results are collected on the first\ntext sample of PG-19 [54]."
    },
    {
      "page_no": 10,
      "bbox": [
        107.64099884033203,
        112.53016662597656,
        337.2931823730469,
        242.57608032226562
      ],
      "text": "Q2: Does the number of shots during inference effects\nthe effectiveness of H2O? A2: Effective across zero-shot\nto ten-shots inference. We further examine H2O under\ndifferent numbers of shots during inference, and the re-\nsults are reported in Table 10 and Figure 8. With different\nshots inference, our H2O achieves matching performance\n(difference less than 1.00%) as the full model across dif-\nferent downstream tasks. The \"Local\" strategy encounters\nsignificant performance degradation (up to 37.00%. Such\nresults demonstrate the effectiveness of our H2O under\ndifferent inference scenarios. More details about zero-shot\nand one-shot inference are reported in Appendix C.3."
    },
    {
      "page_no": 10,
      "bbox": [
        107.64099884033203,
        248.8354034423828,
        337.2980041503906,
        357.1470642089844
      ],
      "text": "Q3: Compatible with Quatization? A3: Yes. To pur-\nsue further efficiency, we show the compatibility of H2O\nwith another orthogonal approach, i.e., quantization in Ta-\nble 6. We use OPT-30B as our base model and COPA,\nOpenBookWA, and PiQA as evaluation tasks. Intuitively\nsparsity and quantization are highly related so combin-\ning them might introduce larger errors. Surprisingly the\ncombination almost always achieves better accuracy than\nH2O or quantization alone. Experiments about throughput\nimprovement are detailed in Appendix C.2."
    },
    {
      "page_no": 10,
      "bbox": [
        107.64099884033203,
        363.3419189453125,
        504.1683654785156,
        473.2110595703125
      ],
      "text": "Q4: When does H2O match the baseline with full KV\nembeddings? A4: With both H2 and the recent tokens. We investigate the separate effects of\nKV embeddings of H2 and the local tokens. We conduct experiments on 4 tasks with OPT-13B and\nOPT-30B. For each task, we compare the performance of three KV cache eviction policies, including\nonly the KV embeddings of H2, only the ones of local tokens, and our H2O that keep both. As\nshown in Table 9, only retaining the embeddings of H2 or local tokens can’t maintain a similar\nperformance as the model using full embeddings, with a performance degradation from 2.85% to\n22.75%. Incorporating both components, our H2O successfully retains the baseline performance with\nfull embeddings. Besides, the model with only H2 shows a consistent improvement against the one\nwith only local tokens, which indicates H2 might contribute more to maintaining the performance."
    },
    {
      "page_no": 10,
      "bbox": [
        107.64099884033203,
        477.9129333496094,
        504.1890869140625,
        532.3525390625
      ],
      "text": "Q5: Extra benefits from H2O? A5: Increased diversity of generated text. Besides all the benefits\nof our H2O, we also observe an bonus introduced by H2O, i.e., the improved diversity of generated\ncontent. The results are reported in Appendix C.1. Given the same prompts, we visualize the\ngenerated text of the models with different KV cache budgets. Compared with the model of full KV\ncache, our H2O can generate sentences with fewer repeated words and more creativity."
    },
    {
      "page_no": 10,
      "bbox": [
        108.00000762939453,
        540.0481567382812,
        262.12646484375,
        552.0033569335938
      ],
      "text": "6\nConclusion and Discussion"
    },
    {
      "page_no": 10,
      "bbox": [
        107.64099884033203,
        558.8818969726562,
        505.2473449707031,
        645.4390258789062
      ],
      "text": "In this paper, we study one of the key bottlenecks of LLM deployment, KV cache, particularly\nfor long-content and large-batch generation applications. We propose H2O, a simple KV cache\neviction policy for significantly reducing its memory footprint. The main insight of our approach\nis the recognition of a subset of tokens, known as Heavy Hitters, which contribute the most value\nwhen computing attention scores. We formulate the KV cache eviction as a dynamic submodular\nproblem and provide the theoretical guarantees for our algorithm. Through extensive evaluations,\nwe demonstrate that H2O can significantly improve end-to-end throughput and decrease latency in\nwall-clock time, without compromising the generation quality of LLMs across a variety of tasks."
    },
    {
      "page_no": 10,
      "bbox": [
        108.0,
        653.7461547851562,
        220.11585998535156,
        665.7013549804688
      ],
      "text": "7\nAcknowledgement"
    },
    {
      "page_no": 10,
      "bbox": [
        107.4219970703125,
        672.7352294921875,
        504.17144775390625,
        704.591064453125
      ],
      "text": "Ying Sheng and Clark Barrett are partly supported by NSF-2110397 and the Stanford Center for\nAutomated Reasoning. Z. Wang is in part supported by a Google Research Scholar Award and the\nNSF AI Institute for Foundations of Machine Learning (IFML)."
    },
    {
      "page_no": 10,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "10"
    },
    {
      "page_no": 11,
      "bbox": [
        108.0,
        72.78716278076172,
        163.54383850097656,
        84.74236297607422
      ],
      "text": "References"
    },
    {
      "page_no": 11,
      "bbox": [
        117.96299743652344,
        98.19349670410156,
        505.6548767089844,
        129.97409057617188
      ],
      "text": "[1] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96299743652344,
        135.82730102539062,
        504.00445556640625,
        167.68307495117188
      ],
      "text": "[2] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with\nlarge language models. In 27th International Conference on Intelligent User Interfaces, pages\n841–852, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96299743652344,
        173.54653930664062,
        504.001708984375,
        205.39205932617188
      ],
      "text": "[3] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96298217773438,
        211.31947326660156,
        503.9978942871094,
        243.10006713867188
      ],
      "text": "[4] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B\nHashimoto. Benchmarking large language models for news summarization. arXiv preprint\narXiv:2301.13848, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96300506591797,
        249.02845764160156,
        504.00439453125,
        280.8090515136719
      ],
      "text": "[5] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm\nLevskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling\ntransformer inference. arXiv preprint arXiv:2211.05102, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96298217773438,
        286.6622619628906,
        505.6507263183594,
        318.51806640625
      ],
      "text": "[6] Laszlo A Belady, Robert A Nelson, and Gerald S Shedler. An anomaly in space-time char-\nacteristics of certain programs running in a paging machine. Communications of the ACM,\n12(6):349–353, 1969."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96300506591797,
        324.37127685546875,
        505.7421569824219,
        345.3180847167969
      ],
      "text": "[7] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\narXiv preprint arXiv:2001.04451, 2020."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96298217773438,
        351.1712951660156,
        504.00439453125,
        383.0270690917969
      ],
      "text": "[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\nProcessing Systems, 35:16344–16359, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        117.96300506591797,
        388.9554443359375,
        503.99749755859375,
        409.82708740234375
      ],
      "text": "[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98098754882812,
        415.7544860839844,
        505.2430419921875,
        447.53607177734375
      ],
      "text": "[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. arXiv preprint arXiv:2009.14794, 2020."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98101806640625,
        453.4634704589844,
        504.0043640136719,
        485.2450866699219
      ],
      "text": "[11] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In International conference on\nmachine learning, pages 5156–5165. PMLR, 2020."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98100280761719,
        490.9190673828125,
        504.00347900390625,
        512.0440673828125
      ],
      "text": "[12] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\narXiv:1911.02150, 2019."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98100280761719,
        517.972412109375,
        505.387451171875,
        549.7530517578125
      ],
      "text": "[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98098754882812,
        555.6814575195312,
        505.7431335449219,
        576.5530395507812
      ],
      "text": "[14] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens.\narXiv preprint arXiv:2304.08467, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98098754882812,
        582.4814453125,
        505.24420166015625,
        636.0800170898438
      ],
      "text": "[15] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds,\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot\nlanguage model evaluation. In Zenodo. https://doi.org/10.5281/zenodo.5371628, September\n2021."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98099517822266,
        642.0084838867188,
        505.2450866699219,
        673.7890625
      ],
      "text": "[16] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of\nlanguage models. arXiv preprint arXiv:2211.09110, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        112.98098754882812,
        679.7174682617188,
        504.0044250488281,
        722.4070434570312
      ],
      "text": "[17] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng\nLi, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed\ninference: Enabling efficient inference of transformer models at unprecedented scale. arXiv\npreprint arXiv:2207.00032, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "11"
    },
    {
      "page_no": 12,
      "bbox": [
        112.98095703125,
        74.33230590820312,
        505.74322509765625,
        722.4070434570312
      ],
      "text": "[18] HuggingFace. Hugging face accelerate. https://huggingface.co/docs/accelerate/\nindex.\n[19] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu,\nZhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative\ninference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\n[20] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot.\narXiv preprint arXiv:2301.00774, 2023.\n[21] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning\napproach for large language models. arXiv preprint arXiv:2306.11695, 2023.\n[22] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pech-\nenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl):\nA missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175,\n2023.\n[23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. arXiv preprint\narXiv:2211.10438, 2022.\n[25] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.\narXiv preprint arXiv:2206.01861, 2022.\n[26] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit\nmatrix multiplication for transformers at scale. In Advances in Neural Information Processing\nSystems, 2022.\n[27] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n[28] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\n[29] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury\nZemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. Colt5: Faster\nlong-range transformers with conditional computation. arXiv preprint arXiv:2303.09752,\n2023.\n[30] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas\nHoffmann. Dynamic context pruning for efficient and interpretable autoregressive transformers.\narXiv preprint arXiv:2305.15805, 2023.\n[31] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.\narXiv preprint arXiv:2009.06732, 2020.\n[32] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture\nwith cascade token and head pruning. In 2021 IEEE International Symposium on High-\nPerformance Computer Architecture (HPCA), pages 97–110. IEEE, 2021.\n[33] Elizabeth J O’neil, Patrick E O’neil, and Gerhard Weikum. The lru-k page replacement\nalgorithm for database disk buffering. Acm Sigmod Record, 22(2):297–306, 1993.\n[34] Donghee Lee, Jongmoo Choi, Jong-Hun Kim, Sam H Noh, Sang Lyul Min, Yookun Cho, and\nChong Sang Kim. Lrfu: A spectrum of policies that subsumes the least recently used and least\nfrequently used policies. IEEE transactions on Computers, 50(12):1352–1361, 2001.\n[35] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power\nof self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.\n[36] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and\nvariable creation in self-attention mechanisms. In International Conference on Machine\nLearning, pages 5793–5831. PMLR, 2022.\n[37] Laszlo A. Belady. A study of replacement algorithms for a virtual-storage computer. IBM\nSystems journal, 5(2):78–101, 1966."
    },
    {
      "page_no": 12,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "12"
    },
    {
      "page_no": 13,
      "bbox": [
        112.98100280761719,
        74.40748596191406,
        505.7430114746094,
        95.27908325195312
      ],
      "text": "[38] Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation.\narXiv preprint arXiv:1911.03014, 2019."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98098754882812,
        100.57327270507812,
        505.244140625,
        132.42904663085938
      ],
      "text": "[39] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98098754882812,
        137.7984161376953,
        505.65814208984375,
        169.57907104492188
      ],
      "text": "[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98098754882812,
        174.9484405517578,
        505.6552429199219,
        228.54708862304688
      ],
      "text": "[41] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-\nNeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL\nWorkshop on Challenges & Perspectives in Creating Large Language Models, 2022."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98101806640625,
        233.84127807617188,
        505.6508483886719,
        265.6970520019531
      ],
      "text": "[42] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium:\nlogical formalizations of commonsense reasoning, pages 90–95, 2011."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98100280761719,
        270.9912414550781,
        505.6526794433594,
        335.5740661621094
      ],
      "text": "[43] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Han-\nnaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2357–2367, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98099517822266,
        340.9434509277344,
        503.9954528808594,
        361.8150634765625
      ],
      "text": "[44] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In EMNLP, 2018."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98098754882812,
        367.1844482421875,
        503.9977111816406,
        398.966064453125
      ],
      "text": "[45] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on\nArtificial Intelligence, 2020."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98101043701172,
        404.3344421386719,
        505.7429504394531,
        436.1160888671875
      ],
      "text": "[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98100280761719,
        441.4844665527344,
        505.24755859375,
        473.2660827636719
      ],
      "text": "[47] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\n2021."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98099517822266,
        478.560302734375,
        504.0042724609375,
        510.41607666015625
      ],
      "text": "[48] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the\nsummary! topic-aware convolutional neural networks for extreme summarization. arXiv\npreprint arXiv:1808.08745, 2018."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98100280761719,
        515.7102661132812,
        505.6506652832031,
        547.5660400390625
      ],
      "text": "[49] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text sum-\nmarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023,\n2016."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98099517822266,
        552.8602294921875,
        505.6526184082031,
        584.964111328125
      ],
      "text": "[50] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-\nfollowing models. https://github.com/tatsu-lab/alpaca_eval, 2023."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98098754882812,
        590.01025390625,
        504.0046081542969,
        621.8660278320312
      ],
      "text": "[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98099517822266,
        627.2354736328125,
        503.995361328125,
        648.1070556640625
      ],
      "text": "[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming\nlanguage models with attention sinks. arXiv preprint arXiv:2309.17453, 2023."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98095703125,
        653.4012451171875,
        505.650634765625,
        685.2570190429688
      ],
      "text": "[53] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang.\nLm-\ninfinite: Simple on-the-fly length generalization for large language models. arXiv preprint\narXiv:2308.16137, 2023."
    },
    {
      "page_no": 13,
      "bbox": [
        112.98100280761719,
        690.62646484375,
        503.9998474121094,
        722.4070434570312
      ],
      "text": "[54] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. In The International Conference on Learning\nRepresentations (ICLR), 2020."
    },
    {
      "page_no": 13,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "13"
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        74.33230590820312,
        504.00439453125,
        106.18807983398438
      ],
      "text": "[55] Song Han, Huizi Mao, and William J Dally.\nDeep compression: Compressing deep\nneural networks with pruning, trained quantization and huffman coding. arXiv preprint\narXiv:1510.00149, 2015."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        111.93849182128906,
        505.24603271484375,
        154.62905883789062
      ],
      "text": "[56] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\nefficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2704–2713, 2018."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        160.3794708251953,
        504.0036926269531,
        192.16006469726562
      ],
      "text": "[57] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization\nthrough weight equalization and bias correction. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1325–1334, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        197.83627319335938,
        504.0044250488281,
        229.69204711914062
      ],
      "text": "[58] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural\nnetwork quantization without retraining using outlier channel splitting. In International\nconference on machine learning, pages 7543–7552. PMLR, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        235.36727905273438,
        505.6507568359375,
        267.2230529785156
      ],
      "text": "[59] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convo-\nlutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440,\n2016."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98099517822266,
        272.8992614746094,
        504.0044860839844,
        293.8460693359375
      ],
      "text": "[60] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the\nvalue of network pruning. arXiv preprint arXiv:1810.05270, 2018."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98098754882812,
        299.52130126953125,
        504.0043640136719,
        331.3770751953125
      ],
      "text": "[61] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric\nmedian for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 4340–4349, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        337.1284484863281,
        505.74066162109375,
        368.9090881347656
      ],
      "text": "[62] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in\ndeep learning: Pruning and growth for efficient inference and training in neural networks. J.\nMach. Learn. Res., 22(241):1–124, 2021."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        374.66046142578125,
        505.74664306640625,
        395.5320739746094
      ],
      "text": "[63] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2(7), 2015."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98098754882812,
        401.0290832519531,
        505.65460205078125,
        433.0630798339844
      ],
      "text": "[64] Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Pro-\nceedings of the IEEE/CVF international conference on computer vision, pages 4794–4802,\n2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98099517822266,
        438.7392883300781,
        505.6505432128906,
        470.5950622558594
      ],
      "text": "[65] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Dis-\ntilling task-specific knowledge from bert into simple neural networks.\narXiv preprint\narXiv:1903.12136, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        476.345458984375,
        504.0018615722656,
        508.1260681152344
      ],
      "text": "[66] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efficient image transformers & distillation through attention. In\nInternational Conference on Machine Learning, pages 10347–10357. PMLR, 2021."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        513.80224609375,
        505.7422180175781,
        534.7490844726562
      ],
      "text": "[67] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98101806640625,
        540.4994506835938,
        504.3565368652344,
        572.2810668945312
      ],
      "text": "[68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in\nneural information processing systems, 32, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        577.9562377929688,
        505.244140625,
        609.8120727539062
      ],
      "text": "[69] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98097229003906,
        615.48828125,
        505.3864440917969,
        647.3440551757812
      ],
      "text": "[70] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:\nA question answering challenge targeting commonsense knowledge.\narXiv preprint\narXiv:1811.00937, 2018."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        653.019287109375,
        505.74212646484375,
        684.8750610351562
      ],
      "text": "[71] Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding.\nRadbert-cl: Factually-aware contrastive learning for radiology report classification. Proceed-\nings of machine learning research, 158:196–208, 2021."
    },
    {
      "page_no": 14,
      "bbox": [
        112.98100280761719,
        690.55126953125,
        504.00439453125,
        722.4070434570312
      ],
      "text": "[72] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and\nJimmy Lin. End-to-end open-domain question answering with bertserini. arXiv preprint\narXiv:1902.01718, 2019."
    },
    {
      "page_no": 14,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "14"
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        74.33230590820312,
        504.16717529296875,
        95.27908325195312
      ],
      "text": "[73] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for\nmulti-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98101806640625,
        100.45950317382812,
        504.34625244140625,
        132.30508422851562
      ],
      "text": "[74] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        137.47427368164062,
        505.244140625,
        169.33108520507812
      ],
      "text": "[75] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang,\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and\nbeyond. arXiv preprint arXiv:2304.13712, 2023."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98097229003906,
        174.50401306152344,
        505.24530029296875,
        206.35604858398438
      ],
      "text": "[76] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98099517822266,
        211.60145568847656,
        505.7397155761719,
        243.3736114501953
      ],
      "text": "[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        248.55227661132812,
        505.741943359375,
        269.4990539550781
      ],
      "text": "[78] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98101806640625,
        274.74444580078125,
        505.650634765625,
        306.52508544921875
      ],
      "text": "[79] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98101806640625,
        311.5953063964844,
        505.24664306640625,
        354.4600830078125
      ],
      "text": "[80] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,\nRoman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A\n176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98099517822266,
        359.7054748535156,
        505.2484130859375,
        380.57708740234375
      ],
      "text": "[81] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi,\nSanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98099517822266,
        385.7463073730469,
        505.24420166015625,
        417.60308837890625
      ],
      "text": "[82] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,\nand Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint\narXiv:1908.03265, 2019."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        422.8248596191406,
        503.9948425292969,
        443.7190856933594
      ],
      "text": "[83] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the\ndifficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98098754882812,
        448.790283203125,
        503.9991455078125,
        469.8360595703125
      ],
      "text": "[84] Dušan Variš and Ondˇrej Bojar. Sequence length is a domain: Length-based overfitting in\ntransformer models. arXiv preprint arXiv:2109.07276, 2021."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98098754882812,
        475.0814514160156,
        503.9999694824219,
        495.95306396484375
      ],
      "text": "[85] Wancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved\ncalibration for the transformer architecture. arXiv preprint arXiv:2102.11402, 2021."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98095703125,
        501.19091796875,
        504.0036926269531,
        522.0700073242188
      ],
      "text": "[86] Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural\nmachine translation. arXiv preprint arXiv:2008.07772, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98097229003906,
        527.2664794921875,
        504.00439453125,
        559.0960083007812
      ],
      "text": "[87] Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit\nCheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper transformers on small\ndatasets. arXiv preprint arXiv:2012.15355, 2020."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        564.3404541015625,
        505.3818359375,
        596.1220092773438
      ],
      "text": "[88] Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit:\nLearning to initialize neural networks for stable and efficient training. Advances in Neural\nInformation Processing Systems, 34:16410–16422, 2021."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        601.366455078125,
        505.2477111816406,
        633.1480102539062
      ],
      "text": "[89] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati,\nMichal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive\ngradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98095703125,
        638.3172607421875,
        505.74224853515625,
        659.2640380859375
      ],
      "text": "[90] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\nDeepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98098754882812,
        664.4342651367188,
        505.24420166015625,
        696.2900390625
      ],
      "text": "[91] Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao,\nand Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. arXiv\npreprint arXiv:2208.01313, 2022."
    },
    {
      "page_no": 15,
      "bbox": [
        112.98100280761719,
        701.2820434570312,
        503.9974060058594,
        722.4070434570312
      ],
      "text": "[92] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017."
    },
    {
      "page_no": 15,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "15"
    },
    {
      "page_no": 16,
      "bbox": [
        107.99996948242188,
        74.33230590820312,
        505.7431335449219,
        722.4070434570312
      ],
      "text": "[93] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu.\nTexygen: A benchmarking platform for text generation models. In The 41st international\nACM SIGIR conference on research & development in information retrieval, pages 1097–1100,\n2018.\n[94] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws.\narXiv preprint arXiv:2212.09720, 2022.\n[95] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023.\n[96] Pranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task\nhinting. arXiv preprint arXiv:2310.00726, 2023.\n[97] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating trans-\nformers via kernel density estimation. arXiv preprint arXiv:2302.02451, 2023.\n[98] Josh Alman and Zhao Song.\nFast attention requires bounded entries.\narXiv preprint\narXiv:2302.13214, 2023.\n[99] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the perfor-\nmance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023.\n[100] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic\nattention maintenance in large language models. arXiv preprint arXiv:2304.02207, 2023.\n[101] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. arXiv\npreprint arXiv:2305.04701, 2023.\n[102] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations\nof transformers. arXiv preprint arXiv:2306.02896, 2023.\n[103] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention\nsparsification algorithms for over-parameterized feature dimension. arxiv preprint: arxiv\n2304.03426, 2023.\n[104] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression\nproblems. arXiv preprint, 2303.15725, 2023.\n[105] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression.\narXiv preprint arXiv:2304.10411, 2023.\n[106] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers\nvia sketches for polynomial kernels. arXiv preprint arXiv:2310.01655, 2023.\n[107] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions\nregression. arXiv preprint arXiv:2305.00660, 2023.\n[108] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir\nZandieh.\nHyperattention: Long-context attention in near-linear time.\narXiv preprint\narXiv:2310.05869, 2023.\n[109] Timothy Chu, Zhao Song, and Chiwun Yang. How to protect copyright data in optimization of\nlarge language models? arXiv preprint arXiv:2308.12247, 2023.\n[110] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction for balancing the trade-\noff between creativity and reality in large language models. arXiv preprint arXiv:2306.02295,\n2023.\n[111] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformu-\nlating single layer attention in llm based on tensor and svm trick, and solving it in matrix\nmultiplication time. arXiv preprint arXiv:2309.07418, 2023.\n[112] Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. arXiv\npreprint arXiv:2204.13807, 2022.\n[113] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention\ncomputation. arXiv preprint arXiv:2307.08045, 2023.\n[114] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the\noptimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680,\n2023."
    },
    {
      "page_no": 16,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "16"
    },
    {
      "page_no": 17,
      "bbox": [
        107.99996948242188,
        74.33230590820312,
        505.7480773925781,
        722.4070434570312
      ],
      "text": "[115] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized large\nlanguage models. arXiv preprint arXiv:2308.10502, 2023.\n[116] Yichuan Deng, Zhao Song, Shenghao Xie, and Chiwun Yang. Unmasking transformers: A\ntheoretical approach to data recovery via attention weights. arXiv preprint arXiv:2310.12462,\n2023.\n[117] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th order algorithm for\nsoftmax attention optimization. arXiv preprint arXiv:2307.08352, 2023.\n[118] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix\nsoftmax attention to kronecker computation. arXiv preprint arXiv:2310.04064, 2023.\n[119] Yichuan Deng, Zhao Song, and Shenghao Xie. Convergence of two-layer regression with\nnonlinear units. arXiv preprint arXiv:2308.08358, 2023.\n[120] Timothy Chu, Zhao Song, and Chiwun Yang. Fine-tune language models to approximate\nunbiased in-context learning. arXiv preprint arXiv:2310.03331, 2023.\n[121] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora. Trainable trans-\nformer in transformer. arXiv preprint arXiv:2307.01189, 2023.\n[122] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while\npredicting the masked word? arXiv preprint arXiv:2303.08117, 2023.\n[123] Alexander Schrijver. Combinatorial optimization: polyhedra and efficiency, volume 24.\nSpringer, 2003.\n[124] Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, and Mikkel Thorup. Heavy hitters\nvia cluster-preserving clustering. In 2016 IEEE 57th Annual Symposium on Foundations of\nComputer Science (FOCS), pages 61–70. IEEE, 2016.\n[125] Vasileios Nakos and Zhao Song. Stronger l2/l2 compressed sensing; without iterating. In\nProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages\n289–297, 2019.\n[126] Vasileios Nakos, Zhao Song, and Zhengyu Wang. (nearly) sample-optimal sparse fourier\ntransform in any dimension; ripless and filterless. In 2019 IEEE 60th Annual Symposium on\nFoundations of Computer Science (FOCS), pages 1568–1577. IEEE, 2019.\n[127] Andreas Krause and Carlos Guestrin. Beyond convexity: Submodularity in machine learning.\nICML Tutorials, 2008.\n[128] Jeff Bilmes. Submodularity in machine learning applications. In Twenty-Ninth Conference on\nArtificial Intelligence, AAAI-15 Tutorial Forum, 2015.\n[129] Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation.\narXiv preprint arXiv:1911.03014, 2019.\n[130] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approxima-\ntions for maximizing submodular set functions—i. Mathematical programming, 14(1):265–\n294, 1978.\n[131] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization. arXiv\npreprint arXiv:2305.08367, 2023.\n[132] Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai\nLi, and Ricardo Henao. Infoprompt: Information-theoretic soft prompt tuning for natural\nlanguage understanding. arXiv preprint arXiv:2306.04933, 2023.\n[133] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning\nand weight shifting for softmax regression. arXiv preprint, 2023.\n[134] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix\nmultiplication time. In STOC, 2019.\n[135] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current\nmatrix multiplication time. In Conference on Learning Theory, pages 2140–2157. PMLR,\n2019.\n[136] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster\ninterior point method for semidefinite programming. In 2020 IEEE 61st annual symposium on\nfoundations of computer science (FOCS), pages 910–918. IEEE, 2020."
    },
    {
      "page_no": 17,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "17"
    },
    {
      "page_no": 18,
      "bbox": [
        107.99996948242188,
        74.33230590820312,
        505.65081787109375,
        358.20806884765625
      ],
      "text": "[137] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear pro-\ngramming. In International Conference on Machine Learning, pages 9835–9847. PMLR,\n2021.\n[138] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for\nsolving general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\nof Computing, pages 823–832, 2021.\n[139] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster:\nA robust ipm framework and efficient implementation. In 2022 IEEE 63rd Annual Symposium\non Foundations of Computer Science (FOCS), pages 233–244. IEEE, 2022.\n[140] Yuzhou Gu and Zhao Song.\nA faster small treewidth sdp solver.\narXiv preprint\narXiv:2211.06033, 2022.\n[141] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm for structured\nsupport vector machines. arXiv preprint arXiv:2307.07735, 2023.\n[142] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm\nfor projection matrix vector multiplication with application to empirical risk minimization. In\nAISTATS, 2023.\n[143] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs: O(√n)\npasses, small space and fast runtime. arXiv preprint arXiv:2309.05135, 2023.\n[144] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer\nminima in eO(n4) time. In ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024.\n[145] S Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou. Space-efficient\ninterior point method, with applications to linear programming and maximum weight bipartite\nmatching. In ICALP. arXiv preprint arXiv:2009.06106, 2020."
    },
    {
      "page_no": 18,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "18"
    },
    {
      "page_no": 19,
      "bbox": [
        107.25599670410156,
        94.49714660644531,
        192.8607635498047,
        115.1596450805664
      ],
      "text": "Appendix"
    },
    {
      "page_no": 19,
      "bbox": [
        108.0,
        137.2630615234375,
        215.8690948486328,
        151.6092529296875
      ],
      "text": "Table of Contents"
    },
    {
      "page_no": 19,
      "bbox": [
        131.91000366210938,
        158.77711486816406,
        480.09332275390625,
        167.74351501464844
      ],
      "text": "A More Implementation Details\n20"
    },
    {
      "page_no": 19,
      "bbox": [
        131.91000366210938,
        179.69813537597656,
        480.093994140625,
        212.65740966796875
      ],
      "text": "B\nExtended Related Works, Discussions, and Limitations\n21\nB.1\nExtended Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.2\nDiscussions and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21"
    },
    {
      "page_no": 19,
      "bbox": [
        131.91000366210938,
        224.53016662597656,
        480.09405517578125,
        353.1304016113281
      ],
      "text": "C Extended Experiments\n23\nC.1\nIncreased Diversity of Generated Text . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.2\nThroughput Improvement of H2O Combining with Quantization . . . . . . . . .\n24\nC.3\nEffectiveness on Zero-shot and One-shot Inference . . . . . . . . . . . . . . . .\n25\nC.4\nComparison with StreamingLLM . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.5\nEnhancing the \"Top-K\" Baseline\n. . . . . . . . . . . . . . . . . . . . . . . . .\n26\nC.6\nSeparate Effects of Heavy-Hitter and Local Tokens . . . . . . . . . . . . . . . .\n26\nC.7\nPerformance of Inference with Different Number of Shots. . . . . . . . . . . . .\n26\nC.8\nHeavy-Hitter in Attention Blocks . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nC.9\nComparsion with SpAtten . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nC.10 Heavy-Hitter in MLP Blocks\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27"
    },
    {
      "page_no": 19,
      "bbox": [
        131.91001892089844,
        365.0041198730469,
        480.0941162109375,
        637.0652465820312
      ],
      "text": "D Theoretical Analysis\n32\nD.1\nNotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nD.2\nSubmodular . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nD.3\nDynamic Submodular . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.4\nStatic Attention\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.5\nRecursive Attention Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.6\nEviction Policy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nD.7\nExplaining Submodular Diminishing Return Property in Attention Scheme\n. . .\n37\nD.8\nSubmodular: High Level Ideas\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nD.9\nRobust Greedy with Error Propagation\n. . . . . . . . . . . . . . . . . . . . . .\n38\nD.10 Robust Submodular and Adding Items\n. . . . . . . . . . . . . . . . . . . . . .\n39\nD.11 Universal Conditions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nD.12 Induction Lemma for Exact Function . . . . . . . . . . . . . . . . . . . . . . .\n40\nD.13 Induction Lemma for Approximate Function . . . . . . . . . . . . . . . . . . .\n41\nD.14 Theoretical Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nD.15 Extended Related Work for Theoretical Attention Problems\n. . . . . . . . . . .\n43\nD.16 Sparsity Preserving\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nD.17 Definition of Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nD.18 Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nD.19 Hessian\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD.20 Hessian is Positive Definite . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD.21 Hessian is Lipschitz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nD.22 Greedy Type Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48"
    },
    {
      "page_no": 19,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "19"
    },
    {
      "page_no": 20,
      "bbox": [
        108.0,
        72.78716278076172,
        279.1147766113281,
        84.74236297607422
      ],
      "text": "A\nMore Implementation Details"
    },
    {
      "page_no": 20,
      "bbox": [
        108.0,
        97.6933822631836,
        504.6654357910156,
        118.48904418945312
      ],
      "text": "In this section, our goal is to provide the details of system implementation (mentioned in Section 4.2)\nand experiment settings (mentioned in Section 5), as well as the pseudocode."
    },
    {
      "page_no": 20,
      "bbox": [
        108.0,
        131.57188415527344,
        504.1702880859375,
        185.4846649169922
      ],
      "text": "System Details.\nWe implement H2O on top of FlexGen. FlexGen is a white-box implementation of\nOPT models, and we have done some surgery on handling the KV cache. Specifically, for the given\nparameter K, we always maintain a list of KV cache with the first K entries as heavy hitters, and the\nlast K entries as most recent tokens. In order to avoid data movement in memory, the memory for\nKV cache is preallocated. We use a circular queue to update the last K entries efficiently."
    },
    {
      "page_no": 20,
      "bbox": [
        107.64099884033203,
        198.48289489746094,
        505.7434997558594,
        274.1310729980469
      ],
      "text": "Experiment Details.\nOur study involves the evaluation with varying sizes of KV cache that encom-\npass 4%, 10%, 20%, and 60% of the prompt’s length. We select two tasks (XSUM and CNN/Daily\nMail) from the HELM framework [16] and present the performance based on 1000 test samples.\nAdditionally, we employ the lm-eval-harness framework [15] for six other tasks (COPA, MathQA,\nOpenBookQA, PiQA, RTE, and Winogrande). For the tasks derived from the HELM framework, we\nreport the performance for zero-shot inference, while for the tasks from lm-eval-harness, we default\nto conduct five-shot inference."
    },
    {
      "page_no": 20,
      "bbox": [
        107.64099884033203,
        287.2953796386719,
        505.6586608886719,
        417.6540832519531
      ],
      "text": "Pseudocode.\nWe show a simplified pseudocode below to demonstrate our implementation skele-\nton. The function generation_loop() is the base loop in FlexGen that controls prefetch and\noverlap the I/O streams and computation.\nThen in function compute_layer(), the function\nattention_forward() will be called for attention layers. During prefill iterations, the function\ncompute_attention() will return K heavy hitters and K recent tokens if the prompt has a length\ngreater than or equal to 2K, otherwise return all the KV cache. The function compute_attention()\nwill return new KV cache and the indices for evicted entries during decoding iterations, which would\nbe the place to implement a customized eviction strategy. (If the current number of stored KV cache is\nless than 2K, the eviction will be ignored.) During each decoding iteration, the oldest one among the\nlast K tokens (if we have no less than K tokens’ KV cache stored) will be removed from the reserved\nlast K entries, one heavy hitter will be evicted for each head, and the newest token will be added to\nthe KV cache with a position in the last K entries. This happens in the function store_cache()."
    },
    {
      "page_no": 20,
      "bbox": [
        108.70600128173828,
        424.5062255859375,
        254.38058471679688,
        473.3236083984375
      ],
      "text": "def\ngeneration_loop (...):\n# Prologue\n...\n# Generate\nfor i in range(gen_len):"
    },
    {
      "page_no": 20,
      "bbox": [
        131.29600524902344,
        474.3202209472656,
        282.6221008300781,
        483.28662109375
      ],
      "text": "for j in range(num_layers):"
    },
    {
      "page_no": 20,
      "bbox": [
        142.59100341796875,
        484.2822265625,
        322.16119384765625,
        493.2486267089844
      ],
      "text": "for k in range( num_gpu_batches ):"
    },
    {
      "page_no": 20,
      "bbox": [
        119.76599884033203,
        494.2452087402344,
        282.38519287109375,
        582.91259765625
      ],
      "text": "load_weight(i, j+1, k)\nload_cache(i, j, k+1)\nstore_hidden(i, j, k-1)\nload_hidden(i, j, k+1)\ncompute_layer(i, j, k)\nstore_cache(i, j, k-1)\nsync ()\n# Epilogue\n..."
    },
    {
      "page_no": 20,
      "bbox": [
        108.47100067138672,
        593.8712158203125,
        468.62225341796875,
        632.7255859375
      ],
      "text": "# h is the hidden\nstates (activations)\ndef\nattention_forward (h, ...):\n# the read/write\nbuffer are\nintermediate\nstops for\nprefetching\nif prefill:"
    },
    {
      "page_no": 20,
      "bbox": [
        120.0479965209961,
        633.7222290039062,
        480.313720703125,
        682.53857421875
      ],
      "text": "h, new_k_cache , new_v_cache = compute_attention (h, ...)\n# select K heavy\nhitters\nand K recent\ntokens\nnew_k_cache , new_v_cache = select(new_k_cache , new_v_cache , K)\ncache_write_buffer .store(new_k_cache , new_v_cache)\nelse:"
    },
    {
      "page_no": 20,
      "bbox": [
        130.9040069580078,
        683.5352172851562,
        412.1759338378906,
        712.4265747070312
      ],
      "text": "k_cache , v_cache = cache_read_buf .pop()\n# evict_ids\ntrack the\nentries\nthat will be evicted\nh, new_k_cache , new_v_cache , evict_ids ="
    },
    {
      "page_no": 20,
      "bbox": [
        176.65899658203125,
        713.4232177734375,
        417.9004821777344,
        722.3895874023438
      ],
      "text": "compute_attention (h, k_cache , v_cache , ...)"
    },
    {
      "page_no": 20,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "20"
    },
    {
      "page_no": 21,
      "bbox": [
        120.10199737548828,
        75.38621520996094,
        474.67120361328125,
        94.31459045410156
      ],
      "text": "cache_write_buffer .store(new_k_cache , new_v_cache , evict_ids)\nreturn h"
    },
    {
      "page_no": 21,
      "bbox": [
        108.70600128173828,
        105.27421569824219,
        226.14804077148438,
        124.20259094238281
      ],
      "text": "def\nstore_cache (...):\nif prefill:"
    },
    {
      "page_no": 21,
      "bbox": [
        120.0479965209961,
        125.19920349121094,
        254.0173797607422,
        154.09059143066406
      ],
      "text": "# store\ncache\ndirectly\n...\nelse:"
    },
    {
      "page_no": 21,
      "bbox": [
        131.06100463867188,
        155.0872039794922,
        457.3396301269531,
        223.82960510253906
      ],
      "text": "k_new , v_new , evict_ids = cache_write_buffer .pop()\n# circular\nqueue for the last K entries\n# extract\nthe index for the oldest\ntoken at i-th iteration\noldest = ((i - 1) % K) - K\n# update the KV cache (k_home and v_home)\ncache_replace(k_home , evict_ids , k_new , K, oldest)\ncache_replace(v_home , evict_ids , v_new , K, oldest)"
    },
    {
      "page_no": 21,
      "bbox": [
        108.0,
        246.77215576171875,
        407.01153564453125,
        258.72735595703125
      ],
      "text": "B\nExtended Related Works, Discussions, and Limitations"
    },
    {
      "page_no": 21,
      "bbox": [
        107.69100189208984,
        271.4732971191406,
        505.2431945800781,
        303.330078125
      ],
      "text": "The goal of this section is to first introduce more background and related works for Section 2,\nthen describe some previous attempts in our experiments as well as discuss the social impact and\nlimitations of this work."
    },
    {
      "page_no": 21,
      "bbox": [
        108.0,
        317.8065185546875,
        238.15139770507812,
        327.76910400390625
      ],
      "text": "B.1\nExtended Related Works"
    },
    {
      "page_no": 21,
      "bbox": [
        107.64099884033203,
        337.73443603515625,
        505.71612548828125,
        424.2256774902344
      ],
      "text": "Quantization, Pruning, Distillation for Inference.\nPreviously, model compression algorithms\nhave been extensively investigated as a viable approach for mitigating the computational resource\nrequirements of model inference. These algorithms can be broadly categorized into three groups:\n(1) quantization [55, 56, 57, 58], which involves mapping model parameters or activations from\nhigh-precision data types to low-precision counterparts, such as using 8-bit integers instead of the\ncommonly employed 32-bit floating point format; (2) pruning or sparsity [59, 60, 61, 62], which aims\nto eliminate unnecessary neurons or weights within the models; (3) and distillation [63, 64, 65, 66]\nwhere predictions from larger models are utilized as supervised information to train smaller models."
    },
    {
      "page_no": 21,
      "bbox": [
        107.64099884033203,
        437.284423828125,
        505.3817138671875,
        556.5050659179688
      ],
      "text": "Transformer in NLP.\nTransformers [67] as a popular option have been frequently adopted by\nplenty of natural language processing (NLP) applications with prevailing successes [68, 69, 70, 71, 72,\n46, 73, 13, 74, 75]. Roughly, modern transformer-based networks can be categorized into two groups:\n(1) Encoder-Decoder or Encoder-only (i.e., BERT-style models [76]). This type of transformers\ncommonly leverages the Masked Language Modeling task which encourages models to capture\nthe intrinsic relationship between words and their context. Notable examples include BERT [76],\nRoBBERTa [69] and T5 [77]. (2) Decoder-only (i.e., GPT-style models [78]). Usually, this group of\ntransformers adopts the Casual Language Modeling task, which is optimized to generate the next\nword/token in a sequence based on the preceding words/tokens. Such an autoregressive manner\nis highly preferred by downstream tasks like text generation and question answering. GPT-3 [79],\nOPT [39], PaLM [13], and BLOOM [80] are representative architectures within this huge family."
    },
    {
      "page_no": 21,
      "bbox": [
        108.0,
        569.71337890625,
        505.2420349121094,
        634.2360229492188
      ],
      "text": "Training of Transformer.\nTraining a gigantic transformer-based model is not trivial. It notoriously\nsuffers from various issues such as overfitting, instability, etc. [81, 82, 83] analyze these bottlenecks\nfrom the optimization perspective. To address the issues, a great amount of pioneering effort is\ndevoted, including data augmentations [84, 85], a better initialization [86, 83, 87, 88], customized\noptimizers [89], improved normalization [90, 91], weight decay [92], and early stopping. However,\nthere is still a long way to go before we can fully clarify the mystery of transformer training."
    },
    {
      "page_no": 21,
      "bbox": [
        108.0,
        648.7135009765625,
        251.102783203125,
        658.6760864257812
      ],
      "text": "B.2\nDiscussions and Limitations"
    },
    {
      "page_no": 21,
      "bbox": [
        108.0,
        668.5779418945312,
        505.2454528808594,
        722.3828735351562
      ],
      "text": "Previous Attempts.\nDuring our experiments, we find several noteworthy observations. In H2O,\nemploying the accumulated attention score to evict KV embeddings can lead to a potential bias\nfavoring the least recent tokens. This bias arises because most previous tokens have a higher number\nof attention scores, resulting in a higher accumulated attention score and, consequently, a greater\nlikelihood of being retained. To address this concern, we conducted an additional experiment utilizing"
    },
    {
      "page_no": 21,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "21"
    },
    {
      "page_no": 22,
      "bbox": [
        108.0,
        74.17692565917969,
        504.0003662109375,
        117.09707641601562
      ],
      "text": "the averaged attention score to determine which KV embeddings should be retained. However, this\nalternative approach resulted in performance degradation. Additionally, we observed a significant\nproportion of H2 occurrences at the beginning of sentences. This finding suggests that the initial\ntokens play a substantial role in subsequent generation tasks."
    },
    {
      "page_no": 22,
      "bbox": [
        108.0,
        130.1488494873047,
        505.7483215332031,
        227.46615600585938
      ],
      "text": "Social Impact.\nOur work represents an initial effort in designing a KV Cache policy, a realm that\nhas been relatively unexplored and yet is a significant bottleneck in LLMs. The proposed Heavy\nHitter Oracle (H2O) provide a solution to improve the efficiency of LLM generation, which can save\nenergy cost and contribute to green AI. Besides, our approach also serves as a source of inspiration\nfor future advanced algorithm designs. We envision H2O as a foundational framework that could\nfacilitate further innovation in this area. Moreover, long content generation is an area of growing\nimportance that currently grapples with several efficiency issues. We hope our work that supports the\ngeneration of very long sequences will support further research in this direction, particularly in terms\nof enhancing consistency, devising superior evaluation methods, and establishing robust benchmarks."
    },
    {
      "page_no": 22,
      "bbox": [
        107.64099884033203,
        233.8670196533203,
        505.64996337890625,
        276.5990905761719
      ],
      "text": "Furthermore, another contribution of this study is the formulation of a dynamic submodular frame-\nwork. We believe that this theoretical framework possesses the potential to be applicable beyond\nspecific domains of interest. For instance, there may exist numerous other dynamic problems where\nthe task involves solving a submodular problem with slight variations at each time."
    },
    {
      "page_no": 22,
      "bbox": [
        108.0,
        289.5189208984375,
        505.6527404785156,
        354.257080078125
      ],
      "text": "Limitations.\nFurthermore, despite the notable advancements in throughput of our H2O, implement-\ning LLMs for generative inference remains challenging due to the immense parameter count. As a\nsubstantial portion of these parameters is encompassed within the MLP blocks, building upon our\nobservations of H2 occurrences in the MLP blocks, future research efforts can be directed towards\nleveraging the characteristics of H2 to devise an offloading policy. Such a policy can potentially\nenhance the efficiency of LLM inference even further."
    },
    {
      "page_no": 22,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "22"
    },
    {
      "page_no": 23,
      "bbox": [
        108.0,
        72.78716278076172,
        245.15005493164062,
        84.74236297607422
      ],
      "text": "C\nExtended Experiments"
    },
    {
      "page_no": 23,
      "bbox": [
        107.6709976196289,
        97.28990173339844,
        505.6572570800781,
        172.93710327148438
      ],
      "text": "In this section, our goal is to demonstrate that H2O can improve the diversity of generated text\n(mentioned in Section 5.3), throughput is further improved when combined with quantization (men-\ntioned in Section 5.3), and superior performance to handle extremely long length inputs (up to four\nmiddle tokens, mentioned in Section 5.3). Moreover, additional investigations about H2 are reported,\nincluding experiments under zero-shot/one-shot inference regime; H2O can also enhance the \"Top-K\"\nbaseline; extra results of H2 in attention blocks; the emergence of H2 in MLP blocks as well as its\nproperties."
    },
    {
      "page_no": 23,
      "bbox": [
        108.0,
        187.342529296875,
        294.0216979980469,
        197.3051300048828
      ],
      "text": "C.1\nIncreased Diversity of Generated Text"
    },
    {
      "page_no": 23,
      "bbox": [
        114.85289764404297,
        213.836181640625,
        156.47129821777344,
        222.80294799804688
      ],
      "text": "Model Input"
    },
    {
      "page_no": 23,
      "bbox": [
        126.21817016601562,
        232.72909545898438,
        495.25347900390625,
        336.1604309082031
      ],
      "text": "In the year 2087, humanity has achieved remarkable technological advancements and established colonies on\nmultiple planets within the Milky Way galaxy. Interstellar travel has become commonplace, with faster-than-light\nspacecraft enabling people to explore distant star systems. Earth has undergone significant changes due to\nsustainable development efforts, such as harnessing renewable energy sources and implementing widespread\necological restoration projects. However, alongside these triumphs, new challenges have emerged, including the\nrise of artificial intelligence, ethical dilemmas surrounding genetic engineering, and interplanetary political\ntensions. Against this backdrop, a team of intrepid scientists embarks on a mission to uncover the secrets of an\nancient alien civilization, hidden deep within an uncharted exoplanet. As they navigate treacherous terrains and\nencounter otherworldly phenomena, they must confront their own fears and reconcile humanity's thirst for\nknowledge with the potential consequences of uncovering secrets that were better left buried. The fate of both\ntheir mission and the future of humanity hang in the balance."
    },
    {
      "page_no": 23,
      "bbox": [
        120.17707824707031,
        346.7613830566406,
        197.02444458007812,
        355.7281188964844
      ],
      "text": "OPT-6.7B Full Cache"
    },
    {
      "page_no": 23,
      "bbox": [
        145.89126586914062,
        356.20782470703125,
        169.29148864746094,
        365.174560546875
      ],
      "text": "Output"
    },
    {
      "page_no": 23,
      "bbox": [
        120.9677963256836,
        410.1875915527344,
        217.1403350830078,
        419.1543273925781
      ],
      "text": "OPT-6.7B Local 20% Cache"
    },
    {
      "page_no": 23,
      "bbox": [
        156.34983825683594,
        419.6340637207031,
        179.7500762939453,
        428.6007995605469
      ],
      "text": "Output"
    },
    {
      "page_no": 23,
      "bbox": [
        124.00415802001953,
        448.6481628417969,
        223.5609893798828,
        457.6148986816406
      ],
      "text": "OPT-6.7B             20% Cache"
    },
    {
      "page_no": 23,
      "bbox": [
        161.0730743408203,
        458.0946350097656,
        184.4733123779297,
        467.0613708496094
      ],
      "text": "Output"
    },
    {
      "page_no": 23,
      "bbox": [
        126.21817016601562,
        470.9148254394531,
        495.01068115234375,
        500.12396240234375
      ],
      "text": "The game is set in the year 2087, and is a first-person exploration game. The player takes on the role of a scientist\nwho has been sent to a distant planet to investigate a mysterious signal. The planet is inhabited by a race of\nintelligent beings known as the \"Titans,\" who have been"
    },
    {
      "page_no": 23,
      "bbox": [
        126.21817016601562,
        429.08050537109375,
        253.745361328125,
        438.0472412109375
      ],
      "text": "........................................,,,,,,,,,,,,,,,,,,,,,,,"
    },
    {
      "page_no": 23,
      "bbox": [
        126.89291381835938,
        369.7027587890625,
        493.2400817871094,
        398.9118957519531
      ],
      "text": "The game is set in 2087, a few years after the events of the first game. The player takes the role of a team of\nscientists who are sent to an uncharted planet to investigate a mysterious signal. The team is composed of a pilot,\na scientist, a mechanic, and a technician."
    },
    {
      "page_no": 23,
      "bbox": [
        107.99998474121094,
        511.3865051269531,
        503.99810791015625,
        530.658935546875
      ],
      "text": "Figure 6: Visualization of one generation example with OPT-6.7B. Results are compared between the baseline\nmodel with full cache, our H2O, and the \"Local\" strategy that utilizes the most recent KV embeddings."
    },
    {
      "page_no": 23,
      "bbox": [
        107.64099884033203,
        538.9548950195312,
        505.747314453125,
        647.1070556640625
      ],
      "text": "Given the same prompt text, we visualize the generated text with OPT-6.7B and LLaMA-7B across\ndifferent methods, including the baseline model with full cache, our H2O, and the \"Local\" strategy.\nResults are reported in Figure 6 and 7. Even with less KV cache budget, our H2O can generate\nmore diverse content. Specifically, with the OPT-6.7B, the full model generates some redundant\nworks, like \"a few years after the events of the first game\" while our H2O describes \"the game is\na first-person exploration game\". As a comparison, when all KV cache budget is assigned to the\nmost recent tokens, the model fails to generate meaningful text and only repeats the word \".\" and\n\",\". Similar observations can also be drawn from the results of LLaMA-7B, in which the full model\nrepeatedly says \"so moving that\", \"so moved that\", and \"began to cry\" while our H2O describes both\nthe people and the environment."
    },
    {
      "page_no": 23,
      "bbox": [
        107.64099884033203,
        653.457275390625,
        504.4981689453125,
        707.7425537109375
      ],
      "text": "Moreover, We conducted a quantitative comparison via diversity metric(Self-BELU [93]). We\nrandomly sample 100 instances from the XSUM dataset, as prompts. Subsequently, we employed the\nLLaMa-7B to generate text of equal length. The full model reaches a Self-BELU score of 0.0057\nwhile H2O and the local method achieve 0.0051 and 0.0436, respectively. The comparatively lower\nSelf-BELU score of H2O indicates the slightly increased diversity of generated text."
    },
    {
      "page_no": 23,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "23"
    },
    {
      "page_no": 24,
      "bbox": [
        114.79315948486328,
        59.050174713134766,
        156.0487823486328,
        67.93876647949219
      ],
      "text": "Model Input"
    },
    {
      "page_no": 24,
      "bbox": [
        118.69139099121094,
        116.57257843017578,
        200.29295349121094,
        125.46117401123047
      ],
      "text": "LLaMA-7B Full Cache"
    },
    {
      "page_no": 24,
      "bbox": [
        146.89869689941406,
        125.93669128417969,
        170.09494018554688,
        134.82528686523438
      ],
      "text": "Output"
    },
    {
      "page_no": 24,
      "bbox": [
        120.15453338623047,
        181.4525146484375,
        220.91281127929688,
        190.3411102294922
      ],
      "text": "LLaMA-7B Local 20% Cache"
    },
    {
      "page_no": 24,
      "bbox": [
        157.93496704101562,
        190.81661987304688,
        181.13121032714844,
        199.70521545410156
      ],
      "text": "Output"
    },
    {
      "page_no": 24,
      "bbox": [
        123.15397644042969,
        224.2598876953125,
        227.26702880859375,
        233.1484832763672
      ],
      "text": "LLaMA-7B             20% Cache"
    },
    {
      "page_no": 24,
      "bbox": [
        162.61703491210938,
        233.62400817871094,
        185.8132781982422,
        242.51260375976562
      ],
      "text": "Output"
    },
    {
      "page_no": 24,
      "bbox": [
        122.71503448486328,
        76.44066619873047,
        495.6770324707031,
        105.39521789550781
      ],
      "text": "In a small, bustling cafe nestled in the heart of a vibrant city, a serendipitous event unfolded, leaving a lasting\nimpression on all who witnessed it. As the patrons sat sipping their coffees and engaging in animated conversations,\na talented street musician entered the cafe, carrying a weathered guitar and radiating an aura of creativity."
    },
    {
      "page_no": 24,
      "bbox": [
        124.72163391113281,
        139.9828643798828,
        482.4896545410156,
        168.93740844726562
      ],
      "text": "He began to play, and the patrons were captivated. The musician’s performance was so moving that the patrons\nbegan to applaud, and the musician was so moved that he began to cry. The patrons were so moved that they\nbegan to cry, and the musician was so"
    },
    {
      "page_no": 24,
      "bbox": [
        128.0659637451172,
        247.00131225585938,
        482.7197265625,
        275.9558410644531
      ],
      "text": "He began to play, and the room was filled with the sound of his music. The patrons of the cafe were enthralled\nby the music, and the atmosphere was electric. The cafe was packed with people, all of whom were enjoying\nthe music. The musician was a young"
    },
    {
      "page_no": 24,
      "bbox": [
        127.3970947265625,
        204.86279296875,
        458.17340087890625,
        213.7513885498047
      ],
      "text": "He ( ( ( ( ( ( ( ( ( ( ( ( ( (,  [)), 1999, 2000, 2001, 1, and, and, and, and, and, and, and, and, and, and, and,"
    },
    {
      "page_no": 24,
      "bbox": [
        108.0,
        290.0804748535156,
        504.00048828125,
        309.3529052734375
      ],
      "text": "Figure 7: Visualization of one generation example with LLaMA-7B. Results are compared between the baseline\nmodel with full cache, our H2O, and the \"Local\" strategy that utilizes the most recent KV embeddings."
    },
    {
      "page_no": 24,
      "bbox": [
        230.6070098876953,
        324.63897705078125,
        381.1169128417969,
        333.6053771972656
      ],
      "text": "Table 6: Compatibility with Quantization."
    },
    {
      "page_no": 24,
      "bbox": [
        232.50770568847656,
        337.65899658203125,
        398.6615295410156,
        346.1487121582031
      ],
      "text": "Models\nCOPA\nOpenBookQA\nPiQA"
    },
    {
      "page_no": 24,
      "bbox": [
        238.39955139160156,
        351.3907165527344,
        398.6598205566406,
        359.95166015625
      ],
      "text": "Full\n85.00\n43.20\n78.51"
    },
    {
      "page_no": 24,
      "bbox": [
        237.31985473632812,
        364.9267883300781,
        398.6598205566406,
        374.1331787109375
      ],
      "text": "H2O\n84.00\n43.00\n78.45"
    },
    {
      "page_no": 24,
      "bbox": [
        226.849365234375,
        378.85498046875,
        398.6598205566406,
        387.4159240722656
      ],
      "text": "Quant-4bit\n84.00\n43.28\n78.67"
    },
    {
      "page_no": 24,
      "bbox": [
        212.97109985351562,
        392.39105224609375,
        398.6598205566406,
        401.5974426269531
      ],
      "text": "H2O w. Quant-4bit\n84.00\n43.20\n78.80"
    },
    {
      "page_no": 24,
      "bbox": [
        108.0,
        418.7849426269531,
        406.3930358886719,
        429.5895690917969
      ],
      "text": "C.2\nThroughput Improvement of H2O Combining with Quantization"
    },
    {
      "page_no": 24,
      "bbox": [
        107.64099884033203,
        439.2849426269531,
        505.7463684082031,
        580.4706420898438
      ],
      "text": "Table 6 demonstrates H2O are capable of quantization and perverse the full model’s performance\nwith even 4-bit quantization. To further explore the compatibility of our H2O with quantization\nwith respect to throughput improvement, we conduct an additional evaluation with the quantization\nmethod implemented in FlexGen (Note that [94] employed a different 4-bit quantization method).\nThe corresponding results are presented in Table 7. Notably, for OPT-6.7B, we observed extra\nperformance enhancements in H2O when utilizing quantization compared to the vanilla version.\nThis improvement results from the GPU memory freed by weight quantization, which allows for a\nsignificant increase in batch size. However, it should be emphasized that the quantization method\nemployed in FlexGen is not implemented most efficiently, resulting in considerable computational\noverhead. Despite the batch size being enlarged by 20 times, the actual throughput improvement is\nless than 2 times. Nevertheless, it is important to acknowledge the potential benefits of combining\nH2O with quantization, as exemplified by the ability to increase the batch size further. For instance,\nthe implementation of 4-bit quantization could be accelerated by an optimized CUDA kernel."
    },
    {
      "page_no": 24,
      "bbox": [
        107.72200012207031,
        594.5275268554688,
        504.001953125,
        633.58935546875
      ],
      "text": "Table 7: Generation throughput (token/s) with different systems without offloading. We use H2O-c denotes the\nH2O with 4-bit weights compression. In the sequence length row, we use “512 + 32” to denote a prompt length\nof 512 and a generation length of 32. “OOM” means out-of-memory. The gray text in the bracket denotes the\nbatch size. We run OPT-6.7B on a single T4 GPU."
    },
    {
      "page_no": 24,
      "bbox": [
        211.5489959716797,
        638.2969360351562,
        398.21160888671875,
        657.2263793945312
      ],
      "text": "Seq. length\n512+32\n512+512\n512+1024\nModel size\n6.7B\n6.7B\n6.7B"
    },
    {
      "page_no": 24,
      "bbox": [
        211.5489959716797,
        663.427978515625,
        390.1597595214844,
        692.3193969726562
      ],
      "text": "Accelerate\n20.4 (2)\n15.5 (1)\nOOM\nDeepSpeed\nOOM\nOOM\nOOM\nFlexGen\n20.2 (2)\n16.8 (1)\n16.9 (1)"
    },
    {
      "page_no": 24,
      "bbox": [
        211.54898071289062,
        698.3134765625,
        394.6416931152344,
        717.5859375
      ],
      "text": "H2O (20%)\n35.1 (4)\n51.7 (4)\n52.1 (4)\nH2O-c (20%)\n50.5 (70)\n72.5 (52)\n62.3 (44)"
    },
    {
      "page_no": 24,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "24"
    },
    {
      "page_no": 25,
      "bbox": [
        108.0,
        74.31652069091797,
        347.0726318359375,
        84.27912139892578
      ],
      "text": "C.3\nEffectiveness on Zero-shot and One-shot Inference"
    },
    {
      "page_no": 25,
      "bbox": [
        107.53199768066406,
        94.15190124511719,
        505.7450256347656,
        169.88267517089844
      ],
      "text": "When facing zero-shot and one-shot inference tasks, H2O can successfully mitigate the memory\nrequirements of the KV cache by up to 5× under both one/zero-shot inference across different tasks,\nachieving matching performance as the model with full KV cache while the local method fails.\nHowever, unlike 5-shot, we also found that some tasks are more difficult, and the model requires\nmore information to generate the correct content, resulting in a higher KV cache budget (30-40%).\nThis might be expected since 0/1 shots in our benchmarks have shorter sequence lengths ranging\nfrom 100 to 300."
    },
    {
      "page_no": 25,
      "bbox": [
        163.44581604003906,
        173.6245880126953,
        490.2407531738281,
        182.01068115234375
      ],
      "text": "PiQA, LLaMA-7B\nCOPA, LLaMA-7B\nOpenBookQA, LLaMA-7B"
    },
    {
      "page_no": 25,
      "bbox": [
        154.8733673095703,
        395.12384033203125,
        232.26559448242188,
        403.50994873046875
      ],
      "text": "Winogrande, LLaMA-7B"
    },
    {
      "page_no": 25,
      "bbox": [
        109.01386260986328,
        216.5319366455078,
        117.39996337890625,
        341.4802551269531
      ],
      "text": "1-shot\n0-shot"
    },
    {
      "page_no": 25,
      "bbox": [
        288.86151123046875,
        395.12384033203125,
        476.653564453125,
        404.6458435058594
      ],
      "text": "MathQA, LLaMA-7B\nXSUM, LLaMA-7B"
    },
    {
      "page_no": 25,
      "bbox": [
        108.82453155517578,
        436.32733154296875,
        117.21062469482422,
        561.275634765625
      ],
      "text": "1-shot\n0-shot"
    },
    {
      "page_no": 25,
      "bbox": [
        187.0709991455078,
        622.846923828125,
        424.9316101074219,
        631.8133544921875
      ],
      "text": "Figure 8: Comparsion results on zero-shot and one-shot inference."
    },
    {
      "page_no": 25,
      "bbox": [
        108.0,
        648.7425537109375,
        275.800048828125,
        658.7051391601562
      ],
      "text": "C.4\nComparison with StreamingLLM"
    },
    {
      "page_no": 25,
      "bbox": [
        108.0,
        668.7332763671875,
        504.3529357910156,
        722.4070434570312
      ],
      "text": "Recent works, such as StreamLLM [52] and LM-Infinite [53], have shown promising potential in\nenabling Language Models (LLMs) to handle input of infinite length. They achieve this by only\nretaining the initial tokens and a limited local context. However, this approach may pose challenges\nfor certain tasks where vital information lies within the middle of the input and would be lost using\nthis strategy. We investigate it through two specific types of tasks:"
    },
    {
      "page_no": 25,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "25"
    },
    {
      "page_no": 26,
      "bbox": [
        107.53199768066406,
        74.33183288574219,
        505.7394104003906,
        117.09707641601562
      ],
      "text": "Multi-document question answering [95]: In this task, each test sample comprises ten documents,\nfollowed by a question. The key information to answer this question is stored in one of the documents.\nWe rearranged the crucial document’s position and found that the eviction strategy in StreamLLM or\nLM-Infinite can not perform well when the key document has been dropped."
    },
    {
      "page_no": 26,
      "bbox": [
        107.6709976196289,
        123.48137664794922,
        503.9989929199219,
        177.12210083007812
      ],
      "text": "Text Summarization Task (XSUM and CNN-DailyMail): Text summarization tasks require models\nto generate concise summaries of lengthy texts. Effective summarization demands a comprehensive\nunderstanding of the entire document, making it challenging when crucial information is dispersed\nthroughout the input. In particular, summarization often relies on long-context attention, and critical\ninformation may not be effectively captured within the limited local tokens."
    },
    {
      "page_no": 26,
      "bbox": [
        107.69100189208984,
        183.62342834472656,
        505.74273681640625,
        237.757568359375
      ],
      "text": "The results are reported in Figure 9, illustrating a consistent decline in the performance of StreamLLM.\nSince StreamLLM will always maintain the first few tokens as well as the local tokens, regardless\nof various input content, such a strategy will inevitably result in the loss of crucial information and\nsubsequently lead to a decrease in performance. In contrast, our H2O delivers markedly superior\nperformance. Also, with H2O, The model can successfully stream to four million tokens."
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        419.3704528808594,
        504.15362548828125,
        448.4703674316406
      ],
      "text": "Figure 9: Comparison results of StreamLLM [52] and our H2O on generization tasks. The number in each\nmethod represents the KV Cache budget of the start/heavy-hitter tokens and the local tokens, respectively. For\nexample, H2O-256-256 means maintaining 256 Heavy-Hitters and 256 local tokens."
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        462.3525085449219,
        273.1399841308594,
        472.3150939941406
      ],
      "text": "C.5\nEnhancing the \"Top-K\" Baseline"
    },
    {
      "page_no": 26,
      "bbox": [
        107.53199768066406,
        482.18792724609375,
        503.99993896484375,
        514.2826538085938
      ],
      "text": "We find H2 can further enhance another strong baseline with a \"Top-K\" strategy. The results are\nreported in Table 8. After combing with H2, the \"Top-K\" method achieves an extra improvement with\nup to 2.00% accuracy across 4 different tasks."
    },
    {
      "page_no": 26,
      "bbox": [
        107.72200012207031,
        531.1884765625,
        504.1759033203125,
        550.3253784179688
      ],
      "text": "Table 8: Results of the \"Top-K\" method w. or w.o. H2. Experiments are conducted with OPT-30B with 20% KV\ncache budget."
    },
    {
      "page_no": 26,
      "bbox": [
        195.97335815429688,
        554.7289428710938,
        427.8207702636719,
        564.137939453125
      ],
      "text": "Models\nCOPA\nOpenBookQA\nPiQA\nWinogrande"
    },
    {
      "page_no": 26,
      "bbox": [
        202.50314331054688,
        569.9474487304688,
        415.4708557128906,
        579.4353637695312
      ],
      "text": "Full\n85.00\n43.20\n78.51\n70.24"
    },
    {
      "page_no": 26,
      "bbox": [
        184.0414276123047,
        584.94921875,
        415.4708557128906,
        605.4552612304688
      ],
      "text": "TopK w.o. H2\n80.00\n41.40\n76.96\n65.35\nTopK w. H2\n82.00\n42.80\n77.96\n66.48"
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        626.8724975585938,
        348.8558349609375,
        636.8350830078125
      ],
      "text": "C.6\nSeparate Effects of Heavy-Hitter and Local Tokens"
    },
    {
      "page_no": 26,
      "bbox": [
        107.69100189208984,
        646.9839477539062,
        504.00140380859375,
        667.81005859375
      ],
      "text": "Table 9 demonstrates the separate effects of Heavy-Hitters and the local tokens. And we can observe\nHeavy-Hitters contribute more to maintaining the performance of models."
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        681.469482421875,
        380.26800537109375,
        691.4320678710938
      ],
      "text": "C.7\nPerformance of Inference with Different Number of Shots."
    },
    {
      "page_no": 26,
      "bbox": [
        107.69100189208984,
        701.304931640625,
        503.9954528808594,
        722.4906616210938
      ],
      "text": "Table 10 demonstrates our H2O achieves consistent improvements across different number of shots\nduring inference and maintains the full model’s performance with 20% memory budget."
    },
    {
      "page_no": 26,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "26"
    },
    {
      "page_no": 27,
      "bbox": [
        209.03500366210938,
        70.43644714355469,
        402.6886291503906,
        79.74585723876953
      ],
      "text": "Table 9: Ablation study of H2O across different tasks."
    },
    {
      "page_no": 27,
      "bbox": [
        205.553466796875,
        83.19730377197266,
        418.5273742675781,
        91.58494567871094
      ],
      "text": "Tasks\nModels\nFull\nw. Local\nw. H2\nw. Local + H2"
    },
    {
      "page_no": 27,
      "bbox": [
        206.06044006347656,
        95.8867416381836,
        405.9738464355469,
        112.15564727783203
      ],
      "text": "PiQA\nOPT-13B\n77.37\n54.62\n76.12\n77.26\nOPT-30B\n78.51\n55.82\n67.25\n78.45"
    },
    {
      "page_no": 27,
      "bbox": [
        192.52830505371094,
        116.8673324584961,
        405.9738464355469,
        133.1363067626953
      ],
      "text": "OpenBookQA\nOPT-13B\n41.40\n25.60\n30.40\n41.20\nOPT-30B\n43.20\n25.20\n26.60\n43.00"
    },
    {
      "page_no": 27,
      "bbox": [
        201.1219482421875,
        137.84718322753906,
        405.9738464355469,
        154.11695861816406
      ],
      "text": "MathQA\nOPT-13B\n26.67\n22.04\n23.82\n26.93\nOPT-30B\n26.23\n20.87\n21.98\n26.87"
    },
    {
      "page_no": 27,
      "bbox": [
        195.9086151123047,
        158.82777404785156,
        405.9738464355469,
        175.09681701660156
      ],
      "text": "Winogrande\nOPT-13B\n68.59\n49.96\n51.85\n67.32\nOPT-30B\n70.24\n49.17\n47.36\n69.06"
    },
    {
      "page_no": 27,
      "bbox": [
        141.802001953125,
        186.9705047607422,
        469.92083740234375,
        196.1444091796875
      ],
      "text": "Table 10: Results under different sequence length of OPT-30B with 20% KV cache budget."
    },
    {
      "page_no": 27,
      "bbox": [
        222.0247802734375,
        199.226806640625,
        400.10601806640625,
        212.08261108398438
      ],
      "text": "Tasks\nMethods\n5-shots\n10-shots\nOPT-30B\nOPT-66B\nOPT-30B\nOPT-66B"
    },
    {
      "page_no": 27,
      "bbox": [
        210.92459106445312,
        222.3978271484375,
        247.7847900390625,
        228.48338317871094
      ],
      "text": "OpenBookQA"
    },
    {
      "page_no": 27,
      "bbox": [
        259.7276611328125,
        215.7897186279297,
        395.4835510253906,
        235.5755157470703
      ],
      "text": "Full\n43.20\n44.40\n43.00\n44.80\nLocal\n25.20\n30.60\n26.60\n38.80\nH2O\n43.00\n44.20\n42.80\n44.80"
    },
    {
      "page_no": 27,
      "bbox": [
        220.95947265625,
        245.56881713867188,
        237.74951171875,
        251.6543731689453
      ],
      "text": "COPA"
    },
    {
      "page_no": 27,
      "bbox": [
        259.7276611328125,
        238.96070861816406,
        395.4835510253906,
        258.7464599609375
      ],
      "text": "Full\n85.00\n83.00\n86.00\n85.00\nLocal\n48.00\n59.00\n60.00\n76.00\nH2O\n84.00\n82.00\n85.00\n86.00"
    },
    {
      "page_no": 27,
      "bbox": [
        217.69146728515625,
        268.73919677734375,
        241.01742553710938,
        274.8247375488281
      ],
      "text": "MathQA"
    },
    {
      "page_no": 27,
      "bbox": [
        259.7276611328125,
        262.131103515625,
        395.4835510253906,
        281.9174499511719
      ],
      "text": "Full\n26.23\n27.87\n26.67\n27.00\nLocal\n20.87\n25.49\n21.11\n23.08\nH2O\n26.87\n27.67\n26.47\n27.30"
    },
    {
      "page_no": 27,
      "bbox": [
        108.0,
        301.9255065917969,
        273.229736328125,
        311.8880920410156
      ],
      "text": "C.8\nHeavy-Hitter in Attention Blocks"
    },
    {
      "page_no": 27,
      "bbox": [
        107.69100189208984,
        322.8324279785156,
        503.9951477050781,
        345.1220703125
      ],
      "text": "The distribution of accumulated attention scores of all the tokens within attentions blocks is illustrated\nin Figure 10. We can observe that H2 broadly exists in each layer."
    },
    {
      "page_no": 27,
      "bbox": [
        108.00001525878906,
        359.9435119628906,
        243.8798828125,
        369.9060974121094
      ],
      "text": "C.9\nComparsion with SpAtten"
    },
    {
      "page_no": 27,
      "bbox": [
        107.53199768066406,
        380.5429382324219,
        505.6515808105469,
        467.1000671386719
      ],
      "text": "Compared with SpAtten [32], the main differences of H2O are i) They accumulate attention scores\nacross attention heads and layers, while in our algorithm, each token can be kept or evicted indepen-\ndently across heads and layers, providing more flexibility for selecting critical KV embeddings; ii)\nWe also use KV of the most recent tokens during generation and demonstrate that such H2 tokens\ncan effectively enhance other sparse attention strategies; iii) we formulate the KV cache eviction as a\ndynamic submodular problem and prove a theoretical guarantee (under mild assumptions) for our\nnovel algorithms. Moreover, we also provide a quantitative comparison with SpAtten, and the results\nare reported in Table 11."
    },
    {
      "page_no": 27,
      "bbox": [
        143.0749969482422,
        487.4434509277344,
        468.647216796875,
        496.7528991699219
      ],
      "text": "Table 11: Comparison between SpAtten [32] and H2O across various tasks with OPT-30B."
    },
    {
      "page_no": 27,
      "bbox": [
        233.79827880859375,
        500.61322021484375,
        378.9542541503906,
        508.95123291015625
      ],
      "text": "Models\nCOPA\nOpenBookQA\nPiQA"
    },
    {
      "page_no": 27,
      "bbox": [
        239.58480834960938,
        514.1004028320312,
        378.95343017578125,
        522.50830078125
      ],
      "text": "Full\n85.00\n43.20\n78.51"
    },
    {
      "page_no": 27,
      "bbox": [
        232.63912963867188,
        527.586669921875,
        378.95343017578125,
        535.9945678710938
      ],
      "text": "SpAtten\n82.00\n41.90\n77.06"
    },
    {
      "page_no": 27,
      "bbox": [
        238.52442932128906,
        540.8807983398438,
        378.95343017578125,
        549.9226684570312
      ],
      "text": "H2O\n84.00\n43.00\n78.45"
    },
    {
      "page_no": 27,
      "bbox": [
        108.0,
        577.0435180664062,
        259.9495544433594,
        587.006103515625
      ],
      "text": "C.10\nHeavy-Hitter in MLP Blocks"
    },
    {
      "page_no": 27,
      "bbox": [
        108.0,
        597.6439208984375,
        504.3534851074219,
        651.556640625
      ],
      "text": "Besides the attention blocks, the presence of Heavy-Hitters (H2) is observed within the MLP blocks\nof LLMs. We utilize the Wiki-Text-103 dataset as the input and record the activated frequency of\nneurons in the hidden layer of MLP blocks. As depicted in Figure 11, the activated frequency of\nneurons follows a power-law distribution, wherein a small number of neurons are activated by nearly\nall input tokens (with a 100% frequency) while the majority of other neurons are rarely activated."
    },
    {
      "page_no": 27,
      "bbox": [
        108.0,
        657.6689453125,
        504.4151611328125,
        722.4070434570312
      ],
      "text": "Subsequently, a thorough examination of various characteristics pertaining to H2 in MLP blocks is\nconducted, encompassing the following aspects: (1) The elimination of H2 leads to a substantial\ndecline in performance, although such degradation can be easily recovered even with a mere 1%\nof the training data; (2) H2 exhibits a significant degree of overlap across different type of input\ncontent; (3) The emergence of H2 occurs early in the training process, thus exhibiting an \"early-bird\"\ncharacteristic, and their positions undergo gradual changes during subsequent training phases."
    },
    {
      "page_no": 27,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "27"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        188.51329040527344,
        218.30357360839844,
        194.46493530273438
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        181.56492614746094,
        140.9589080810547,
        187.51657104492188
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        170.11744689941406,
        140.9589080810547,
        176.069091796875
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        158.66993713378906,
        140.9589080810547,
        164.62158203125
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        147.2224578857422,
        140.9589080810547,
        153.17410278320312
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        135.7749481201172,
        140.9589080810547,
        141.72659301757812
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        124.32750701904297,
        140.9589080810547,
        130.27914428710938
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        128.5609893798828,
        124.25746154785156,
        134.51263427734375,
        179.61032104492188
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        109.69518280029297,
        186.68319702148438,
        116.08912658691406
      ],
      "text": "1e5\nLayer 1"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        188.51329040527344,
        306.2133483886719,
        194.46493530273438
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        181.56492614746094,
        228.86865234375,
        187.51657104492188
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        169.28822326660156,
        228.86865234375,
        175.2398681640625
      ],
      "text": "0.2"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        157.01148986816406,
        228.86865234375,
        162.963134765625
      ],
      "text": "0.4"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        144.7347869873047,
        228.86865234375,
        150.68643188476562
      ],
      "text": "0.6"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        132.4580841064453,
        228.86865234375,
        138.40972900390625
      ],
      "text": "0.8"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        120.18135833740234,
        228.86865234375,
        126.13298797607422
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        109.69518280029297,
        274.59295654296875,
        116.08912658691406
      ],
      "text": "1e6\nLayer 2"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        188.51329040527344,
        394.1230773925781,
        194.46493530273438
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        181.56492614746094,
        316.7784118652344,
        187.51657104492188
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        173.3303680419922,
        316.7784118652344,
        179.28201293945312
      ],
      "text": "0.2"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        165.09580993652344,
        316.7784118652344,
        171.04745483398438
      ],
      "text": "0.4"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        156.86122131347656,
        316.7784118652344,
        162.8128662109375
      ],
      "text": "0.6"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        148.6266326904297,
        316.7784118652344,
        154.57827758789062
      ],
      "text": "0.8"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        140.39207458496094,
        316.7784118652344,
        146.34371948242188
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        132.1575164794922,
        316.7784118652344,
        138.10916137695312
      ],
      "text": "1.2"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        123.92296600341797,
        316.7784118652344,
        129.87460327148438
      ],
      "text": "1.4"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        115.6883773803711,
        316.7784118652344,
        121.64000701904297
      ],
      "text": "1.6"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        109.69518280029297,
        362.502685546875,
        116.08912658691406
      ],
      "text": "1e6\nLayer 3"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        188.51329040527344,
        482.0328063964844,
        194.46493530273438
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        181.56492614746094,
        404.68817138671875,
        187.51657104492188
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        167.5432586669922,
        404.68817138671875,
        173.49490356445312
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        153.52159118652344,
        404.68817138671875,
        159.47323608398438
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        139.49989318847656,
        404.68817138671875,
        145.4515380859375
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        125.47823333740234,
        404.68817138671875,
        131.42987060546875
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        109.69518280029297,
        450.41241455078125,
        116.08912658691406
      ],
      "text": "1e6\nLayer 4"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        274.608154296875,
        218.30357360839844,
        280.5597839355469
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        267.6597900390625,
        140.9589080810547,
        273.6114196777344
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        256.1817932128906,
        140.9589080810547,
        262.1334228515625
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        244.70384216308594,
        140.9589080810547,
        250.65548706054688
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        233.2259063720703,
        140.9589080810547,
        239.17755126953125
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        221.74790954589844,
        140.9589080810547,
        227.69955444335938
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        210.2699737548828,
        140.9589080810547,
        216.22161865234375
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        128.5609893798828,
        210.352294921875,
        134.51263427734375,
        265.70513916015625
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        195.7899627685547,
        186.68319702148438,
        202.18397521972656
      ],
      "text": "1e6\nLayer 5"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        274.608154296875,
        306.2133483886719,
        280.5597839355469
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        267.6597900390625,
        228.86865234375,
        273.6114196777344
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        256.53271484375,
        228.86865234375,
        262.4843444824219
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        245.40565490722656,
        228.86865234375,
        251.3572998046875
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        234.2786407470703,
        228.86865234375,
        240.23028564453125
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        223.1515655517578,
        228.86865234375,
        229.10321044921875
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        212.02452087402344,
        228.86865234375,
        217.97616577148438
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        200.8975067138672,
        228.86865234375,
        206.84915161132812
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        195.7899627685547,
        274.59295654296875,
        202.18397521972656
      ],
      "text": "1e6\nLayer 6"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        274.608154296875,
        394.1230773925781,
        280.5597839355469
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        267.6597900390625,
        316.7784118652344,
        273.6114196777344
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        257.1693420410156,
        316.7784118652344,
        263.1209716796875
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        246.67884826660156,
        316.7784118652344,
        252.6304931640625
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        236.1884002685547,
        316.7784118652344,
        242.14004516601562
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        225.6979522705078,
        316.7784118652344,
        231.64959716796875
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        215.20750427246094,
        316.7784118652344,
        221.15914916992188
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        311.11663818359375,
        204.7169952392578,
        316.7784118652344,
        210.66864013671875
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        195.7899627685547,
        362.502685546875,
        202.18397521972656
      ],
      "text": "1e6\nLayer 7"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        274.608154296875,
        482.0328063964844,
        280.5597839355469
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        267.6597900390625,
        404.68817138671875,
        273.6114196777344
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        258.87921142578125,
        404.68817138671875,
        264.8308410644531
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        250.09864807128906,
        404.68817138671875,
        256.05029296875
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        241.31809997558594,
        404.68817138671875,
        247.26974487304688
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        232.5375213623047,
        404.68817138671875,
        238.48916625976562
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        223.7570037841797,
        404.68817138671875,
        229.70864868164062
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        214.97642517089844,
        404.68817138671875,
        220.92807006835938
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        399.0263977050781,
        206.1958770751953,
        404.68817138671875,
        212.14752197265625
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        195.7899627685547,
        450.41241455078125,
        202.18397521972656
      ],
      "text": "1e6\nLayer 8"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        360.7030029296875,
        218.30357360839844,
        366.6546325683594
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        353.7546081542969,
        140.9589080810547,
        359.70623779296875
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        345.5313720703125,
        140.9589080810547,
        351.4830017089844
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        337.30810546875,
        140.9589080810547,
        343.2597351074219
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        329.0848693847656,
        140.9589080810547,
        335.0364990234375
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        320.86163330078125,
        140.9589080810547,
        326.8132629394531
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        312.63836669921875,
        140.9589080810547,
        318.5899963378906
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        304.4151306152344,
        140.9589080810547,
        310.36676025390625
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        296.1918640136719,
        140.9589080810547,
        302.14349365234375
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        287.9686279296875,
        140.9589080810547,
        293.9202575683594
      ],
      "text": "4.0"
    },
    {
      "page_no": 28,
      "bbox": [
        128.5609893798828,
        296.4471435546875,
        134.51263427734375,
        351.79998779296875
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        281.88482666015625,
        186.68319702148438,
        288.27880859375
      ],
      "text": "1e6\nLayer 9"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        360.7030029296875,
        306.2133483886719,
        366.6546325683594
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        353.7546081542969,
        228.86865234375,
        359.70623779296875
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        345.3127746582031,
        228.86865234375,
        351.264404296875
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        336.8709411621094,
        228.86865234375,
        342.82257080078125
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        328.42913818359375,
        228.86865234375,
        334.3807678222656
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        319.9873046875,
        228.86865234375,
        325.9389343261719
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        311.5455017089844,
        228.86865234375,
        317.49713134765625
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        303.1036682128906,
        228.86865234375,
        309.0552978515625
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        294.6618347167969,
        228.86865234375,
        300.61346435546875
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        286.2200012207031,
        228.86865234375,
        292.171630859375
      ],
      "text": "4.0"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        281.88482666015625,
        275.72528076171875,
        288.27880859375
      ],
      "text": "1e6\nLayer 10"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        360.7030029296875,
        394.1230773925781,
        366.6546325683594
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        353.7546081542969,
        316.7784118652344,
        359.70623779296875
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        338.6195983886719,
        316.7784118652344,
        344.57122802734375
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        323.4845886230469,
        316.7784118652344,
        329.43621826171875
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        308.34954833984375,
        316.7784118652344,
        314.3011779785156
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        293.21453857421875,
        316.7784118652344,
        299.1661682128906
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        281.88482666015625,
        363.5453796386719,
        288.27880859375
      ],
      "text": "1e6\nLayer 11"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        360.7030029296875,
        482.0328063964844,
        366.6546325683594
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        353.7546081542969,
        404.6881408691406,
        359.70623779296875
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        338.7255859375,
        404.6881408691406,
        344.6772155761719
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        323.6965637207031,
        404.6881408691406,
        329.648193359375
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        308.6675109863281,
        404.6881408691406,
        314.619140625
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        293.6385192871094,
        404.6881408691406,
        299.59014892578125
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        281.88482666015625,
        451.54473876953125,
        288.27880859375
      ],
      "text": "1e6\nLayer 12"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        446.7978210449219,
        218.30357360839844,
        452.74945068359375
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        439.8494567871094,
        140.9589080810547,
        445.80108642578125
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        431.82757568359375,
        140.9589080810547,
        437.7792053222656
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        423.80572509765625,
        140.9589080810547,
        429.7573547363281
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        415.7839050292969,
        140.9589080810547,
        421.73553466796875
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        407.76202392578125,
        140.9589080810547,
        413.7136535644531
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        399.74017333984375,
        140.9589080810547,
        405.6918029785156
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        391.7183532714844,
        140.9589080810547,
        397.66998291015625
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        383.69647216796875,
        140.9589080810547,
        389.6481018066406
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        135.2971649169922,
        375.67462158203125,
        140.9589080810547,
        381.6262512207031
      ],
      "text": "4.0"
    },
    {
      "page_no": 28,
      "bbox": [
        128.5609893798828,
        382.5419921875,
        134.51263427734375,
        437.89483642578125
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        367.97967529296875,
        187.81553649902344,
        374.3736572265625
      ],
      "text": "1e6\nLayer 13"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        446.7978210449219,
        306.2133483886719,
        452.74945068359375
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        439.8494567871094,
        228.86865234375,
        445.80108642578125
      ],
      "text": "0.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        431.4608154296875,
        228.86865234375,
        437.4124450683594
      ],
      "text": "0.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        423.07220458984375,
        228.86865234375,
        429.0238342285156
      ],
      "text": "1.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        414.68359375,
        228.86865234375,
        420.6352233886719
      ],
      "text": "1.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        406.294921875,
        228.86865234375,
        412.2465515136719
      ],
      "text": "2.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        397.90631103515625,
        228.86865234375,
        403.8579406738281
      ],
      "text": "2.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        389.5177001953125,
        228.86865234375,
        395.4693298339844
      ],
      "text": "3.0"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        381.12908935546875,
        228.86865234375,
        387.0807189941406
      ],
      "text": "3.5"
    },
    {
      "page_no": 28,
      "bbox": [
        223.2069091796875,
        372.7404479980469,
        228.86865234375,
        378.69207763671875
      ],
      "text": "4.0"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        367.97967529296875,
        275.72528076171875,
        374.3736572265625
      ],
      "text": "1e6\nLayer 14"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        446.7978210449219,
        394.1230773925781,
        452.74945068359375
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        439.8494567871094,
        316.7784118652344,
        445.80108642578125
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        424.1071472167969,
        316.7784118652344,
        430.05877685546875
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        408.3648681640625,
        316.7784118652344,
        414.3164978027344
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        392.62255859375,
        316.7784118652344,
        398.5741882324219
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        376.8802490234375,
        316.7784118652344,
        382.8318786621094
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        367.97967529296875,
        363.635009765625,
        374.3736572265625
      ],
      "text": "1e6\nLayer 15"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        446.7978210449219,
        482.0328063964844,
        452.74945068359375
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        439.8494567871094,
        404.6881408691406,
        445.80108642578125
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        425.57220458984375,
        404.6881408691406,
        431.5238342285156
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        411.294921875,
        404.6881408691406,
        417.2465515136719
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        397.01763916015625,
        404.6881408691406,
        402.9692687988281
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        382.7403869628906,
        404.6881408691406,
        388.6920166015625
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        367.97967529296875,
        451.54473876953125,
        374.3736572265625
      ],
      "text": "1e6\nLayer 16"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        532.8926391601562,
        218.30357360839844,
        538.84423828125
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        525.9442749023438,
        140.9589080810547,
        531.8958740234375
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        510.16522216796875,
        140.9589080810547,
        516.1168212890625
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        494.38616943359375,
        140.9589080810547,
        500.3377990722656
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        478.60711669921875,
        140.9589080810547,
        484.5587463378906
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        462.82806396484375,
        140.9589080810547,
        468.7796936035156
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        131.95803833007812,
        468.63677978515625,
        137.90966796875,
        523.9896240234375
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        454.07452392578125,
        187.81553649902344,
        460.468505859375
      ],
      "text": "1e6\nLayer 17"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        532.8926391601562,
        306.2133483886719,
        538.84423828125
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        525.9442749023438,
        228.86865234375,
        531.8958740234375
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        510.57611083984375,
        228.86865234375,
        516.5277099609375
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        495.2078857421875,
        228.86865234375,
        501.1595153808594
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        479.8397216796875,
        228.86865234375,
        485.7913513183594
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        464.47149658203125,
        228.86865234375,
        470.4231262207031
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        454.07452392578125,
        275.72528076171875,
        460.468505859375
      ],
      "text": "1e6\nLayer 18"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        532.8926391601562,
        394.1230773925781,
        538.84423828125
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        525.9442749023438,
        316.7784118652344,
        531.8958740234375
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        511.50030517578125,
        316.7784118652344,
        517.451904296875
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        497.0562744140625,
        316.7784118652344,
        503.0079040527344
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        482.6122741699219,
        316.7784118652344,
        488.56390380859375
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        468.16827392578125,
        316.7784118652344,
        474.1199035644531
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        454.07452392578125,
        363.635009765625,
        460.468505859375
      ],
      "text": "1e6\nLayer 19"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        532.8926391601562,
        482.0328063964844,
        538.84423828125
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        525.9442749023438,
        404.6881408691406,
        531.8958740234375
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        512.6267700195312,
        404.6881408691406,
        518.578369140625
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        499.30926513671875,
        404.6881408691406,
        505.2608947753906
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        485.9917297363281,
        404.6881408691406,
        491.943359375
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        472.6742248535156,
        404.6881408691406,
        478.6258544921875
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        459.356689453125,
        404.6881408691406,
        465.3083190917969
      ],
      "text": "5"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        454.07452392578125,
        451.54473876953125,
        460.468505859375
      ],
      "text": "1e6\nLayer 20"
    },
    {
      "page_no": 28,
      "bbox": [
        145.2701873779297,
        618.9874877929688,
        218.30357360839844,
        624.9390869140625
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        612.0391235351562,
        140.9589080810547,
        617.99072265625
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        598.512451171875,
        140.9589080810547,
        604.4640502929688
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        584.98583984375,
        140.9589080810547,
        590.9374389648438
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        571.459228515625,
        140.9589080810547,
        577.4108276367188
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        557.9325561523438,
        140.9589080810547,
        563.8841552734375
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        138.6942138671875,
        544.4059448242188,
        140.9589080810547,
        550.3575439453125
      ],
      "text": "5"
    },
    {
      "page_no": 28,
      "bbox": [
        131.95803833007812,
        554.7315673828125,
        137.90966796875,
        610.08447265625
      ],
      "text": "Accumulated Attention Scores"
    },
    {
      "page_no": 28,
      "bbox": [
        143.0726318359375,
        540.1693115234375,
        187.81553649902344,
        546.5632934570312
      ],
      "text": "1e6\nLayer 21"
    },
    {
      "page_no": 28,
      "bbox": [
        233.179931640625,
        618.9874877929688,
        306.2133483886719,
        624.9390869140625
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        256.920166015625,
        624.3095092773438,
        278.3208312988281,
        630.2611083984375
      ],
      "text": "Word Index"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        612.0391235351562,
        228.86865234375,
        617.99072265625
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        598.806396484375,
        228.86865234375,
        604.7579956054688
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        585.5736083984375,
        228.86865234375,
        591.5252075195312
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        572.3408813476562,
        228.86865234375,
        578.29248046875
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        559.108154296875,
        228.86865234375,
        565.0597534179688
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        226.6039581298828,
        545.8754272460938,
        228.86865234375,
        551.8270263671875
      ],
      "text": "5"
    },
    {
      "page_no": 28,
      "bbox": [
        230.9823760986328,
        540.1693115234375,
        275.72528076171875,
        546.5632934570312
      ],
      "text": "1e6\nLayer 22"
    },
    {
      "page_no": 28,
      "bbox": [
        321.0896911621094,
        618.9874877929688,
        394.1230773925781,
        624.9390869140625
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        344.82989501953125,
        624.3095092773438,
        366.2305603027344,
        630.2611083984375
      ],
      "text": "Word Index"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        612.0391235351562,
        316.7784118652344,
        617.99072265625
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        599.7423706054688,
        316.7784118652344,
        605.6939697265625
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        587.445556640625,
        316.7784118652344,
        593.3971557617188
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        575.1488037109375,
        316.7784118652344,
        581.1004028320312
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        562.85205078125,
        316.7784118652344,
        568.8036499023438
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        314.5137023925781,
        550.5552368164062,
        316.7784118652344,
        556.5068359375
      ],
      "text": "5"
    },
    {
      "page_no": 28,
      "bbox": [
        318.8921203613281,
        540.1693115234375,
        363.635009765625,
        546.5632934570312
      ],
      "text": "1e6\nLayer 23"
    },
    {
      "page_no": 28,
      "bbox": [
        408.9994201660156,
        618.9874877929688,
        482.0328063964844,
        624.9390869140625
      ],
      "text": "0\n10000 20000 30000 40000 50000"
    },
    {
      "page_no": 28,
      "bbox": [
        432.7396240234375,
        624.3095092773438,
        454.1402893066406,
        630.2611083984375
      ],
      "text": "Word Index"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        612.0391235351562,
        404.6881408691406,
        617.99072265625
      ],
      "text": "0"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        597.6588745117188,
        404.6881408691406,
        603.6104736328125
      ],
      "text": "1"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        583.2786254882812,
        404.6881408691406,
        589.230224609375
      ],
      "text": "2"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        568.8984375,
        404.6881408691406,
        574.8500366210938
      ],
      "text": "3"
    },
    {
      "page_no": 28,
      "bbox": [
        402.4234313964844,
        554.5181884765625,
        404.6881408691406,
        560.4697875976562
      ],
      "text": "4"
    },
    {
      "page_no": 28,
      "bbox": [
        406.8018493652344,
        540.1693115234375,
        451.54473876953125,
        546.5632934570312
      ],
      "text": "1e6\nLayer 24"
    },
    {
      "page_no": 28,
      "bbox": [
        108.0,
        639.180419921875,
        504.00079345703125,
        668.0513916015625
      ],
      "text": "Figure 10: The distribution of accumulated attention scores with respect to the corresponding word. The x-axis\nrepresents the word index in the vocabulary, and the y-axis represents the accumulated attention score. Results\nare obtained from OPT-1.3B."
    },
    {
      "page_no": 28,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "28"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        161.8562774658203,
        224.56011962890625,
        172.9683074951172
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        151.7459259033203,
        160.33926391601562,
        157.0485076904297
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        141.6355743408203,
        160.33926391601562,
        146.9381561279297
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        131.5251922607422,
        160.33926391601562,
        136.82777404785156
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        121.4148178100586,
        160.33926391601562,
        126.71739959716797
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        111.3044662475586,
        160.33924865722656,
        116.60704803466797
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        118.88059997558594,
        153.5786895751953,
        159.67092895507812
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        188.63986206054688,
        105.8298568725586,
        201.07711791992188,
        111.13243865966797
      ],
      "text": "Layer 1"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        161.8562774658203,
        302.8829345703125,
        172.9683074951172
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        151.7590789794922,
        238.66207885742188,
        157.06166076660156
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        141.6619110107422,
        238.66207885742188,
        146.96449279785156
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        131.56468200683594,
        238.66207885742188,
        136.8672637939453
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        121.46749114990234,
        238.66207885742188,
        126.77007293701172
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        111.3702621459961,
        238.66207885742188,
        116.67284393310547
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        266.9626770019531,
        105.8298568725586,
        279.3999328613281,
        111.13243865966797
      ],
      "text": "Layer 2"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        161.8562774658203,
        381.20574951171875,
        172.9683074951172
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        151.7590789794922,
        316.98492431640625,
        157.06166076660156
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        141.6619110107422,
        316.98492431640625,
        146.96449279785156
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        131.56468200683594,
        316.98492431640625,
        136.8672637939453
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        121.46749114990234,
        316.98492431640625,
        126.77007293701172
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        111.3702621459961,
        316.9848937988281,
        116.67284393310547
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        345.2854919433594,
        105.8298568725586,
        357.7227478027344,
        111.13243865966797
      ],
      "text": "Layer 3"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        161.8562774658203,
        459.5285949707031,
        172.9683074951172
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        151.70497131347656,
        395.3077392578125,
        157.00755310058594
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        141.5536346435547,
        395.3077392578125,
        146.85621643066406
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        131.4022674560547,
        395.3077392578125,
        136.70484924316406
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        121.25093841552734,
        395.3077392578125,
        126.55352020263672
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        111.09957122802734,
        395.3077392578125,
        116.40215301513672
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        423.6083068847656,
        105.8298568725586,
        436.0455627441406,
        111.13243865966797
      ],
      "text": "Layer 4"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        228.49790954589844,
        224.56011962890625,
        239.60987854003906
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        218.3448944091797,
        160.33926391601562,
        223.64747619628906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        208.19190979003906,
        160.33926391601562,
        213.49449157714844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        198.03892517089844,
        160.33926391601562,
        203.3415069580078
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        187.88597106933594,
        160.33926391601562,
        193.1885528564453
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        177.7329559326172,
        160.33924865722656,
        183.03553771972656
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        185.52215576171875,
        153.5786895751953,
        226.3125
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        188.63986206054688,
        172.47145080566406,
        201.07711791992188,
        177.77403259277344
      ],
      "text": "Layer 5"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        228.49790954589844,
        302.8829345703125,
        239.60987854003906
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        218.3372344970703,
        238.66207885742188,
        223.6398162841797
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        208.1765899658203,
        238.66207885742188,
        213.4791717529297
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        198.0159149169922,
        238.66207885742188,
        203.31849670410156
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        187.85520935058594,
        238.66207885742188,
        193.1577911376953
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        177.69456481933594,
        238.66207885742188,
        182.9971466064453
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        266.9626770019531,
        172.47145080566406,
        279.3999328613281,
        177.77403259277344
      ],
      "text": "Layer 6"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        228.49790954589844,
        381.20574951171875,
        239.60987854003906
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        218.22169494628906,
        316.98492431640625,
        223.52427673339844
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        207.9455108642578,
        316.98492431640625,
        213.2480926513672
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        197.66929626464844,
        316.98492431640625,
        202.9718780517578
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        187.3931121826172,
        316.98492431640625,
        192.69569396972656
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        177.1168975830078,
        316.9848937988281,
        182.4194793701172
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        345.2854919433594,
        172.47145080566406,
        357.7227478027344,
        177.77403259277344
      ],
      "text": "Layer 7"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        228.49790954589844,
        459.5285949707031,
        239.60987854003906
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        218.23350524902344,
        395.3077392578125,
        223.5360870361328
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        207.96910095214844,
        395.3077392578125,
        213.2716827392578
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        197.70469665527344,
        395.3077392578125,
        203.0072784423828
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        187.44032287597656,
        395.3077392578125,
        192.74290466308594
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        177.17588806152344,
        395.3077392578125,
        182.4784698486328
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        423.6083068847656,
        172.47145080566406,
        436.0455627441406,
        177.77403259277344
      ],
      "text": "Layer 8"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        295.1395263671875,
        224.56011962890625,
        306.2514953613281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        283.6346130371094,
        160.33926391601562,
        288.93719482421875
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        272.12969970703125,
        160.33926391601562,
        277.4322814941406
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        260.6247863769531,
        160.33926391601562,
        265.9273681640625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        249.11988830566406,
        160.33926391601562,
        254.42247009277344
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        150.29383850097656,
        252.16375732421875,
        155.59642028808594,
        292.9541015625
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        188.63986206054688,
        239.11305236816406,
        201.07711791992188,
        244.41563415527344
      ],
      "text": "Layer 9"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        295.1395263671875,
        302.8829345703125,
        306.2514953613281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        283.8152770996094,
        238.66207885742188,
        289.11785888671875
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        272.49102783203125,
        238.66207885742188,
        277.7936096191406
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        261.16680908203125,
        238.66207885742188,
        266.4693908691406
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        249.84254455566406,
        238.66207885742188,
        255.14512634277344
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        239.11305236816406,
        280.4088134765625,
        244.41563415527344
      ],
      "text": "Layer 10"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        295.1395263671875,
        381.20574951171875,
        306.2514953613281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        285.04229736328125,
        316.98492431640625,
        290.3448791503906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        274.9450988769531,
        316.98492431640625,
        280.2476806640625
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        264.8478698730469,
        316.98492431640625,
        270.15045166015625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        254.7506561279297,
        316.98492431640625,
        260.0532531738281
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        244.65342712402344,
        316.9848937988281,
        249.9560089111328
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.3438720703125,
        239.11305236816406,
        358.6517333984375,
        244.41563415527344
      ],
      "text": "Layer 11"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        295.1395263671875,
        459.5285949707031,
        306.2514953613281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        285.04229736328125,
        395.3077392578125,
        290.3448791503906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        274.9450988769531,
        395.3077392578125,
        280.2476806640625
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        264.8478698730469,
        395.3077392578125,
        270.15045166015625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        254.7506561279297,
        395.3077392578125,
        260.0532531738281
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        244.65342712402344,
        395.3077392578125,
        249.9560089111328
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        239.11305236816406,
        437.054443359375,
        244.41563415527344
      ],
      "text": "Layer 12"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        361.7810974121094,
        224.56011962890625,
        372.8930969238281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        351.68389892578125,
        160.33926391601562,
        356.9864807128906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        341.586669921875,
        160.33926391601562,
        346.8892517089844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        331.4894714355469,
        160.33926391601562,
        336.79205322265625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        321.3922424316406,
        160.33926391601562,
        326.69482421875
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        311.2950439453125,
        160.33924865722656,
        316.5976257324219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        318.8053894042969,
        153.5786895751953,
        359.5957336425781
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        187.63099670410156,
        305.754638671875,
        202.0859832763672,
        311.0572204589844
      ],
      "text": "Layer 13"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        361.7810974121094,
        302.8829345703125,
        372.8930969238281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        351.68389892578125,
        238.66207885742188,
        356.9864807128906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        341.586669921875,
        238.66207885742188,
        346.8892517089844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        331.4894714355469,
        238.66207885742188,
        336.79205322265625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        321.3922424316406,
        238.66207885742188,
        326.69482421875
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        311.2950439453125,
        238.66207885742188,
        316.5976257324219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        305.754638671875,
        280.4088134765625,
        311.0572204589844
      ],
      "text": "Layer 14"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        361.7810974121094,
        381.20574951171875,
        372.8930969238281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        351.68389892578125,
        316.98492431640625,
        356.9864807128906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        341.586669921875,
        316.98492431640625,
        346.8892517089844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        331.4894714355469,
        316.98492431640625,
        336.79205322265625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        321.3922424316406,
        316.98492431640625,
        326.69482421875
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        311.2950439453125,
        316.9848937988281,
        316.5976257324219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.276611328125,
        305.754638671875,
        358.7315979003906,
        311.0572204589844
      ],
      "text": "Layer 15"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        361.7810974121094,
        459.5285949707031,
        372.8930969238281
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        351.68389892578125,
        395.3077392578125,
        356.9864807128906
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        341.586669921875,
        395.3077392578125,
        346.8892517089844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        331.4894714355469,
        395.3077392578125,
        336.79205322265625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        321.3922424316406,
        395.3077392578125,
        326.69482421875
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        311.2950439453125,
        395.3077392578125,
        316.5976257324219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        305.754638671875,
        437.054443359375,
        311.0572204589844
      ],
      "text": "Layer 16"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        428.4226989746094,
        224.56011962890625,
        439.53466796875
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        418.3254699707031,
        160.33926391601562,
        423.6280517578125
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        408.228271484375,
        160.33926391601562,
        413.5308532714844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        398.1310729980469,
        160.33926391601562,
        403.43365478515625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        388.0338439941406,
        160.33926391601562,
        393.33642578125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        377.9366455078125,
        160.33924865722656,
        383.2392272949219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        385.44696044921875,
        153.5786895751953,
        426.2373046875
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        187.63099670410156,
        372.3962707519531,
        202.0859832763672,
        377.6988525390625
      ],
      "text": "Layer 17"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        428.4226989746094,
        302.8829345703125,
        439.53466796875
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        418.3254699707031,
        238.66207885742188,
        423.6280517578125
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        408.228271484375,
        238.66207885742188,
        413.5308532714844
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        398.1310729980469,
        238.66207885742188,
        403.43365478515625
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        388.0338439941406,
        238.66207885742188,
        393.33642578125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        377.9366455078125,
        238.66207885742188,
        383.2392272949219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        372.3962707519531,
        280.4088134765625,
        377.6988525390625
      ],
      "text": "Layer 18"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        428.4247131347656,
        381.20574951171875,
        439.53466796875
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        418.3259582519531,
        316.98492431640625,
        423.6285400390625
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        408.2271423339844,
        316.98492431640625,
        413.52972412109375
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        398.1283264160156,
        316.98492431640625,
        403.430908203125
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        388.0295715332031,
        316.98492431640625,
        393.3321533203125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        377.9307556152344,
        316.9848937988281,
        383.23333740234375
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.276611328125,
        372.3962707519531,
        358.7315979003906,
        377.6988525390625
      ],
      "text": "Layer 19"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        428.4253234863281,
        459.5285949707031,
        439.53466796875
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        418.3276062011719,
        395.3077392578125,
        423.63018798828125
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        408.2298583984375,
        395.3077392578125,
        413.5324401855469
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        398.1321105957031,
        395.3077392578125,
        403.4346923828125
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        388.0343933105469,
        395.3077392578125,
        393.33697509765625
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        377.9366455078125,
        395.3077392578125,
        383.2392272949219
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        372.3962707519531,
        437.054443359375,
        377.6988525390625
      ],
      "text": "Layer 20"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        495.0689697265625,
        224.56011962890625,
        506.17626953125
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        484.9707946777344,
        160.33926391601562,
        490.27337646484375
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        474.8726501464844,
        160.33926391601562,
        480.17523193359375
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        464.7745056152344,
        160.33926391601562,
        470.07708740234375
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        454.6763916015625,
        160.33926391601562,
        459.9789733886719
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        444.5782165527344,
        160.33924865722656,
        449.88079833984375
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        452.08856201171875,
        153.5786895751953,
        492.87890625
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        187.63099670410156,
        439.0378723144531,
        202.0859832763672,
        444.3404541015625
      ],
      "text": "Layer 21"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        495.0657043457031,
        302.8829345703125,
        506.17626953125
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        484.9668884277344,
        238.66207885742188,
        490.26947021484375
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        474.8680419921875,
        238.66207885742188,
        480.1706237792969
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        464.7691955566406,
        238.66207885742188,
        470.07177734375
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        454.6703796386719,
        238.66207885742188,
        459.97296142578125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        444.571533203125,
        238.66207885742188,
        449.8741149902344
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        439.0378723144531,
        280.4088134765625,
        444.3404541015625
      ],
      "text": "Layer 22"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        495.0768737792969,
        381.20574951171875,
        506.17626953125
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        484.9771423339844,
        316.98492431640625,
        490.27972412109375
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        474.8774108886719,
        316.98492431640625,
        480.17999267578125
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        464.7776794433594,
        316.98492431640625,
        470.08026123046875
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        454.6779479980469,
        316.98492431640625,
        459.98052978515625
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        444.5782165527344,
        316.9848937988281,
        449.88079833984375
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.276611328125,
        439.0378723144531,
        358.7315979003906,
        444.3404541015625
      ],
      "text": "Layer 23"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        495.07220458984375,
        459.5285949707031,
        506.17626953125
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        484.9734191894531,
        395.3077392578125,
        490.2760009765625
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        474.8746032714844,
        395.3077392578125,
        480.17718505859375
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        464.77581787109375,
        395.3077392578125,
        470.0783996582031
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        454.6770324707031,
        395.3077392578125,
        459.9796142578125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        444.5782165527344,
        395.3077392578125,
        449.88079833984375
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        439.0378723144531,
        437.054443359375,
        444.3404541015625
      ],
      "text": "Layer 24"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        561.7153930664062,
        224.56011962890625,
        572.81787109375
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        549.85107421875,
        160.33926391601562,
        555.1536865234375
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        537.9867553710938,
        160.33926391601562,
        543.2893676757812
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        526.1223754882812,
        160.33926391601562,
        531.4249877929688
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        514.258056640625,
        160.33926391601562,
        519.5606689453125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        150.29383850097656,
        518.7301635742188,
        155.59642028808594,
        559.5205078125
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        187.63099670410156,
        505.679443359375,
        202.0859832763672,
        510.9820251464844
      ],
      "text": "Layer 25"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        561.718017578125,
        302.8829345703125,
        572.81787109375
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        551.618408203125,
        238.66207885742188,
        556.9210205078125
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        541.5187377929688,
        238.66207885742188,
        546.8213500976562
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        531.4190673828125,
        238.66207885742188,
        536.7216796875
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        521.3194580078125,
        238.66207885742188,
        526.6220703125
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        511.2198181152344,
        238.66207885742188,
        516.5223999023438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        505.679443359375,
        280.4088134765625,
        510.9820251464844
      ],
      "text": "Layer 26"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        561.7060546875,
        381.20574951171875,
        572.81787109375
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        551.6088256835938,
        316.98492431640625,
        556.9114379882812
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        541.5115356445312,
        316.98492431640625,
        546.8141479492188
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        531.414306640625,
        316.98492431640625,
        536.7169189453125
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        521.3170776367188,
        316.98492431640625,
        526.6196899414062
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        511.2198181152344,
        316.9848937988281,
        516.5223999023438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.276611328125,
        505.679443359375,
        358.7315979003906,
        510.9820251464844
      ],
      "text": "Layer 27"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        561.71337890625,
        459.5285949707031,
        572.81787109375
      ],
      "text": "0\n2500 5000 7500100001250015000\n0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        551.6146240234375,
        395.3077392578125,
        556.917236328125
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        541.5159301757812,
        395.3077392578125,
        546.8185424804688
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        531.417236328125,
        395.3077392578125,
        536.7198486328125
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        521.3184814453125,
        395.3077392578125,
        526.62109375
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        511.2198181152344,
        395.3077392578125,
        516.5223999023438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        505.679443359375,
        437.054443359375,
        510.9820251464844
      ],
      "text": "Layer 28"
    },
    {
      "page_no": 29,
      "bbox": [
        164.1803741455078,
        634.1568603515625,
        224.56011962890625,
        639.45947265625
      ],
      "text": "0\n2500 5000 7500100001250015000"
    },
    {
      "page_no": 29,
      "bbox": [
        183.70693969726562,
        638.8984985351562,
        206.0068359375,
        644.2011108398438
      ],
      "text": "Neuron Index"
    },
    {
      "page_no": 29,
      "bbox": [
        158.321533203125,
        628.3612670898438,
        160.33926391601562,
        633.6638793945312
      ],
      "text": "0"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        618.2612915039062,
        160.33926391601562,
        623.5639038085938
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        608.1613159179688,
        160.33926391601562,
        613.4639282226562
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        598.0613403320312,
        160.33926391601562,
        603.3639526367188
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        156.30380249023438,
        587.9613647460938,
        160.33926391601562,
        593.2639770507812
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        154.28607177734375,
        577.8613891601562,
        160.33924865722656,
        583.1640014648438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        148.27610778808594,
        585.3717651367188,
        153.5786895751953,
        626.162109375
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 29,
      "bbox": [
        187.63099670410156,
        572.321044921875,
        202.0859832763672,
        577.6236572265625
      ],
      "text": "Layer 29"
    },
    {
      "page_no": 29,
      "bbox": [
        242.50318908691406,
        634.1568603515625,
        302.8829345703125,
        639.45947265625
      ],
      "text": "0\n2500 5000 7500100001250015000"
    },
    {
      "page_no": 29,
      "bbox": [
        262.0297546386719,
        638.8984985351562,
        284.32965087890625,
        644.2011108398438
      ],
      "text": "Neuron Index"
    },
    {
      "page_no": 29,
      "bbox": [
        236.64434814453125,
        628.369140625,
        238.66207885742188,
        633.6717529296875
      ],
      "text": "0"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        618.2676391601562,
        238.66207885742188,
        623.5702514648438
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        608.1660766601562,
        238.66207885742188,
        613.4686889648438
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        598.0645141601562,
        238.66207885742188,
        603.3671264648438
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        234.62661743164062,
        587.9629516601562,
        238.66207885742188,
        593.2655639648438
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        232.60890197753906,
        577.8613891601562,
        238.66207885742188,
        583.1640014648438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        265.9538269042969,
        572.321044921875,
        280.4088134765625,
        577.6236572265625
      ],
      "text": "Layer 30"
    },
    {
      "page_no": 29,
      "bbox": [
        320.8260192871094,
        634.1568603515625,
        381.20574951171875,
        639.45947265625
      ],
      "text": "0\n2500 5000 7500100001250015000"
    },
    {
      "page_no": 29,
      "bbox": [
        340.3525695800781,
        638.8984985351562,
        362.6524658203125,
        644.2011108398438
      ],
      "text": "Neuron Index"
    },
    {
      "page_no": 29,
      "bbox": [
        314.9671630859375,
        628.3484497070312,
        316.9848937988281,
        633.6510620117188
      ],
      "text": "0"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        618.2510375976562,
        316.98492431640625,
        623.5536499023438
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        608.1536254882812,
        316.98492431640625,
        613.4562377929688
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        598.0562133789062,
        316.98492431640625,
        603.3588256835938
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        312.949462890625,
        587.9588012695312,
        316.98492431640625,
        593.2614135742188
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        310.93170166015625,
        577.8613891601562,
        316.9848937988281,
        583.1640014648438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        344.276611328125,
        572.321044921875,
        358.7315979003906,
        577.6236572265625
      ],
      "text": "Layer 31"
    },
    {
      "page_no": 29,
      "bbox": [
        399.1488342285156,
        634.1568603515625,
        459.5285949707031,
        639.45947265625
      ],
      "text": "0\n2500 5000 7500100001250015000"
    },
    {
      "page_no": 29,
      "bbox": [
        418.6753845214844,
        638.8984985351562,
        440.97528076171875,
        644.2011108398438
      ],
      "text": "Neuron Index"
    },
    {
      "page_no": 29,
      "bbox": [
        393.28997802734375,
        628.3478393554688,
        395.3077087402344,
        633.6504516601562
      ],
      "text": "0"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        618.2505493164062,
        395.3077392578125,
        623.5531616210938
      ],
      "text": "20"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        608.1532592773438,
        395.3077392578125,
        613.4558715820312
      ],
      "text": "40"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        598.0559692382812,
        395.3077392578125,
        603.3585815429688
      ],
      "text": "60"
    },
    {
      "page_no": 29,
      "bbox": [
        391.27227783203125,
        587.9586791992188,
        395.3077392578125,
        593.2612915039062
      ],
      "text": "80"
    },
    {
      "page_no": 29,
      "bbox": [
        389.2545471191406,
        577.8613891601562,
        395.3077392578125,
        583.1640014648438
      ],
      "text": "100"
    },
    {
      "page_no": 29,
      "bbox": [
        422.5994567871094,
        572.321044921875,
        437.054443359375,
        577.6236572265625
      ],
      "text": "Layer 32"
    },
    {
      "page_no": 29,
      "bbox": [
        108.0,
        652.7005004882812,
        504.0014343261719,
        671.83740234375
      ],
      "text": "Figure 11: The emergence of H2 in MLP blocks of OPT-6.7B. The x-axis represents the index of neurons in the\nhidden layers of MLP blocks, and the y-axis represents the activated frequency."
    },
    {
      "page_no": 29,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "29"
    },
    {
      "page_no": 30,
      "bbox": [
        107.50199890136719,
        74.17692565917969,
        504.3479919433594,
        184.04605102539062
      ],
      "text": "Elimination of H2.\nWe first train a GPT-2 using Wiki-Text-103 dataset and subsequently identify\nand prune the neurons exhibiting an activation frequency exceeding 20% (i.e., H2). This pruning\noperation leads to a substantial decline in performance, as evidenced by an increase in perplexity\nfrom 19.32 to 31.78. The results emphasize the criticality of H2 in preserving the functionality of the\nmodel. To assess the recoverability of the discarded information, we conduct a few-shot fine-tuning\nexperiment, and the results are summarized in Table 12. The pruned model is fine-tuned with varying\nratios of training data for 500 iterations, and it successfully regains performance levels equivalent\nto those of the pre-trained model. In contrast, when training the model from scratch using only\n1% of the training data, the resulting model achieves a perplexity of 554.12 only. These findings\ndemonstrate that the knowledge encoded in H2 can be easily restored."
    },
    {
      "page_no": 30,
      "bbox": [
        189.79803466796875,
        201.1964569091797,
        421.9246520996094,
        210.370361328125
      ],
      "text": "Table 12: . Perplexity on the test-set of Wiki-Text-3 with GPT-2."
    },
    {
      "page_no": 30,
      "bbox": [
        229.80015563964844,
        214.5537109375,
        398.44659423828125,
        223.45643615722656
      ],
      "text": "Settings\n1%\n10%\n40%\n100%"
    },
    {
      "page_no": 30,
      "bbox": [
        213.25515747070312,
        228.8336181640625,
        352.158203125,
        247.87062072753906
      ],
      "text": "Pretrained Model\n19.32\nRemove H2\n31.78"
    },
    {
      "page_no": 30,
      "bbox": [
        223.4258575439453,
        252.78173828125,
        398.21478271484375,
        261.6844482421875
      ],
      "text": "Fine-tuning\n19.86\n19.84\n19.76\n19.83"
    },
    {
      "page_no": 30,
      "bbox": [
        108.0,
        284.3824462890625,
        504.00372314453125,
        338.1480712890625
      ],
      "text": "Overlap across Diverse Input Contents.\nMoreover, we conduct a comparative analysis of the\nactivation frequencies acquired from various input contents. Specifically, utilizing the pretrained\nOPT-1.3B model, we evaluate three datasets, namely Wiki-Text-103, Penn Treebank, and Amazon\nReview. The positioning of H2 is depicted in Figure 12, revealing significant concurrence across\nmultiple datasets."
    },
    {
      "page_no": 30,
      "bbox": [
        357.48199462890625,
        472.6163330078125,
        503.9984436035156,
        511.53839111328125
      ],
      "text": "Figure 13: The distribution of activated\nfrequency during training. Experiments\nare conducted with different checkpoints\nof OPT-2.7B during training."
    },
    {
      "page_no": 30,
      "bbox": [
        107.7509994506836,
        351.1324462890625,
        349.1742248535156,
        437.62506103515625
      ],
      "text": "Early-Bird Property.\nFurthermore, our investigation re-\nveals that H2 displays an \"early-bird\" characteristic, as illus-\ntrated in Figure 13. By visualizing the distribution of activa-\ntion frequencies across various checkpoints throughout the\ntraining process, we observe the emergence of a power-law\nbehavior at an initial stage, specifically as early as a train-\ning budget of 4%. Subsequently, the positions of H2 exhibit\ngradual and minimal changes."
    },
    {
      "page_no": 30,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "30"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        222.79981994628906,
        207.47256469726562,
        236.23797607421875
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        211.97129821777344,
        123.92542266845703,
        218.59957885742188
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        201.14610290527344,
        123.92542266845703,
        207.77438354492188
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        190.32093811035156,
        123.92542266845703,
        196.94921875
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        179.4923858642578,
        123.92542266845703,
        186.12066650390625
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        168.66722106933594,
        123.92545318603516,
        175.29550170898438
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        173.58547973632812,
        115.47468566894531,
        224.57376098632812
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        159.30137634277344,
        162.2306365966797,
        174.84805297851562,
        168.85891723632812
      ],
      "text": "Layer 1"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        172.31932067871094,
        178.705322265625,
        193.20968627929688
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        222.79981994628906,
        305.37701416015625,
        236.23797607421875
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        211.9679412841797,
        221.8297882080078,
        218.59622192382812
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        201.1360321044922,
        221.8297882080078,
        207.76431274414062
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        190.30076599121094,
        221.8297882080078,
        196.92904663085938
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        179.46885681152344,
        221.8297882080078,
        186.09713745117188
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        168.63697814941406,
        221.82980346679688,
        175.2652587890625
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        257.205810546875,
        162.2306365966797,
        272.75250244140625,
        168.85891723632812
      ],
      "text": "Layer 2"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        172.31932067871094,
        276.6097717285156,
        193.20968627929688
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        222.79981994628906,
        403.2811279296875,
        236.23797607421875
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        211.97804260253906,
        319.73394775390625,
        218.6063232421875
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        201.1562042236328,
        319.73394775390625,
        207.78448486328125
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        190.3377227783203,
        319.73394775390625,
        196.96600341796875
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        179.5159454345703,
        319.73394775390625,
        186.14422607421875
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        312.1673583984375,
        168.69410705566406,
        319.73388671875,
        175.3223876953125
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        355.10992431640625,
        162.2306365966797,
        370.6566162109375,
        168.85891723632812
      ],
      "text": "Layer 3"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        172.31932067871094,
        374.51385498046875,
        193.20968627929688
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        222.79981994628906,
        501.1868896484375,
        236.23797607421875
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        211.9208526611328,
        417.63836669921875,
        218.54913330078125
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        201.04188537597656,
        417.63836669921875,
        207.670166015625
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        190.1628875732422,
        417.63836669921875,
        196.79116821289062
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        179.2838897705078,
        417.63836669921875,
        185.91217041015625
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        410.0718078613281,
        168.4082794189453,
        417.6383361816406,
        175.03656005859375
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        453.0133361816406,
        162.2306365966797,
        468.5600280761719,
        168.85891723632812
      ],
      "text": "Layer 4"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        172.31932067871094,
        472.4183044433594,
        193.20968627929688
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        294.200927734375,
        207.47256469726562,
        307.64007568359375
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        283.35760498046875,
        123.92542266845703,
        289.9858703613281
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        272.51458740234375,
        123.92542266845703,
        279.1428527832031
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        261.6712646484375,
        123.92542266845703,
        268.2995300292969
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        250.82960510253906,
        123.92542266845703,
        257.4578857421875
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        239.98423767089844,
        123.92545318603516,
        246.61251831054688
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        244.98858642578125,
        115.47468566894531,
        295.97686767578125
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        159.30137634277344,
        233.6350860595703,
        174.84805297851562,
        240.26336669921875
      ],
      "text": "Layer 5"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        243.72377014160156,
        178.705322265625,
        264.6111145019531
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        294.200927734375,
        305.37701416015625,
        307.64007568359375
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        283.2748718261719,
        221.8297882080078,
        289.90313720703125
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        272.3491516113281,
        221.8297882080078,
        278.9774169921875
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        261.423095703125,
        221.8297882080078,
        268.0513610839844
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        250.49668884277344,
        221.8297882080078,
        257.1249694824219
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        239.5706024169922,
        221.82980346679688,
        246.19888305664062
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        257.205810546875,
        233.6350860595703,
        272.75250244140625,
        240.26336669921875
      ],
      "text": "Layer 6"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        243.72377014160156,
        276.6097717285156,
        264.6111145019531
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        294.200927734375,
        403.2811279296875,
        307.64007568359375
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        279.7377624511719,
        319.73394775390625,
        286.36602783203125
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        265.2749328613281,
        319.73394775390625,
        271.9031982421875
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        250.8127899169922,
        319.73394775390625,
        257.4410705566406
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        355.10992431640625,
        233.6350860595703,
        370.6566162109375,
        240.26336669921875
      ],
      "text": "Layer 7"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        243.72377014160156,
        374.51385498046875,
        264.6111145019531
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        294.200927734375,
        501.1868896484375,
        307.64007568359375
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        280.5818786621094,
        417.63836669921875,
        287.21014404296875
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        266.963134765625,
        417.63836669921875,
        273.5914001464844
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        253.3450469970703,
        417.63836669921875,
        259.97332763671875
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        239.72532653808594,
        417.63836669921875,
        246.35360717773438
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        453.0133361816406,
        233.6350860595703,
        468.5600280761719,
        240.26336669921875
      ],
      "text": "Layer 8"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        243.72377014160156,
        472.4183044433594,
        264.6111145019531
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        365.60302734375,
        207.47256469726562,
        379.0425109863281
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        354.7559814453125,
        123.92542266845703,
        361.3842468261719
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        343.9089660644531,
        123.92542266845703,
        350.5372314453125
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        333.0619201660156,
        123.92542266845703,
        339.690185546875
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        322.21490478515625,
        123.92542266845703,
        328.8431701660156
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        311.3678894042969,
        123.92545318603516,
        317.99615478515625
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        316.3906555175781,
        115.47468566894531,
        367.37896728515625
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        159.30137634277344,
        305.0358581542969,
        174.84805297851562,
        311.66412353515625
      ],
      "text": "Layer 9"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        315.12457275390625,
        178.705322265625,
        336.0135498046875
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        365.60302734375,
        305.37701416015625,
        379.0425109863281
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        354.78460693359375,
        221.8297882080078,
        361.4128723144531
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        343.9658203125,
        221.8297882080078,
        350.5940856933594
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        333.1473388671875,
        221.8297882080078,
        339.7756042480469
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        322.32891845703125,
        221.8297882080078,
        328.9571838378906
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        311.5101318359375,
        221.82980346679688,
        318.1383972167969
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        255.94473266601562,
        305.0358581542969,
        274.01361083984375,
        311.66412353515625
      ],
      "text": "Layer 10"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        315.12457275390625,
        276.6097717285156,
        336.0135498046875
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        365.60302734375,
        403.2811279296875,
        379.0425109863281
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        354.7825927734375,
        319.73394775390625,
        361.4108581542969
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        343.96209716796875,
        319.73394775390625,
        350.5903625488281
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        333.1416320800781,
        319.73394775390625,
        339.7698974609375
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        322.3215026855469,
        319.73394775390625,
        328.94976806640625
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        312.1673583984375,
        311.50103759765625,
        319.73388671875,
        318.1293029785156
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        353.93292236328125,
        305.0358581542969,
        371.8179016113281,
        311.66412353515625
      ],
      "text": "Layer 11"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        315.12457275390625,
        374.51385498046875,
        336.0135498046875
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        365.60302734375,
        501.1868896484375,
        379.0425109863281
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        354.7835693359375,
        417.63836669921875,
        361.4118347167969
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        343.964111328125,
        417.63836669921875,
        350.5923767089844
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        333.1446533203125,
        417.63836669921875,
        339.7729187011719
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        322.3252258300781,
        417.63836669921875,
        328.9534912109375
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        410.0718078613281,
        311.5057373046875,
        417.6383361816406,
        318.1340026855469
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        451.7522888183594,
        305.0358581542969,
        469.8211669921875,
        311.66412353515625
      ],
      "text": "Layer 12"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        315.12457275390625,
        472.4183044433594,
        336.0135498046875
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        437.0054931640625,
        207.47256469726562,
        450.4446105957031
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        426.18670654296875,
        123.92542266845703,
        432.8149719238281
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        415.3682556152344,
        123.92542266845703,
        421.99652099609375
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        404.5498046875,
        123.92542266845703,
        411.1780700683594
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        393.73101806640625,
        123.92542266845703,
        400.3592834472656
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        382.91253662109375,
        123.92545318603516,
        389.5408020019531
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        387.7930908203125,
        115.47468566894531,
        438.7814025878906
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        158.040283203125,
        376.4379577636719,
        176.10914611816406,
        383.06622314453125
      ],
      "text": "Layer 13"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        386.52667236328125,
        178.705322265625,
        407.4156188964844
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        437.0054931640625,
        305.37701416015625,
        450.4446105957031
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        426.18670654296875,
        221.8297882080078,
        432.8149719238281
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        415.3682556152344,
        221.8297882080078,
        421.99652099609375
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        404.5498046875,
        221.8297882080078,
        411.1780700683594
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        393.73101806640625,
        221.8297882080078,
        400.3592834472656
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        382.91253662109375,
        221.82980346679688,
        389.5408020019531
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        255.94473266601562,
        376.4379577636719,
        274.01361083984375,
        383.06622314453125
      ],
      "text": "Layer 14"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        386.52667236328125,
        276.6097717285156,
        407.4156188964844
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        437.0054931640625,
        403.2811279296875,
        450.4446105957031
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        426.18670654296875,
        319.73394775390625,
        432.8149719238281
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        415.367919921875,
        319.73394775390625,
        421.9961853027344
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        404.54913330078125,
        319.73394775390625,
        411.1773986816406
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        393.7303466796875,
        319.73394775390625,
        400.3586120605469
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        312.1673583984375,
        382.91156005859375,
        319.73388671875,
        389.5398254394531
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        353.8488464355469,
        376.4379577636719,
        371.917724609375,
        383.06622314453125
      ],
      "text": "Layer 15"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        386.52667236328125,
        374.51385498046875,
        407.4156188964844
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        437.0054931640625,
        501.1868896484375,
        450.4446105957031
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        426.1856689453125,
        417.63836669921875,
        432.8139343261719
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        415.3662109375,
        417.63836669921875,
        421.9944763183594
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        404.5467529296875,
        417.63836669921875,
        411.1750183105469
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        393.72698974609375,
        417.63836669921875,
        400.3552551269531
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        410.0718078613281,
        382.90753173828125,
        417.6383361816406,
        389.5357971191406
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        451.7522888183594,
        376.4379577636719,
        469.8211669921875,
        383.06622314453125
      ],
      "text": "Layer 16"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        386.52667236328125,
        472.4183044433594,
        407.4156188964844
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        508.4075927734375,
        207.47256469726562,
        521.8467407226562
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        497.5891418457031,
        123.92542266845703,
        504.2174072265625
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        486.77069091796875,
        123.92542266845703,
        493.3989562988281
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        475.951904296875,
        123.92542266845703,
        482.5801696777344
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        465.1334228515625,
        123.92542266845703,
        471.7616882324219
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        454.31500244140625,
        123.92545318603516,
        460.9432678222656
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        459.1952209472656,
        115.47468566894531,
        510.18353271484375
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        158.040283203125,
        447.840087890625,
        176.10914611816406,
        454.4683532714844
      ],
      "text": "Layer 17"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        457.92877197265625,
        178.705322265625,
        478.8180847167969
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        508.4075927734375,
        305.37701416015625,
        521.8467407226562
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        497.5891418457031,
        221.8297882080078,
        504.2174072265625
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        486.77069091796875,
        221.8297882080078,
        493.3989562988281
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        475.951904296875,
        221.8297882080078,
        482.5801696777344
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        465.1334228515625,
        221.8297882080078,
        471.7616882324219
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        454.31500244140625,
        221.82980346679688,
        460.9432678222656
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        255.94473266601562,
        447.840087890625,
        274.01361083984375,
        454.4683532714844
      ],
      "text": "Layer 18"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        457.92877197265625,
        276.6097717285156,
        478.8180847167969
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        508.4075927734375,
        403.2811279296875,
        521.8467407226562
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        497.5891418457031,
        319.73394775390625,
        504.2174072265625
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        486.77069091796875,
        319.73394775390625,
        493.3989562988281
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        475.951904296875,
        319.73394775390625,
        482.5801696777344
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        465.1334228515625,
        319.73394775390625,
        471.7616882324219
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        312.1673583984375,
        454.31500244140625,
        319.73388671875,
        460.9432678222656
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        353.8488464355469,
        447.840087890625,
        371.917724609375,
        454.4683532714844
      ],
      "text": "Layer 19"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        457.92877197265625,
        374.51385498046875,
        478.8180847167969
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        508.4075927734375,
        501.1868896484375,
        521.8467407226562
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        497.5891418457031,
        417.63836669921875,
        504.2174072265625
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        486.77069091796875,
        417.63836669921875,
        493.3989562988281
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        475.951904296875,
        417.63836669921875,
        482.5801696777344
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        465.1334228515625,
        417.63836669921875,
        471.7616882324219
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        410.0718078613281,
        454.31500244140625,
        417.6383361816406,
        460.9432678222656
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        451.7522888183594,
        447.840087890625,
        469.8211669921875,
        454.4683532714844
      ],
      "text": "Layer 20"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        457.92877197265625,
        472.4183044433594,
        478.8180847167969
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        121.40324401855469,
        579.809814453125,
        207.47256469726562,
        593.2491455078125
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        568.9912719726562,
        123.92542266845703,
        575.6195678710938
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        558.1727905273438,
        123.92542266845703,
        564.8010864257812
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        547.3543090820312,
        123.92542266845703,
        553.9826049804688
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        118.88106536865234,
        536.535888671875,
        123.92542266845703,
        543.1641845703125
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        116.35892486572266,
        525.7171020507812,
        123.92545318603516,
        532.3453979492188
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        108.8464126586914,
        530.5974731445312,
        115.47468566894531,
        581.5857543945312
      ],
      "text": "Activated Frequency (%)"
    },
    {
      "page_no": 31,
      "bbox": [
        158.040283203125,
        519.2424926757812,
        176.10914611816406,
        525.8707885742188
      ],
      "text": "Layer 21"
    },
    {
      "page_no": 31,
      "bbox": [
        144.9434814453125,
        529.3312377929688,
        178.705322265625,
        550.22021484375
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        219.3076171875,
        579.809814453125,
        305.37701416015625,
        593.2491455078125
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        568.9912719726562,
        221.8297882080078,
        575.6195678710938
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        558.1727905273438,
        221.8297882080078,
        564.8010864257812
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        547.3543090820312,
        221.8297882080078,
        553.9826049804688
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        216.7854461669922,
        536.535888671875,
        221.8297882080078,
        543.1641845703125
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        214.26327514648438,
        525.7171020507812,
        221.82980346679688,
        532.3453979492188
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        255.94473266601562,
        519.2424926757812,
        274.01361083984375,
        525.8707885742188
      ],
      "text": "Layer 22"
    },
    {
      "page_no": 31,
      "bbox": [
        242.84791564941406,
        529.3312377929688,
        276.6097717285156,
        550.22021484375
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        317.21173095703125,
        579.809814453125,
        403.2811279296875,
        593.2491455078125
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        568.9889526367188,
        319.73394775390625,
        575.6172485351562
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        558.1680908203125,
        319.73394775390625,
        564.79638671875
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        547.3472900390625,
        319.73394775390625,
        553.9755859375
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        314.6895751953125,
        536.5264892578125,
        319.73394775390625,
        543.15478515625
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        312.1673583984375,
        525.705322265625,
        319.73388671875,
        532.3336181640625
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        353.8488464355469,
        519.2424926757812,
        371.917724609375,
        525.8707885742188
      ],
      "text": "Layer 23"
    },
    {
      "page_no": 31,
      "bbox": [
        340.75201416015625,
        529.3312377929688,
        374.51385498046875,
        550.22021484375
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        415.1161804199219,
        579.809814453125,
        501.1868896484375,
        593.2491455078125
      ],
      "text": "0\n2000\n4000\n6000\n8000\n0"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        568.9696655273438,
        417.63836669921875,
        575.5979614257812
      ],
      "text": "20"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        558.1294555664062,
        417.63836669921875,
        564.7577514648438
      ],
      "text": "40"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        547.2894287109375,
        417.63836669921875,
        553.917724609375
      ],
      "text": "60"
    },
    {
      "page_no": 31,
      "bbox": [
        412.593994140625,
        536.4490966796875,
        417.63836669921875,
        543.077392578125
      ],
      "text": "80"
    },
    {
      "page_no": 31,
      "bbox": [
        410.0718078613281,
        525.6088256835938,
        417.6383361816406,
        532.2371215820312
      ],
      "text": "100"
    },
    {
      "page_no": 31,
      "bbox": [
        451.7522888183594,
        519.2424926757812,
        469.8211669921875,
        525.8707885742188
      ],
      "text": "Layer 24"
    },
    {
      "page_no": 31,
      "bbox": [
        438.6564636230469,
        529.3312377929688,
        472.4183044433594,
        550.22021484375
      ],
      "text": "Wiki-Text-103\nAmazon Review\nPenn Treebank"
    },
    {
      "page_no": 31,
      "bbox": [
        108.0,
        602.3605346679688,
        504.2210998535156,
        621.275390625
      ],
      "text": "Figure 12: The distribution of activated frequency across diverse input content. The x-axis represents the index\nof neurons, which is ordered by the activated frequency from Wiki-Text-103."
    },
    {
      "page_no": 31,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "31"
    },
    {
      "page_no": 32,
      "bbox": [
        108.0,
        72.78716278076172,
        232.9796600341797,
        84.74236297607422
      ],
      "text": "D\nTheoretical Analysis"
    },
    {
      "page_no": 32,
      "bbox": [
        107.25299835205078,
        105.25838470458984,
        505.24481201171875,
        235.14505004882812
      ],
      "text": "Recently, a number of works have studied the attention scheme in LLMs from a theoretical perspective\n[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n117, 118, 119, 120, 121, 122]. In this work, we provide a different and novel angle compared to the\nprevious work. We present the concept of the submodular property and propose an eviction policy,\nknown as greedy H2, which is a modification of dynamic submodular maximization. Furthermore,\nassuming the attention scheme to be submodular, we establish that constructing the set Si without\nany cache size limitation satisfies the near-optimal property in terms of submodularity. We provide\ntheoretical guarantees for our robust and approximate greedy eviction policy algorithm (Algorithm\n2). Due to space limitation, we only give informal description of algorithm (Algorithm 2) in Section\n4.1 In Section D.6, we give an algorithm (Algorithm 2) which has full and complete implementation\ndetails for Algorithm 1. We also offer a mathematical formulation for sparsity preservation that is\nobserved in Section 3 and proposed an algorithm (Algorithm 4) to solve the problem."
    },
    {
      "page_no": 32,
      "bbox": [
        107.64099884033203,
        241.5262908935547,
        505.74072265625,
        469.716064453125
      ],
      "text": "Specifically, in Section D.1, we provide several basic definitions and notations. In Section D.2, we\nbriefly the definition of submodular function. In Section D.3, we define the dynamic submodular\nframework, which gives the formal version of Definition 4.1. In Section D.4, we briefly review the\nstatic attention computation problem. In Section D.5, we formulate the attention computation in\nrecursive fashion. In Section D.6, we briefly review our eviction policy, which gives the formal\nversion of Definition 4.3. In Section D.7, we discuss the diminishing return for submodular. In\nSection D.8, we discuss the high-level ideas for submodular. In Section D.9, we analyze the robust\ngreedy algorithm error propagation. In Section D.10, we explain how to add items into sets via\napproximate function. In Section D.11, we provide several definitions related to dynamic properties.\nIn Section D.12, we prove an induction lemma for the exact function. In Section D.13, we prove an\ninduction lemma for the approximation function. In Section D.14, we provide theoretical guarantees\nfor both the full-knowledge version (formal version of Lemma 3.1) and the limited-cache-size\nversion (formal version of Theorem 4.4). In Section D.15, we provide a more detailed discussion\nof theoretical work about attention computation and regression-related problems. In Section D.16,\nwe provide a mathematical formulation for sparsity preserving. In Section D.17, we provide the\ndefinition of loss function which can potentially generate sparse (heavy hitter type attention sore). In\nSection D.18, we explain how to compute the gradient of the loss function. In Section D.19, we show\nhow to compute the Hessian of the loss function. In Section D.20, we show that Hessian is positive\ndefinite. In Section D.21, we prove the Lipschitz property for the Hessian matrix. In Section D.22,\nwe show that using a gradient-type algorithm is sufficient to optimize that (heavy hitter type) loss\nfunction."
    },
    {
      "page_no": 32,
      "bbox": [
        108.0,
        496.8905029296875,
        173.38453674316406,
        506.85308837890625
      ],
      "text": "D.1\nNotations"
    },
    {
      "page_no": 32,
      "bbox": [
        108.0,
        521.8999633789062,
        302.34661865234375,
        539.18505859375
      ],
      "text": "For a positive integer n, let [n] := {1, 2, · · · , n}."
    },
    {
      "page_no": 32,
      "bbox": [
        107.99993896484375,
        531.1119995117188,
        504.0030517578125,
        592.9100952148438
      ],
      "text": "For a vector x ∈Rn, let √x ∈Rn denote the vector with the i-th entry being √xi and diag(x) ∈\nRn×n denote the diagonal matrix with the i-th digonal entry being xi. For two matrices A, W ∈\nRn×n, let ∥A∥W := (Pn\ni=1\nPn\nj=1 Wi,jA2\ni,j)1/2 and W ◦A denote the matrix where (W ◦A)i,j =\nWi,jAi,j. For matrix W ∈Rn×n, let DWi := diag(Wi,:) with i ∈[n]."
    },
    {
      "page_no": 32,
      "bbox": [
        108.0,
        590.271240234375,
        504.00079345703125,
        628.8116455078125
      ],
      "text": "For two vectors x ∈Rn and w ∈Rn\n≥0, let ∥x∥w := (Pn\ni=1 wix2\ni )1/2. For a vector x, its ℓ2 norm\nis defined as ∥x∥2 := (Pn\ni=1 x2\ni )1/2 and its ℓp norm is defined as ∥x∥p := (Pn\ni=1 |xi|p)1/p. For a\nsquare matrix A, we denote tr[A] as the trace of matrix A."
    },
    {
      "page_no": 32,
      "bbox": [
        108.0,
        632.499267578125,
        504.0019226074219,
        669.0841064453125
      ],
      "text": "For a matrix A ∈Rn×k (suppose n ≥k), we use ∥A∥to denote its spectral norm, i.e., ∥A∥=\nsupx ∥Ax∥2/∥x∥2. We use ∥A∥F to denote its Frobenius norm ∥A∥F := (Pn\ni=1\nPk\nj=1 A2\ni,j)1/2."
    },
    {
      "page_no": 32,
      "bbox": [
        107.99996948242188,
        665.1172485351562,
        504.00189208984375,
        723.9010620117188
      ],
      "text": "Suppose matrix A ∈Rn×k has SVD decomposition UΣV ⊤where U ∈Rn×k (this matrix has\northonormal columns), Σ ∈Rk×k is a diagonal matrix, and V ∈Rk×k. We call columns of U\nsingular vectors. We use A† ∈Rk×n to denote the Moore-Penrose pseudoinverse, then A† =\nV Σ−1U ⊤. Suppose Σ ∈Rk×k is sorted diagonal matrix, let σ1, · · · , σk denote the diagonal entries\nof Σ. Then we call σi the i-th singular value of the matrix, and we write it as σi(A)."
    },
    {
      "page_no": 32,
      "bbox": [
        301.0189208984375,
        742.3324584960938,
        310.98150634765625,
        752.2950439453125
      ],
      "text": "32"
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        72.00041198730469,
        504.68304443359375,
        107.01847076416016
      ],
      "text": "For any symmetric matrix B ∈Rk×k, we denote its eigenvalue decomposition as UΛU ⊤, where Λ\nis a diagonal matrix. Let λ1, · · · , λk denote the entries on diagonal of Λ ∈Rk×k. We say λi is the\ni-th eigenvalue. Usually, we write it as λi(B)."
    },
    {
      "page_no": 33,
      "bbox": [
        107.69099426269531,
        112.61347961425781,
        342.25042724609375,
        122.57608032226562
      ],
      "text": "The connection between eigenvalues and singular values is"
    },
    {
      "page_no": 33,
      "bbox": [
        267.2149963378906,
        130.0425262451172,
        344.7854309082031,
        144.5626678466797
      ],
      "text": "σ2\ni (A) = λi(A⊤A)"
    },
    {
      "page_no": 33,
      "bbox": [
        107.53199768066406,
        155.60699462890625,
        505.2431640625,
        183.8011016845703
      ],
      "text": "We use the notation A ⪰0 to denote that matrix A is positive semidefinite (psd). Mathematically,\nA ⪰0 means for all vectors x, we have x⊤Ax ≥0."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        182.90496826171875,
        503.99822998046875,
        211.0990753173828
      ],
      "text": "Similarly, for two squarer matrices A and B, we use A ⪰B to denote the case where for all vectors\nx, x⊤Ax ≥x⊤Bx."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        207.9603729248047,
        504.00421142578125,
        242.2316131591797
      ],
      "text": "Let Pr[] and E[] denote the probability and expectation. We define the maximum between a and b as\nmax{a, b}. We denote min{a, b} (resp. max{a, b}) as the minimum (reps. maximum) between a\nand b."
    },
    {
      "page_no": 33,
      "bbox": [
        107.69100189208984,
        248.40899658203125,
        504.0030517578125,
        276.60308837890625
      ],
      "text": "Throughout, for non-negative real numbers a and b, we use the notation a = (1 ± ϵ)b if a ∈\n[(1 −ϵ)b, (1 + ϵ)b]."
    },
    {
      "page_no": 33,
      "bbox": [
        108.00000762939453,
        284.25152587890625,
        185.5787353515625,
        294.214111328125
      ],
      "text": "D.2\nSubmodular"
    },
    {
      "page_no": 33,
      "bbox": [
        107.5320053100586,
        304.47845458984375,
        358.1112365722656,
        314.4410705566406
      ],
      "text": "We provide the standard definition of the submodular function."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        319.397216796875,
        504.0000305175781,
        351.15460205078125
      ],
      "text": "Definition D.1 (Submodular function [123]). For a finite set Ω, a submodular function is defined as\nf : 2Ω→R, where 2Ωdenotes the power set of Ω. It is characterized by the fulfillment of any of the\nfollowing equivalent criteria:"
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        361.7440185546875,
        193.35992431640625,
        371.8830871582031
      ],
      "text": "• Condition 1."
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        380.22607421875,
        504.0019226074219,
        408.50909423828125
      ],
      "text": "– For S, T, ⊆Ωwith S ⊆T and every a ∈Ω\\T we have that f(S ∪{a}) −f(S) ≥\nf(T ∪{a}) −f(T)"
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        409.7720031738281,
        193.35992431640625,
        419.91107177734375
      ],
      "text": "• Condition 2."
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        428.33099365234375,
        455.79266357421875,
        445.6270751953125
      ],
      "text": "– For every S, T ⊆Ωwe have that f(S) + f(T) ≥f(S ∪T) + f(S ∩T)."
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        446.8899841308594,
        193.35992431640625,
        457.029052734375
      ],
      "text": "• Condition 3."
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        465.373046875,
        505.9342041015625,
        493.65509033203125
      ],
      "text": "– For every S ⊆Ωand a1, a2 ∈Ω\\S such that a1 ̸= a2 we have that f(S ∪{a1}) +\nf(S ∪{a2}) ≥f(S ∪{a1, a2}) + f(S)."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        497.1481628417969,
        504.0020751953125,
        517.9590454101562
      ],
      "text": "For convenience of discussion, in this paper, we always choose Ω= [n] when we want to discuss the\nsubmodular function."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        524.3092651367188,
        503.9974670410156,
        545.2560424804688
      ],
      "text": "Next, we provide some examples/types of submodular functions. One important class is called\nmonotone,"
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        550.2009887695312,
        505.7432556152344,
        567.486083984375
      ],
      "text": "Definition D.2 (Monotone). A set function f is monotone if for every T ⊆S we have f(T) ≤f(S)."
    },
    {
      "page_no": 33,
      "bbox": [
        108.0,
        570.91748046875,
        324.08868408203125,
        580.8800659179688
      ],
      "text": "Here are a number of monotone submodular functions"
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        592.5554809570312,
        252.8759765625,
        602.51806640625
      ],
      "text": "• Linear (Modular) functions"
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        607.2662963867188,
        372.76751708984375,
        624.700927734375
      ],
      "text": "– A linear function can be represented as f(S) = P"
    },
    {
      "page_no": 33,
      "bbox": [
        163.79100036621094,
        607.730224609375,
        503.9996643066406,
        628.7606811523438
      ],
      "text": "i∈S wi. If all the weights wi are\nnonnegative, then the function f is considered monotone."
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        633.9654541015625,
        247.23712158203125,
        643.9280395507812
      ],
      "text": "• Budget-additive functions"
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        648.6773071289062,
        504.0042724609375,
        671.58203125
      ],
      "text": "– A budget-additive function has the form f(S) = min B, P\ni∈S wi where each weight\nwi and the budget B are nonnegative."
    },
    {
      "page_no": 33,
      "bbox": [
        135.39700317382812,
        675.37548828125,
        221.31443786621094,
        685.3380737304688
      ],
      "text": "• Coverage functions"
    },
    {
      "page_no": 33,
      "bbox": [
        153.8280029296875,
        689.1124877929688,
        505.74346923828125,
        722.4070434570312
      ],
      "text": "– We have a set Ω= {E1, E2, · · · , En} where each Ei is a subset of a broader set Ω′.\nThe coverage function can be expressed as f(S) = | ∪Ei∈S Ei| for any S ⊂Ω. This\nfunction can be generalized by assigning non-negative weights to the elements."
    },
    {
      "page_no": 33,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "33"
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        74.31652069091797,
        226.25601196289062,
        84.27912139892578
      ],
      "text": "D.3\nDynamic Submodular"
    },
    {
      "page_no": 34,
      "bbox": [
        107.69100189208984,
        90.70997619628906,
        504.0047302246094,
        123.50110626220703
      ],
      "text": "The standard submodular function is mapping 2[n] to real. Here we need a more general definition\nthat maps 2[n] × 2[n] to real."
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        118.25896453857422,
        503.9991760253906,
        151.15809631347656
      ],
      "text": "Definition D.3 (Strong submodular). We define function F : 2[n] × 2[n] →R, then for any set\nZ ⊂[n], we assume that F(Z, ·) : 2[n] →R is a submodular function."
    },
    {
      "page_no": 34,
      "bbox": [
        107.53199768066406,
        154.09503173828125,
        505.74700927734375,
        200.423583984375
      ],
      "text": "In fact, the above definition is stronger than we want, what we really need can be written as follows.\nWe remark that in Definition 4.1 we provide an informal definition. Here we provide a more detailed\nversion of the definition.\nDefinition D.4 (Dynamic submodular framework, formal version of Definition 4.1). Define function"
    },
    {
      "page_no": 34,
      "bbox": [
        252.375,
        204.85504150390625,
        359.62762451171875,
        226.2531280517578
      ],
      "text": "F : 2[n] × [n] × 2[n] →R."
    },
    {
      "page_no": 34,
      "bbox": [
        107.44203186035156,
        226.02801513671875,
        360.0467834472656,
        243.32411193847656
      ],
      "text": "Then for any index i ∈[n], any set Z ⊆[i −1], we assume that"
    },
    {
      "page_no": 34,
      "bbox": [
        264.26800537109375,
        240.91802978515625,
        347.7300109863281,
        262.31610107421875
      ],
      "text": "F(Z, i, ·) : 2[n] →R"
    },
    {
      "page_no": 34,
      "bbox": [
        108.00001525878906,
        262.09100341796875,
        269.1058349609375,
        272.31365966796875
      ],
      "text": "is a submodular function w.r.t. to Z, i.e.,"
    },
    {
      "page_no": 34,
      "bbox": [
        135.39700317382812,
        282.1400146484375,
        337.8396301269531,
        299.43609619140625
      ],
      "text": "• For all sets X, Y ⊂[n] satisfy that Z ⊂X ⊂Y ,"
    },
    {
      "page_no": 34,
      "bbox": [
        135.39697265625,
        300.43902587890625,
        328.8086242675781,
        317.735107421875
      ],
      "text": "• For all element x ∈[n] satisfy that x ∈[n]\\Y ,"
    },
    {
      "page_no": 34,
      "bbox": [
        107.99996948242188,
        320.488037109375,
        140.36846923828125,
        330.45062255859375
      ],
      "text": "we have"
    },
    {
      "page_no": 34,
      "bbox": [
        194.44497680664062,
        337.57000732421875,
        417.5575256347656,
        354.8551025390625
      ],
      "text": "fZ,i(X ∪{x}) −fZ,i(X) ≥fZ,i(Y ∪{x}) −fZ,i(Y ),"
    },
    {
      "page_no": 34,
      "bbox": [
        107.99993896484375,
        354.6290283203125,
        504.0013427734375,
        400.9795837402344
      ],
      "text": "where fZ,i(·) := F(Z, i, ·).\nRemark D.5. We remark that Definition D.4 is a weaker version of Definition D.3. We also like to\nmention that the informal definition (see Definition 4.1) only contains two input parameters, but in\nfact we need three input parameters (see Definition D.4)."
    },
    {
      "page_no": 34,
      "bbox": [
        107.64099884033203,
        407.4159851074219,
        504.00244140625,
        428.2765808105469
      ],
      "text": "In the later, when we use fZ,i(·), we will replace Z by Si for convenient of analysis, for example see\nDefinition D.23, Definition D.24, Definition D.25 and Definition D.26."
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        442.7124938964844,
        199.6658935546875,
        452.6750793457031
      ],
      "text": "D.4\nStatic Attention"
    },
    {
      "page_no": 34,
      "bbox": [
        107.25299835205078,
        462.70330810546875,
        505.2455749511719,
        494.55908203125
      ],
      "text": "Before we describe the recursive attention computation, we will first describe the static version of\nattention computation as follows (for examples, see Definition 1.1 in [98] and others [97, 100, 101,\n106, 103, 118, 108]):"
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        496.7584228515625,
        420.9269104003906,
        516.2850952148438
      ],
      "text": "Definition D.6. Given three matrices Q, K, V ∈Rd×d, the goal is to compute"
    },
    {
      "page_no": 34,
      "bbox": [
        249.67498779296875,
        514.2244873046875,
        360.1081237792969,
        533.3560791015625
      ],
      "text": "Att(Q, K, V ) := D−1A · V"
    },
    {
      "page_no": 34,
      "bbox": [
        107.99992370605469,
        530.8079833984375,
        345.53863525390625,
        551.7191162109375
      ],
      "text": "where square matrix A ∈Rn×n can be rewritten as follows"
    },
    {
      "page_no": 34,
      "bbox": [
        272.24993896484375,
        550.739501953125,
        339.7513732910156,
        562.8250732421875
      ],
      "text": "A = exp(QK⊤)"
    },
    {
      "page_no": 34,
      "bbox": [
        107.99990844726562,
        567.322021484375,
        338.35284423828125,
        588.234130859375
      ],
      "text": "and diagonal matrix D ∈Rn×n can be written as follows"
    },
    {
      "page_no": 34,
      "bbox": [
        272.7079162597656,
        587.9539184570312,
        339.29339599609375,
        598.9784545898438
      ],
      "text": "D = diag(A1n)"
    },
    {
      "page_no": 34,
      "bbox": [
        107.64099884033203,
        605.0020751953125,
        504.0028991699219,
        626.2106323242188
      ],
      "text": "Here we apply exp() to a matrix entry-wisely to a matrix. 1n is a length-n vector where all the\nentries are ones. The operator diag() is turning a vector into a diagonal matrix."
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        640.385498046875,
        261.73291015625,
        650.3480834960938
      ],
      "text": "D.5\nRecursive Attention Definition"
    },
    {
      "page_no": 34,
      "bbox": [
        107.53199768066406,
        660.4514770507812,
        503.998291015625,
        703.0501098632812
      ],
      "text": "We first provide some definitions.\nDefinition D.7. Given a query matrix Q ∈Rn×d, we use Qi,∗to denote a length-d row vector that\nrepresents the i-th row of Q for each i ∈[n]."
    },
    {
      "page_no": 34,
      "bbox": [
        108.0,
        699.12841796875,
        503.9958801269531,
        722.4906616210938
      ],
      "text": "Definition D.8. Given a key matrix K ∈Rn×d, we use K≤i,∗to denote a i × d matrix that selects\nthe first i rows from K."
    },
    {
      "page_no": 34,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "34"
    },
    {
      "page_no": 35,
      "bbox": [
        179.03118896484375,
        119.70235443115234,
        192.793212890625,
        129.437255859375
      ],
      "text": "o1,1"
    },
    {
      "page_no": 35,
      "bbox": [
        179.03118896484375,
        145.46749877929688,
        192.793212890625,
        155.20150756835938
      ],
      "text": "o2,1"
    },
    {
      "page_no": 35,
      "bbox": [
        179.03118896484375,
        171.23175048828125,
        192.793212890625,
        180.96665954589844
      ],
      "text": "o3,1"
    },
    {
      "page_no": 35,
      "bbox": [
        179.03118896484375,
        196.99691772460938,
        192.793212890625,
        206.73089599609375
      ],
      "text": "o4,1"
    },
    {
      "page_no": 35,
      "bbox": [
        204.79544067382812,
        145.46749877929688,
        218.55836486816406,
        155.20150756835938
      ],
      "text": "o2,2"
    },
    {
      "page_no": 35,
      "bbox": [
        204.79544067382812,
        171.23175048828125,
        218.55836486816406,
        180.96665954589844
      ],
      "text": "o3,2"
    },
    {
      "page_no": 35,
      "bbox": [
        204.79544067382812,
        196.99691772460938,
        218.5574493408203,
        206.73089599609375
      ],
      "text": "o4,2"
    },
    {
      "page_no": 35,
      "bbox": [
        230.5596923828125,
        171.23175048828125,
        244.32261657714844,
        180.96665954589844
      ],
      "text": "o3,3"
    },
    {
      "page_no": 35,
      "bbox": [
        230.5596923828125,
        196.99691772460938,
        270.0868225097656,
        206.73089599609375
      ],
      "text": "o4,3\no4,4"
    },
    {
      "page_no": 35,
      "bbox": [
        133.28887939453125,
        120.3296890258789,
        160.66128540039062,
        133.0901336669922
      ],
      "text": "I (Q1,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        129.326904296875,
        146.0948486328125,
        164.6227264404297,
        158.85446166992188
      ],
      "text": "eat (Q2,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        130.45851135253906,
        171.8590850830078,
        163.4908447265625,
        184.61871337890625
      ],
      "text": "an (Q3,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        114.557861328125,
        197.02511596679688,
        168.6980438232422,
        211.16482543945312
      ],
      "text": "apple (Q4,∗)\n→"
    },
    {
      "page_no": 35,
      "bbox": [
        131.4674072265625,
        223.53573608398438,
        151.62158203125,
        231.68540954589844
      ],
      "text": "today"
    },
    {
      "page_no": 35,
      "bbox": [
        174.5256805419922,
        86.69963836669922,
        298.76025390625,
        109.90186309814453
      ],
      "text": "I\neat\nan\napple today\n(K1,∗) (K2,∗) (K3,∗) (K4,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        201.7570037841797,
        245.01400756835938,
        211.70970153808594,
        253.98040771484375
      ],
      "text": "(a)"
    },
    {
      "page_no": 35,
      "bbox": [
        377.2691650390625,
        116.94469451904297,
        391.0311584472656,
        126.67959594726562
      ],
      "text": "o1,1"
    },
    {
      "page_no": 35,
      "bbox": [
        377.2691650390625,
        142.7098388671875,
        391.0311584472656,
        152.44383239746094
      ],
      "text": "o2,1"
    },
    {
      "page_no": 35,
      "bbox": [
        377.2691650390625,
        168.47409057617188,
        391.0311584472656,
        178.208984375
      ],
      "text": "o3,1"
    },
    {
      "page_no": 35,
      "bbox": [
        377.2691650390625,
        194.2392578125,
        391.0311584472656,
        203.97323608398438
      ],
      "text": "o4,1"
    },
    {
      "page_no": 35,
      "bbox": [
        403.0334167480469,
        142.7098388671875,
        416.79632568359375,
        152.44383239746094
      ],
      "text": "o2,2"
    },
    {
      "page_no": 35,
      "bbox": [
        403.0334167480469,
        168.47409057617188,
        416.79632568359375,
        178.208984375
      ],
      "text": "o3,2"
    },
    {
      "page_no": 35,
      "bbox": [
        403.0334167480469,
        194.2392578125,
        416.79541015625,
        203.97323608398438
      ],
      "text": "o4,2"
    },
    {
      "page_no": 35,
      "bbox": [
        428.79766845703125,
        168.47409057617188,
        442.5605773925781,
        178.208984375
      ],
      "text": "o3,3"
    },
    {
      "page_no": 35,
      "bbox": [
        428.79766845703125,
        194.2392578125,
        468.3247985839844,
        203.97323608398438
      ],
      "text": "o4,3\no4,4"
    },
    {
      "page_no": 35,
      "bbox": [
        331.52685546875,
        117.572021484375,
        358.8992614746094,
        130.33164978027344
      ],
      "text": "I (Q1,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        327.56488037109375,
        143.33718872070312,
        362.8607177734375,
        156.09681701660156
      ],
      "text": "eat (Q2,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        328.69647216796875,
        169.10142517089844,
        361.72882080078125,
        181.86105346679688
      ],
      "text": "an (Q3,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        312.79583740234375,
        194.2665557861328,
        366.93603515625,
        208.40626525878906
      ],
      "text": "apple (Q4,∗)\n→"
    },
    {
      "page_no": 35,
      "bbox": [
        329.70538330078125,
        220.778076171875,
        349.85955810546875,
        228.92774963378906
      ],
      "text": "today"
    },
    {
      "page_no": 35,
      "bbox": [
        371.94891357421875,
        83.94197845458984,
        496.9982604980469,
        107.14411163330078
      ],
      "text": "I\neat\nan\napple today\n\u0018\u0018\u0018\n(K1,∗) (K2,∗) (K3,∗) (K4,∗)"
    },
    {
      "page_no": 35,
      "bbox": [
        399.7439880371094,
        245.01400756835938,
        410.1988220214844,
        253.98040771484375
      ],
      "text": "(b)"
    },
    {
      "page_no": 35,
      "bbox": [
        107.677001953125,
        262.019287109375,
        505.57086181640625,
        310.9029235839844
      ],
      "text": "Figure 14: (a) Exact version of the attention computation. Here, our example is about the language modeling task.\nAt this stage, the model predicts the word ’apple’ and computes the exact attention vector o4. (b) Approximate\nversion of the attention computation. Let the budget of the number of tokens we can track be 3. We truncate the\nkey matrix for the first token K1,∗and only keep K2,∗, K3,∗and K4,∗in the memory. Compared with the exact\nversion, we don’t compute o4,1."
    },
    {
      "page_no": 35,
      "bbox": [
        108.0,
        333.1664733886719,
        332.5369873046875,
        364.8031005859375
      ],
      "text": "Next, we show the exact version of computing attention.\nDefinition D.9 (Exact Version). Given Q, K ∈Rn×d"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39698791503906,
        363.88702392578125,
        382.7493896484375,
        386.2930908203125
      ],
      "text": "• For each i ∈[n], we use oi ∈Ri to denote a length-i vector."
    },
    {
      "page_no": 35,
      "bbox": [
        153.82801818847656,
        383.5584411621094,
        462.9944152832031,
        404.5801086425781
      ],
      "text": "– For each j ∈[i], we use oi,j to denote the j-th coordinate of vector oi ∈Ri"
    },
    {
      "page_no": 35,
      "bbox": [
        107.39199829101562,
        405.7850341796875,
        213.0553436279297,
        415.74761962890625
      ],
      "text": "For the first layer, we have"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39700317382812,
        425.7810363769531,
        180.25662231445312,
        437.4976806640625
      ],
      "text": "• o1,1 = 1."
    },
    {
      "page_no": 35,
      "bbox": [
        107.39197540283203,
        445.77703857421875,
        225.3192596435547,
        455.7396240234375
      ],
      "text": "For the second layer, we have"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39697265625,
        464.8705139160156,
        252.31280517578125,
        492.6624450683594
      ],
      "text": "• length-2 vector o2 := D−1\n2\n|{z}\nscalar"
    },
    {
      "page_no": 35,
      "bbox": [
        253.96798706054688,
        466.864990234375,
        296.2841796875,
        497.7450866699219
      ],
      "text": "· exp(Q2,∗\n|{z}\n1×d"
    },
    {
      "page_no": 35,
      "bbox": [
        297.9439697265625,
        465.0184631347656,
        338.023193359375,
        497.7450866699219
      ],
      "text": "(K≤2,∗)⊤\n|\n{z\n}\nd×2"
    },
    {
      "page_no": 35,
      "bbox": [
        338.0230407714844,
        467.1140441894531,
        341.89849853515625,
        477.07666015625
      ],
      "text": ")"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39703369140625,
        500.70245361328125,
        281.87347412109375,
        533.0791015625
      ],
      "text": "• scalar D2 := exp(Q2,∗(K≤2,∗)⊤)\n|\n{z\n}\n1×2"
    },
    {
      "page_no": 35,
      "bbox": [
        283.5329895019531,
        502.48291015625,
        305.89373779296875,
        531.7230224609375
      ],
      "text": "· 12\n|{z}\n2×1"
    },
    {
      "page_no": 35,
      "bbox": [
        107.39198303222656,
        538.0969848632812,
        279.52618408203125,
        555.39306640625
      ],
      "text": "For each i ∈[n], for the i-th layer, we have"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39698791503906,
        557.6434936523438,
        249.61080932617188,
        585.5564575195312
      ],
      "text": "• length-i vector oi := D−1\ni\n|{z}\nscalar"
    },
    {
      "page_no": 35,
      "bbox": [
        251.2659912109375,
        559.6380004882812,
        292.72174072265625,
        590.51806640625
      ],
      "text": "· exp(Qi,∗\n|{z}\n1×d"
    },
    {
      "page_no": 35,
      "bbox": [
        294.3819885253906,
        557.79248046875,
        333.3091735839844,
        590.51806640625
      ],
      "text": "(K≤i,∗)⊤\n|\n{z\n}\nd×i"
    },
    {
      "page_no": 35,
      "bbox": [
        333.30902099609375,
        559.8870849609375,
        337.1844787597656,
        569.8496704101562
      ],
      "text": ")"
    },
    {
      "page_no": 35,
      "bbox": [
        135.3970184326172,
        593.4755249023438,
        278.41644287109375,
        625.9730834960938
      ],
      "text": "• scalar Di := exp(Qi,∗(K≤i,∗)⊤)\n|\n{z\n}\n1×i"
    },
    {
      "page_no": 35,
      "bbox": [
        280.07501220703125,
        595.2568969726562,
        302.4357604980469,
        624.6170654296875
      ],
      "text": "· 1i\n|{z}\ni×1"
    },
    {
      "page_no": 35,
      "bbox": [
        108.0,
        630.374267578125,
        503.997314453125,
        672.9951171875
      ],
      "text": "Now, we show the approximate version of computing attention. Instead of computing the entire\nattention oi, we only compute the attention of the tokens that are being tracked.\nDefinition D.10 (Approximate Version). Given Q, K ∈Rn×d"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39700317382812,
        672.0790405273438,
        382.7494201660156,
        694.485107421875
      ],
      "text": "• For each i ∈[n], we use oi ∈Ri to denote a length-i vector."
    },
    {
      "page_no": 35,
      "bbox": [
        153.8280487060547,
        691.7504272460938,
        462.9944763183594,
        712.7720947265625
      ],
      "text": "– For each j ∈[i], we use oi,j to denote the j-th coordinate of vector oi ∈Ri"
    },
    {
      "page_no": 35,
      "bbox": [
        135.39706420898438,
        712.2680053710938,
        500.746337890625,
        729.5640869140625
      ],
      "text": "• Let k ∈[n] be the budget of the number of tokens we can track (due to the memory issue)."
    },
    {
      "page_no": 35,
      "bbox": [
        301.01904296875,
        742.3324584960938,
        310.98162841796875,
        752.2950439453125
      ],
      "text": "35"
    },
    {
      "page_no": 36,
      "bbox": [
        107.39199829101562,
        74.23101806640625,
        213.0553436279297,
        84.19361877441406
      ],
      "text": "For the first layer, we have"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39700317382812,
        94.09100341796875,
        180.2566375732422,
        105.80863952636719
      ],
      "text": "• o1,1 = 1."
    },
    {
      "page_no": 36,
      "bbox": [
        107.3919906616211,
        113.9520263671875,
        225.31927490234375,
        123.91462707519531
      ],
      "text": "For the second layer, we have"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39698791503906,
        132.91050720214844,
        252.31280517578125,
        160.7024383544922
      ],
      "text": "• length-2 vector o2 := D−1\n2\n|{z}\nscalar"
    },
    {
      "page_no": 36,
      "bbox": [
        253.96798706054688,
        134.90496826171875,
        296.2841796875,
        165.78509521484375
      ],
      "text": "· exp(Q2,∗\n|{z}\n1×d"
    },
    {
      "page_no": 36,
      "bbox": [
        297.9439697265625,
        133.0585174560547,
        338.023193359375,
        165.78509521484375
      ],
      "text": "(K≤2,∗)⊤\n|\n{z\n}\nd×2"
    },
    {
      "page_no": 36,
      "bbox": [
        338.0230407714844,
        135.15408325195312,
        341.89849853515625,
        145.11668395996094
      ],
      "text": ")"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39703369140625,
        168.7085418701172,
        281.87347412109375,
        201.0860595703125
      ],
      "text": "• scalar D2 := exp(Q2,∗(K≤2,∗)⊤)\n|\n{z\n}\n1×2"
    },
    {
      "page_no": 36,
      "bbox": [
        283.5329895019531,
        170.4899139404297,
        305.89373779296875,
        199.7301025390625
      ],
      "text": "· 12\n|{z}\n2×1"
    },
    {
      "page_no": 36,
      "bbox": [
        107.39198303222656,
        205.968017578125,
        281.63824462890625,
        223.2641143798828
      ],
      "text": "For each i ∈[n], for the i-th token, we have"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39698791503906,
        225.82904052734375,
        478.1760559082031,
        244.62013244628906
      ],
      "text": "• Let Si ⊂[n] denote the tokens we’re tracking when we are predicting the i-th token."
    },
    {
      "page_no": 36,
      "bbox": [
        135.39697265625,
        244.093017578125,
        177.30050659179688,
        261.37811279296875
      ],
      "text": "• |Si| = k"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39698791503906,
        262.33502197265625,
        348.3172912597656,
        281.1251220703125
      ],
      "text": "• |Si\\Si−1| ≤1 or equivalently |Si ∩Si−1| ≥k −1"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39700317382812,
        280.13848876953125,
        249.61080932617188,
        308.0504455566406
      ],
      "text": "• length-i vector oi := D−1\ni\n|{z}\nscalar"
    },
    {
      "page_no": 36,
      "bbox": [
        251.2659912109375,
        282.13299560546875,
        292.72174072265625,
        313.0130615234375
      ],
      "text": "· exp(Qi,∗\n|{z}\n1×d"
    },
    {
      "page_no": 36,
      "bbox": [
        294.38201904296875,
        280.2874450683594,
        332.3111877441406,
        313.0130920410156
      ],
      "text": "(KSi,∗)⊤\n|\n{z\n}\nd×i"
    },
    {
      "page_no": 36,
      "bbox": [
        332.31201171875,
        282.3820495605469,
        336.1874694824219,
        292.34466552734375
      ],
      "text": ")"
    },
    {
      "page_no": 36,
      "bbox": [
        135.3970184326172,
        315.93646240234375,
        322.283447265625,
        349.1260986328125
      ],
      "text": "• scalar Di := (exp(Qi,∗(KSi,∗)⊤) −1[i]\\Si)\n|\n{z\n}\n1×i"
    },
    {
      "page_no": 36,
      "bbox": [
        323.9429931640625,
        317.71795654296875,
        346.302734375,
        347.0780944824219
      ],
      "text": "· 1i\n|{z}\ni×1"
    },
    {
      "page_no": 36,
      "bbox": [
        108.0,
        353.48638916015625,
        504.16455078125,
        374.3390808105469
      ],
      "text": "In a certain sense, the above definition is related to finding heavy hitters in compressive sensing (for\nmore background on compressive sensing, we refer readers to [124, 125, 126])."
    },
    {
      "page_no": 36,
      "bbox": [
        108.0,
        388.5135192871094,
        196.15904235839844,
        398.4761047363281
      ],
      "text": "D.6\nEviction Policy"
    },
    {
      "page_no": 36,
      "bbox": [
        107.64099884033203,
        408.6097717285156,
        505.74346923828125,
        476.38360595703125
      ],
      "text": "The goal of this section is to our eviction policy under space limitation. We start by giving a process\nwithout having space limitations. We denote the attention query matrix as Q ∈Rn×d and the key\nmatrix as K ∈Rn×d. Qi,∗represents the i-th row of Q and K≤i,∗represents the first i rows of K.\nFor simplicity, KSi,∗denotes a sub-matrix of K which selects Si rows from K.\nDefinition D.11 (The generative process with Full knowledge). For each i ∈[n], for the i-th token,\nwe have"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39700317382812,
        484.2984924316406,
        503.99981689453125,
        507.152587890625
      ],
      "text": "• length-i vector oi := D−1\ni\n· exp(Qi,∗(K≤i,∗)⊤), which denotes the attention of the i-th\nword."
    },
    {
      "page_no": 36,
      "bbox": [
        135.39700317382812,
        514.1065063476562,
        456.9566345214844,
        532.7390747070312
      ],
      "text": "• scalar Di := exp(Qi,∗(K≤i,∗)⊤) · 1i, which denotes the normalization factor."
    },
    {
      "page_no": 36,
      "bbox": [
        108.0,
        535.4052734375,
        504.0032958984375,
        624.111083984375
      ],
      "text": "In the above process, since we have no space limitation, then we can keep all the scores. In the\nfollowing process, we show how to handle things when there is a space limitation. In this case, we\nneed to dynamically maintain set Si such that |Si| ≤k (Compared to the full knowledge case, we\ncan think of |Si| = i).\nDefinition D.12 (H2O Eviction Policy, Formal version of Definition 4.3). Let k denote the budget of\nspace and k < n. Let Fscore : 2[n] →R denote certain score function. Let Si−1 denote the source\nset. Let Si denote the target set. We defined the eviction policy g : Si−1 →Si s.t."
    },
    {
      "page_no": 36,
      "bbox": [
        135.39703369140625,
        625.1268920898438,
        361.7637634277344,
        642.47705078125
      ],
      "text": "• |Si| ≤k (KV cache size is not changing over the time)"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39703369140625,
        643.3799438476562,
        381.9345397949219,
        660.7301025390625
      ],
      "text": "• |Si\\Si−1| ≤1 (we can evict at most 1 KV in the KV cache)"
    },
    {
      "page_no": 36,
      "bbox": [
        135.39700317382812,
        661.697998046875,
        503.99957275390625,
        680.4780883789062
      ],
      "text": "• We construct Si ←(Si−1∪{i})\\{u} as u ←arg maxv∈(Si−1∪{i}) Fscore(Si−1∪{i}\\{v}}"
    },
    {
      "page_no": 36,
      "bbox": [
        108.0,
        679.5565185546875,
        504.0032043457031,
        729.5640869140625
      ],
      "text": "Remark D.13. We remake that, in Definition 4.3, we introduce a simplified notation where we state\nthat |Si| = k in the first bullet point, but in general, we consider the case where |Si| ≤k for the first\nk tokens. To accommodate this more general scenario, we present Definition D.12, which provides a\nbroader definition that handles the situation where |Si| ≤k."
    },
    {
      "page_no": 36,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "36"
    },
    {
      "page_no": 37,
      "bbox": [
        107.64099884033203,
        73.28538513183594,
        503.9993591308594,
        103.72107696533203
      ],
      "text": "Algorithm 2 H2 Eviction Algorithm, Query matrix Q ∈Rn×d, key matrix K ∈Rn×d, budget size\nof KV cache k ∈N. Formal and detailed version of Algorithm 1."
    },
    {
      "page_no": 37,
      "bbox": [
        108.495849609375,
        101.18540954589844,
        504.66851806640625,
        215.0364227294922
      ],
      "text": "1: procedure H2_EVICTION(Q ∈Rn×d, K ∈Rn×d, k ∈N)\n2:\nS0 ←∅, eo0 ←0\n▷Initialization the algorithm\n3:\nfor i = 1 →n do\n▷Linear scan each token\n4:\nif i ≤k then\n▷If the KV cache is not full\n5:\nSi ←Si−1 ∪{i}\n▷Expand the set directly\n6:\nelse\n▷If the KV cache is full\n7:\nDi ←(exp(Qi,∗(KSi−1,∗)⊤) −1[i]\\Si−1) · 1i\n▷Compute the normalization factor\n8:\noi ←D−1\ni\n· exp(Qi,∗(KSi−1,∗)⊤)\n▷Compute score vector\n9:\neoi ←eoi−1 + oi\n▷Accumulate the scores (Remark D.15)\n10:\nLet score function be Fscore(T) := h(P"
    },
    {
      "page_no": 37,
      "bbox": [
        108.49581909179688,
        204.24244689941406,
        504.0000915527344,
        270.9200439453125
      ],
      "text": "s∈T eoi,s) ▷Score function (Remark D.14)\n11:\nu ←arg maxv∈(Si−1∪{i}) Fscore(Si−1 ∪{i}\\{v}}\n▷Find an entry to swap\n12:\nSi ←(Si−1 ∪{i})\\{u}\n▷Construct Si\n13:\nend if\n14:\nend for\n15: end procedure"
    },
    {
      "page_no": 37,
      "bbox": [
        107.64099884033203,
        299.29119873046875,
        505.7443542480469,
        400.43157958984375
      ],
      "text": "Remark D.14. We remark the above function Fscore can have multiple choices. In the later analysis,\nwe provide two choices. If we use the exact function, then Fscore is fSi,i (see Definition D.23 and\nDefinition D.25). If we use the approximate function, then Fscore if efSi,i (see Definition D.24 and\nDefinition D.26). Let h : R →R (the h being used Line 10 in Algorithm 2) denote function. We\nhope to choose h such that it gives the submodular property for the function Fscore in the sense of\nour dynamic framework. For example, h(z) is usually chosen to be some non-decreasing concave\nfunction such as √z + 1 and log(z + 1). For simplicity, in Algorithm 1 (which is the informal\nversion of Algorithm 2), we choose h(z) = z for the purpose of providing readers with an intuitive\nunderstanding and due to space limitation."
    },
    {
      "page_no": 37,
      "bbox": [
        108.0,
        406.3699951171875,
        504.0041809082031,
        438.1775817871094
      ],
      "text": "Remark D.15. We remark that, in Line 10 in Algorithm 2, the eoi,s is the accumulation of attention\nscore for the token s in set T. In our Algorithm 1, we only use os in Line 10 (see Algorithm 1) to\nrepresent that accumulation of the attention score for simplicity."
    },
    {
      "page_no": 37,
      "bbox": [
        107.72100067138672,
        444.6139831542969,
        505.74298095703125,
        487.55364990234375
      ],
      "text": "In Algorithm 2, for simplicity of the presentation, we can create multiple vectors eo0, · · · eon (see Line 2,\nLine 9, Line 10). However, every time at i-th, we only need to use the information for eoi which has at\nmost length n size. Thus, a straightforward and better implementation for only using one length-n to\nstore accumulative score can reduce n2 usage to n."
    },
    {
      "page_no": 37,
      "bbox": [
        108.00001525878906,
        504.20050048828125,
        451.2116394042969,
        514.1630859375
      ],
      "text": "D.7\nExplaining Submodular Diminishing Return Property in Attention Scheme"
    },
    {
      "page_no": 37,
      "bbox": [
        107.69100189208984,
        521.416015625,
        505.74383544921875,
        546.1516723632812
      ],
      "text": "In the standard submodular problem [123], we are given a ground set [n] and a function f : 2[n] →R.\nThe goal is to find a set of elements S such that f(S) is maximized."
    },
    {
      "page_no": 37,
      "bbox": [
        107.53199768066406,
        552.4192504882812,
        504.3524475097656,
        580.5230712890625
      ],
      "text": "We say function f is submodular (Recall the formal definition in Definition D.1), if for every\nX, Y ⊂[n] with X ⊆Y and every x ∈[n]\\Y we have"
    },
    {
      "page_no": 37,
      "bbox": [
        216.02499389648438,
        583.18798828125,
        395.9764709472656,
        600.4730834960938
      ],
      "text": "f(X ∪{x}) −f(X) ≥f(Y ∪{x}) −f(Y )"
    },
    {
      "page_no": 37,
      "bbox": [
        108.0,
        608.708251953125,
        505.2423095703125,
        651.4730224609375
      ],
      "text": "Submodular functions can represent the cost of items, due to their diminishing returns property\n[127, 128]. It suggests that the increase in information obtained from selecting a candidate object,\nsuch as a word or sentence, becomes smaller as more objects have already been chosen for the\nsummary."
    },
    {
      "page_no": 37,
      "bbox": [
        108.0,
        657.941162109375,
        505.7735290527344,
        722.4070434570312
      ],
      "text": "For instance, we introduce a new token, denoted as “w”, into two sets, S and S0, where the concepts\ncovered by S0 are a subset of those covered by S. By intuition, the information added to S0 by “w”\nshould be larger compared to adding it to S, as the new concepts carried by “w” might have already\nbeen covered by the concepts present in S but not in S0. This property is known as the diminishing\nreturn property. Hence, we propose that the neural coverage function [129] should exhibit a desirable\ncharacteristic called submodularity."
    },
    {
      "page_no": 37,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "37"
    },
    {
      "page_no": 38,
      "bbox": [
        108.0,
        74.31652069091797,
        263.69549560546875,
        84.27912139892578
      ],
      "text": "D.8\nSubmodular: High Level Ideas"
    },
    {
      "page_no": 38,
      "bbox": [
        107.53199768066406,
        94.30728149414062,
        503.9967041015625,
        115.25405883789062
      ],
      "text": "We formalize the submodular function maximization problem with cardinality constraint in this\nsection. Informally, a set function is submodular if it has decreasing marginal increment."
    },
    {
      "page_no": 38,
      "bbox": [
        108.0,
        117.17797088623047,
        504.0027770996094,
        143.1486358642578
      ],
      "text": "Definition D.16 (Submodular function). We denote f : 2[n] →R as a set function. The discrete\nderivative ∆f is defined as follows:"
    },
    {
      "page_no": 38,
      "bbox": [
        238.3489990234375,
        148.79498291015625,
        373.6535949707031,
        166.08009338378906
      ],
      "text": "∆f(i | S) := f(S ∪{i}) −f(S)."
    },
    {
      "page_no": 38,
      "bbox": [
        107.39199829101562,
        165.87701416015625,
        473.6409606933594,
        183.17311096191406
      ],
      "text": "A function f is submodular if, for any S ⊆T and i ∈[n]\\T, the following inequality holds:"
    },
    {
      "page_no": 38,
      "bbox": [
        258.6329650878906,
        182.97998046875,
        353.3685302734375,
        200.2650909423828
      ],
      "text": "∆f(i | T) ≤∆f(i | S)."
    },
    {
      "page_no": 38,
      "bbox": [
        108.0,
        205.64230346679688,
        504.3428955078125,
        226.58908081054688
      ],
      "text": "For simplicity, we present the problem of maximizing a submodular function with a cardinality\nconstraint (1). Our goal is to solve the optimization problem efficiently."
    },
    {
      "page_no": 38,
      "bbox": [
        275.0500183105469,
        233.68307495117188,
        326.33148193359375,
        254.6781005859375
      ],
      "text": "max\nS⊆[n]\nf(S)"
    },
    {
      "page_no": 38,
      "bbox": [
        284.3590087890625,
        244.5345001220703,
        504.6673889160156,
        271.97113037109375
      ],
      "text": "s.t.\n|S| ≤k\n(1)"
    },
    {
      "page_no": 38,
      "bbox": [
        108.0,
        278.2413635253906,
        504.0037536621094,
        320.9460754394531
      ],
      "text": "Representation of f(S)\nOne challenge in designing the algorithm is determining the representation\nof input instances. Since the constraint in optimization problem (1) is straightforward, we need to\ndecide how to represent f(S). Suppose S = i1, i2, · · · , im ⊆[n], we can decompose f(S) into a\nsum of increments as follows:"
    },
    {
      "page_no": 38,
      "bbox": [
        223.29400634765625,
        334.79803466796875,
        291.2010803222656,
        345.5074157714844
      ],
      "text": "f(S) = f(S0) +"
    },
    {
      "page_no": 38,
      "bbox": [
        293.4129943847656,
        324.5866394042969,
        307.8089599609375,
        342.1449279785156
      ],
      "text": "m\nX"
    },
    {
      "page_no": 38,
      "bbox": [
        293.7149963378906,
        334.5489807128906,
        504.6673889160156,
        355.7684326171875
      ],
      "text": "j=1\n(f(Sj) −f(Sj−1)),\n(2)"
    },
    {
      "page_no": 38,
      "bbox": [
        107.64099884033203,
        363.3079833984375,
        504.0037841796875,
        395.2540588378906
      ],
      "text": "where S0 = ∅and Sj = Sj−1+ij. Without loss of generality, we assume f(∅) = 0. By the definition\nof ∆f(i|S), we have f(Sj) −f(Sj−1) = ∆f(ij|Sj−1). Therefore, the decomposition (2) can be\nsimplified as follows:"
    },
    {
      "page_no": 38,
      "bbox": [
        255.08299255371094,
        411.1640319824219,
        285.9859619140625,
        421.12664794921875
      ],
      "text": "f(S) ="
    },
    {
      "page_no": 38,
      "bbox": [
        288.74798583984375,
        400.9516296386719,
        303.1439514160156,
        418.51092529296875
      ],
      "text": "m\nX"
    },
    {
      "page_no": 38,
      "bbox": [
        289.04998779296875,
        410.91497802734375,
        504.66741943359375,
        432.1334228515625
      ],
      "text": "j=1\n∆f(ij|Sj−1)\n(3)"
    },
    {
      "page_no": 38,
      "bbox": [
        107.69107055664062,
        439.6729736328125,
        469.2893371582031,
        456.95806884765625
      ],
      "text": "To introduce our advanced data structure later, we further represent ∆f(i|S) in the form of"
    },
    {
      "page_no": 38,
      "bbox": [
        261.49908447265625,
        456.8134460449219,
        504.6674499511719,
        475.945068359375
      ],
      "text": "∆f(i|S) = u⊤\ni h(S)ui\n(4)"
    },
    {
      "page_no": 38,
      "bbox": [
        107.64108276367188,
        474.12042236328125,
        420.9280700683594,
        496.35003662109375
      ],
      "text": "where ui ∈Rd is a d-dimensional vector and h(S) ∈Rd×d is a d-by-d matrix."
    },
    {
      "page_no": 38,
      "bbox": [
        107.6709976196289,
        494.1851501464844,
        505.7455139160156,
        536.8140258789062
      ],
      "text": "In practice, a significant subclass of submodular functions is the monotone submodular functions, i.e.\nfunctions f satisfying f(A) ≤f(B) for all A ⊆B ⊆[n]. When f is monotone, we could restrict\nall h(S) to be positive semidefinite (PSD) matrixes. When the matrix h(S) is positive semidefinite\n(PSD), it makes us achieve a faster acceleration."
    },
    {
      "page_no": 38,
      "bbox": [
        108.0,
        551.0765380859375,
        299.9394226074219,
        561.0391235351562
      ],
      "text": "D.9\nRobust Greedy with Error Propagation"
    },
    {
      "page_no": 38,
      "bbox": [
        107.64099884033203,
        570.9769897460938,
        504.6721496582031,
        678.0546875
      ],
      "text": "The procedure for the greedy selection algorithm initiates with empty set S0 = ∅. For each iteration\nin the main loop, the algorithm chooses the element that maximizes the marginal increment to add to\nthe set. When the size of the set eventually reaches k, the algorithm return that set S. Specifically, for\niteration t ∈{1, 2, · · · , k}, we let\nSt ←St−1 ∪{jt}\nwhere the element in the singleton is jt = arg maxj∈St−1 f(St−1 ∪{j}). The greedy strategy is\neffective in the sense that the approximation error of it is 1 −1/e.\nTheorem D.17 ([130]). For a monotone submodular function f, the greedy algorithm (Algorithm 3)\nguarantees to output a set S satisfying"
    },
    {
      "page_no": 38,
      "bbox": [
        241.25100708007812,
        684.9359741210938,
        370.7515563964844,
        706.1790771484375
      ],
      "text": "f(S) ≥(1 −1/e) max\n|T |=k{f(T)}."
    },
    {
      "page_no": 38,
      "bbox": [
        107.99996948242188,
        712.2680053710938,
        233.2296600341797,
        722.4070434570312
      ],
      "text": "Corollary D.18 ([131]). Given"
    },
    {
      "page_no": 38,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "38"
    },
    {
      "page_no": 39,
      "bbox": [
        107.64099884033203,
        74.00749969482422,
        279.1866149902344,
        84.06106567382812
      ],
      "text": "Algorithm 3 Greedy algorithm benchmark"
    },
    {
      "page_no": 39,
      "bbox": [
        112.97898864746094,
        89.45848846435547,
        505.1664733886719,
        175.78504943847656
      ],
      "text": "1: procedure GREEDYALGORITHM(submodular function f)\n2:\nS0 ←∅\n▷Initialize an empty set S0\n3:\nfor t = 0 →k −1 do\n4:\nj ←arg maxi{f(St ∪{i})}\n▷Find the element i ∈[n] that maximize f(St ∪{i})\n5:\nSt+1 ←St ∪{j}\n▷Append element i to set St to get St+1\n6:\nend for\n7:\nreturn Sk\n8: end procedure"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39700317382812,
        202.58502197265625,
        251.3636474609375,
        212.8076629638672
      ],
      "text": "• accuracy parameter ϵ > 0."
    },
    {
      "page_no": 39,
      "bbox": [
        135.39700317382812,
        221.2430419921875,
        197.84629821777344,
        238.5391387939453
      ],
      "text": "• integer k ≥1"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39700317382812,
        239.82887268066406,
        504.00335693359375,
        268.10711669921875
      ],
      "text": "• Let O denote an oracle that takes an arbitrary set S ⊆[n] and i ∈[n]\\S, returns a value\nO(S, i) with guarantee that ∆(i|S) −ϵ ≤O(S, i) ≤∆(i|S) + ϵ."
    },
    {
      "page_no": 39,
      "bbox": [
        135.39706420898438,
        269.3920593261719,
        504.0019226074219,
        299.1680603027344
      ],
      "text": "• Let A denote an algorithm that each time step t\n=\n1, 2, · · · , k, it selects jt\n=\narg maxj{O(St−1, j)} and lets St ←St−1 ∪{jt}."
    },
    {
      "page_no": 39,
      "bbox": [
        107.44203186035156,
        301.02899169921875,
        196.0089874267578,
        311.25164794921875
      ],
      "text": "Then this algorithm A"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39703369140625,
        321.67999267578125,
        206.14212036132812,
        332.6494140625
      ],
      "text": "• returns a set Sk"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39703369140625,
        340.3379821777344,
        215.48890686035156,
        351.7945556640625
      ],
      "text": "• the Sk satisfy that"
    },
    {
      "page_no": 39,
      "bbox": [
        226.40603637695312,
        358.7859802246094,
        421.46337890625,
        380.0300598144531
      ],
      "text": "f(Sk) ≥(1 −1/e) max\n|T |=k{f(T)} −k(2 −1/e)ϵ."
    },
    {
      "page_no": 39,
      "bbox": [
        107.99996948242188,
        390.5245056152344,
        302.18096923828125,
        400.4870910644531
      ],
      "text": "D.10\nRobust Submodular and Adding Items"
    },
    {
      "page_no": 39,
      "bbox": [
        107.53196716308594,
        410.8404541015625,
        389.64288330078125,
        420.8030700683594
      ],
      "text": "We first propose a lemma that shows the robustness of the submodular."
    },
    {
      "page_no": 39,
      "bbox": [
        107.99996948242188,
        425.8349914550781,
        323.4858093261719,
        443.1310729980469
      ],
      "text": "Lemma D.19. Suppose that for each i ∈[n], we have"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39698791503906,
        446.4859924316406,
        260.7984924316406,
        465.27606201171875
      ],
      "text": "• for all Si ⊂[n] with |Si| ≤k"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39697265625,
        465.14398193359375,
        351.74365234375,
        483.9340515136719
      ],
      "text": "• for all X ⊂[n], X ⊆Si ∪{i} and |X ∩Si| ≤|Si|,"
    },
    {
      "page_no": 39,
      "bbox": [
        203.4659881591797,
        485.99395751953125,
        444.4005126953125,
        503.279052734375
      ],
      "text": "|( efSi,i(X)) −efSi,i(Si) −(fSi,i(X) −fSi,i(Si))| ≤ϵ/(2n)"
    },
    {
      "page_no": 39,
      "bbox": [
        107.72100067138672,
        509.8230285644531,
        504.0002136230469,
        553.7960815429688
      ],
      "text": "Let opti denote the optimal cost that can be achieved by using f in i-th iteration. If the greedy\nalgorithm use f to find the solution has performance at least (1 −1/e) · opti, then using ef, we can\nobtain a solution that has performance at least (1 −1/e) · opti −ϵ"
    },
    {
      "page_no": 39,
      "bbox": [
        108.0,
        559.22900390625,
        288.51275634765625,
        569.3680419921875
      ],
      "text": "Proof. The proof follows from Lemma D.18."
    },
    {
      "page_no": 39,
      "bbox": [
        108.0,
        584.5704345703125,
        372.6863708496094,
        594.5330200195312
      ],
      "text": "Next, we explain how to add items into sets based on exact values."
    },
    {
      "page_no": 39,
      "bbox": [
        108.0,
        599.5650024414062,
        465.7662353515625,
        609.7040405273438
      ],
      "text": "Definition D.20 (Expanding items based on exact value). If the following conditions hold"
    },
    {
      "page_no": 39,
      "bbox": [
        135.3970184326172,
        620.2160034179688,
        210.13365173339844,
        639.006103515625
      ],
      "text": "• Let Si ⊆[i −1]."
    },
    {
      "page_no": 39,
      "bbox": [
        135.39700317382812,
        636.6929931640625,
        223.712646484375,
        657.5941162109375
      ],
      "text": "• Let fSi,i : 2[i] →R."
    },
    {
      "page_no": 39,
      "bbox": [
        107.44200134277344,
        660.947998046875,
        251.20831298828125,
        672.4056396484375
      ],
      "text": "Then, we can define Si+1 as follows"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39700317382812,
        681.5220336914062,
        504.0018615722656,
        704.9202880859375
      ],
      "text": "• If |Si| = k then Si+1 = Si ∪{i}\\u where u = arg maxv∈Si∪{i} fSi,i(Si ∪{i}\\v) −\nfSi,i(Si)"
    },
    {
      "page_no": 39,
      "bbox": [
        135.39695739746094,
        712.2680053710938,
        279.8226013183594,
        731.05810546875
      ],
      "text": "• If |Si| < k, then Si+1 = Si ∪{i}."
    },
    {
      "page_no": 39,
      "bbox": [
        301.0189514160156,
        742.3324584960938,
        310.9815368652344,
        752.2950439453125
      ],
      "text": "39"
    },
    {
      "page_no": 40,
      "bbox": [
        108.0,
        74.16557312011719,
        504.0020446777344,
        107.11262512207031
      ],
      "text": "Remark D.21. We remark that u = arg maxv∈Si∪{i} fSi,i(Si ∪{i}\\v) −fSi,i(Si) is the same as\nu = arg maxv∈Si∪{i} fSi,i(Si ∪{i}\\v). For the convenience of discussion and analysis, we will\nswitch to using both cases in different places."
    },
    {
      "page_no": 40,
      "bbox": [
        108.0,
        117.72248840332031,
        494.69720458984375,
        142.60110473632812
      ],
      "text": "Here, we explain how to add items into sets via approximate values.\nDefinition D.22 (Expanding items based on approximate value). If the following conditions hold"
    },
    {
      "page_no": 40,
      "bbox": [
        135.39698791503906,
        152.8580322265625,
        210.1336212158203,
        171.6481475830078
      ],
      "text": "• Let Si ⊆[i −1]."
    },
    {
      "page_no": 40,
      "bbox": [
        135.39697265625,
        170.0579833984375,
        223.71261596679688,
        190.95912170410156
      ],
      "text": "• Let efSi,i : 2[i] →R."
    },
    {
      "page_no": 40,
      "bbox": [
        107.44197082519531,
        194.05804443359375,
        251.20828247070312,
        205.51564025878906
      ],
      "text": "Then, we can define Si+1 as follows"
    },
    {
      "page_no": 40,
      "bbox": [
        135.39697265625,
        216.61306762695312,
        504.0018615722656,
        245.49600219726562
      ],
      "text": "• If |Si| = k then Si+1 = Si ∪{i}\\u where u = arg maxv∈Si∪{i} efSi,i(Si ∪{i}\\v) −\nefSi,i(Si)"
    },
    {
      "page_no": 40,
      "bbox": [
        135.39694213867188,
        249.45611572265625,
        279.8226013183594,
        268.2462158203125
      ],
      "text": "• If |Si| < k, then Si+1 = Si ∪{i}."
    },
    {
      "page_no": 40,
      "bbox": [
        107.99993896484375,
        274.04962158203125,
        227.16258239746094,
        284.01220703125
      ],
      "text": "D.11\nUniversal Conditions"
    },
    {
      "page_no": 40,
      "bbox": [
        107.53193664550781,
        294.1345520019531,
        504.00396728515625,
        338.5740661621094
      ],
      "text": "We state several definitions for both the exact function and approximation function.\nDefinition D.23 (Universal Monotone Condition (for exact function)). We say f has universal\nmonotone condition, if for all i ∈[n] for all Si ⊆[i −1], we have"
    },
    {
      "page_no": 40,
      "bbox": [
        240.60899353027344,
        337.72198486328125,
        370.6150207519531,
        355.007080078125
      ],
      "text": "fSi,i(X) ≥fSi,i(Y ),\n∀Y ⊂X"
    },
    {
      "page_no": 40,
      "bbox": [
        108.0,
        358.8996887207031,
        503.9963073730469,
        388.5210876464844
      ],
      "text": "Definition D.24 (Universal Monotone Condition (for approximate function)). We say ef has universal\nmonotone condition, if for all i ∈[n] for all Si ⊆[i −1], we have"
    },
    {
      "page_no": 40,
      "bbox": [
        240.60899353027344,
        390.0710144042969,
        370.6150207519531,
        407.3561096191406
      ],
      "text": "efSi,i(X) ≥efSi,i(Y ),\n∀Y ⊂X"
    },
    {
      "page_no": 40,
      "bbox": [
        108.0,
        407.9100646972656,
        505.74346923828125,
        447.10107421875
      ],
      "text": "Definition D.25 (Universal Dynamic Condition 1(for exact function)). We say f has universal\ndynamic condition 1(for exact function), if for all i ∈[n] for all Si ⊆[i −1], Si−1 ⊆[i −2],\n|Si\\Si−1| ≤1, we have"
    },
    {
      "page_no": 40,
      "bbox": [
        235.5549774169922,
        447.74298095703125,
        376.4464111328125,
        465.028076171875
      ],
      "text": "fSi,i(Si) ≥(1 −θ) · fSi−1,i−1(Si)"
    },
    {
      "page_no": 40,
      "bbox": [
        108.0,
        468.96868896484375,
        505.7431945800781,
        508.0050964355469
      ],
      "text": "Definition D.26 (Universal Dynamic Condition 1 (for approximate function)). We say ef has universal\ndynamic condition 1(for approximate function), if for all i ∈[n] for all Si ⊆[i −1], Si−1 ⊆[i −2],\n|Si\\Si−1| ≤1, we have"
    },
    {
      "page_no": 40,
      "bbox": [
        235.5549774169922,
        511.04901123046875,
        376.4464111328125,
        528.3341064453125
      ],
      "text": "efSi,i(Si) ≥(1 −θ) · efSi−1,i−1(Si)"
    },
    {
      "page_no": 40,
      "bbox": [
        107.53195190429688,
        534.3909301757812,
        444.0806884765625,
        566.6571044921875
      ],
      "text": "We define opti as follows:\nDefinition D.27. Let k denote the budget length. For each i ∈[n], we define opti as"
    },
    {
      "page_no": 40,
      "bbox": [
        166.06893920898438,
        567.2990112304688,
        445.9335021972656,
        584.5841064453125
      ],
      "text": "max{fX,i(Y ) | X ⊆[i −1], Y ⊆[i], |X| ≤k, |Y | ≤k, |Y \\X| ≤1}."
    },
    {
      "page_no": 40,
      "bbox": [
        107.1729965209961,
        585.1380615234375,
        505.7432556152344,
        613.4201049804688
      ],
      "text": "Definition D.28 (Universal Dynamic Condition 2). For i ∈[n], we define opti as Definition D.27.\nWe say it has universal dynamic condition if for each i ∈[n], we have"
    },
    {
      "page_no": 40,
      "bbox": [
        256.926025390625,
        613.9969482421875,
        355.0766906738281,
        633.7821044921875
      ],
      "text": "opti ≥(1 −γ) · opti−1."
    },
    {
      "page_no": 40,
      "bbox": [
        108.00009155273438,
        638.6435546875,
        296.56219482421875,
        648.6061401367188
      ],
      "text": "D.12\nInduction Lemma for Exact Function"
    },
    {
      "page_no": 40,
      "bbox": [
        107.69109344482422,
        658.7294921875,
        454.98199462890625,
        683.691650390625
      ],
      "text": "The goal of this section is to prove Lemma D.29\nLemma D.29 (Induction Lemma). For a fixed i, suppose the following conditions hold"
    },
    {
      "page_no": 40,
      "bbox": [
        135.3970947265625,
        693.8760375976562,
        256.6809997558594,
        712.6550903320312
      ],
      "text": "• Set Condition. Si ⊆[i −1]"
    },
    {
      "page_no": 40,
      "bbox": [
        135.3970947265625,
        712.2789916992188,
        258.4446105957031,
        729.5640869140625
      ],
      "text": "• Budget Condition. |Si| ≤k"
    },
    {
      "page_no": 40,
      "bbox": [
        301.01910400390625,
        742.3324584960938,
        310.981689453125,
        752.2950439453125
      ],
      "text": "40"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        73.11663818359375,
        416.96441650390625,
        91.52710723876953
      ],
      "text": "• Value Condition. fSi−1,i−1(Si) ≥(1 −1/e) · (1 −θ)i(1 −γ)iopti"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        92.43226623535156,
        504.001953125,
        120.67206573486328
      ],
      "text": "• Universal Dynamic Condition 1 (for exact function). (See Definition D.25) fSi,i(Si) ≥\n(1 −θ) · fSi−1,i−1(Si)"
    },
    {
      "page_no": 41,
      "bbox": [
        135.3970489501953,
        121.55888366699219,
        465.6781311035156,
        141.3440704345703
      ],
      "text": "• Universal Dynamic Condition 2. (See Definition D.28) opti ≥(1 −γ) · opti+1"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39703369140625,
        139.77589416503906,
        504.001953125,
        168.05409240722656
      ],
      "text": "• Universal Monotone Condition (for exact function). (See Definition D.23) fSi,i(X) ≥\nfSi,i(Y ) for all Y ⊂X"
    },
    {
      "page_no": 41,
      "bbox": [
        107.4420394897461,
        170.55401611328125,
        343.3517761230469,
        182.01063537597656
      ],
      "text": "Then if we construct Si+1 as Definition D.20, then we have"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39703369140625,
        190.3599853515625,
        249.61065673828125,
        209.1391143798828
      ],
      "text": "• Set Condition. Si+1 ⊆[i]"
    },
    {
      "page_no": 41,
      "bbox": [
        135.3970489501953,
        208.59600830078125,
        268.5325622558594,
        225.88111877441406
      ],
      "text": "• Budget Condition. |Si+1| ≤k"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39703369140625,
        225.70867919921875,
        438.3340759277344,
        244.1181182861328
      ],
      "text": "• Value Condition. fSi,i(Si+1) ≥(1 −1/e) · (1 −θ)i+1(1 −γ)i+1opti+1"
    },
    {
      "page_no": 41,
      "bbox": [
        107.99996948242188,
        250.0400390625,
        235.5008544921875,
        260.088134765625
      ],
      "text": "Proof. Proof of Set Condition."
    },
    {
      "page_no": 41,
      "bbox": [
        107.99996948242188,
        266.43902587890625,
        481.3016052246094,
        285.2191162109375
      ],
      "text": "Note that Si ⊆[i −1], by using the way we construct Si+1, then it is obvious that Si+1 ⊆[i]."
    },
    {
      "page_no": 41,
      "bbox": [
        107.99993896484375,
        282.9025573730469,
        222.9483642578125,
        292.8651428222656
      ],
      "text": "Proof of Budget Condition."
    },
    {
      "page_no": 41,
      "bbox": [
        107.99993896484375,
        299.2160339355469,
        498.19659423828125,
        316.5011291503906
      ],
      "text": "Note that |Si| ≤k, by using the way we construct Si+1, then it is straightforward that |Si+1| ≤k."
    },
    {
      "page_no": 41,
      "bbox": [
        107.99993896484375,
        315.6795654296875,
        216.4925994873047,
        325.64215087890625
      ],
      "text": "Proof of Value Condition."
    },
    {
      "page_no": 41,
      "bbox": [
        107.53193664550781,
        332.1595153808594,
        177.5889129638672,
        342.12213134765625
      ],
      "text": "We can show that"
    },
    {
      "page_no": 41,
      "bbox": [
        176.6689453125,
        348.4310607910156,
        327.37042236328125,
        379.6141662597656
      ],
      "text": "fSi,i(Si+1) ≥fSi,i(Si)\n≥(1 −θ) · fSi−1,i−1(Si)"
    },
    {
      "page_no": 41,
      "bbox": [
        225.3069610595703,
        376.7397155761719,
        435.3314514160156,
        398.0831604003906
      ],
      "text": "≥(1 −θ) · ((1 −1/e)(1 −θ)i(1 −γ)iopti −i · ϵ0)"
    },
    {
      "page_no": 41,
      "bbox": [
        225.30699157714844,
        392.7257080078125,
        424.189453125,
        411.6341552734375
      ],
      "text": "≥(1 −θ) · ((1 −1/e)(1 −θ)i(1 −γ)i+1opti+1)"
    },
    {
      "page_no": 41,
      "bbox": [
        225.30699157714844,
        408.7117004394531,
        399.43560791015625,
        427.6201477050781
      ],
      "text": "≥(1 −1/e) · (1 −θ)i+1(1 −γ)i+1opti+1."
    },
    {
      "page_no": 41,
      "bbox": [
        107.64099884033203,
        426.77044677734375,
        503.9964904785156,
        458.71807861328125
      ],
      "text": "where the first step follows from Universal Monotone Condition, the second step follows from\nUniversal Dynamic Condition 1, the third step follows from Value Condition, the forth step follows\nfrom Universal Dynamic Condition 2, and the last step follows from simple algebra."
    },
    {
      "page_no": 41,
      "bbox": [
        107.69100952148438,
        465.14447021484375,
        224.45266723632812,
        475.1070861816406
      ],
      "text": "Thus, we complete the proof."
    },
    {
      "page_no": 41,
      "bbox": [
        108.0,
        489.25152587890625,
        328.23321533203125,
        499.214111328125
      ],
      "text": "D.13\nInduction Lemma for Approximate Function"
    },
    {
      "page_no": 41,
      "bbox": [
        107.69100189208984,
        509.3175048828125,
        454.9819030761719,
        533.6786499023438
      ],
      "text": "The goal of this section is to prove Lemma D.30\nLemma D.30 (Induction Lemma). For a fixed i, suppose the following conditions hold"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        543.2630004882812,
        256.680908203125,
        562.0421142578125
      ],
      "text": "• Set Condition. Si ⊆[i −1]"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        561.4990234375,
        258.4455261230469,
        578.7841186523438
      ],
      "text": "• Budget Condition. |Si| ≤k"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        578.6107177734375,
        448.2870788574219,
        599.4561157226562
      ],
      "text": "• Value Condition. fSi−1,i−1(Si) ≥(1 −1/e) · (1 −θ)i(1 −γ)iopti −i · ϵ0"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        597.884033203125,
        504.6698303222656,
        628.1151123046875
      ],
      "text": "• Universal Dynamic Condition 1 (for approximate function). (see Definition D.26)\nefSi,i(Si) ≥(1 −θ) · efSi−1,i−1(Si)"
    },
    {
      "page_no": 41,
      "bbox": [
        135.3970184326172,
        629.0018920898438,
        378.2461242675781,
        648.787109375
      ],
      "text": "• Universal Dynamic Condition 2. opti ≥(1 −γ) · opti+1"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39703369140625,
        650.403564453125,
        504.0,
        671.6006469726562
      ],
      "text": "• Universal Approximate Condition. (See Definition D.28) fSi,i(X) ≥efSi,i(X) −ϵ0 for\nall X"
    },
    {
      "page_no": 41,
      "bbox": [
        135.39700317382812,
        679.5380249023438,
        504.6720886230469,
        709.7691040039062
      ],
      "text": "• Universal Monotone Condition (for approximate function).\n(see Definition D.24)\nefSi,i(X) ≥efSi,i(Y ) for all Y ⊂X"
    },
    {
      "page_no": 41,
      "bbox": [
        107.44203186035156,
        712.2680053710938,
        343.3517761230469,
        723.724609375
      ],
      "text": "Then if we construct Si+1 as Definition D.22, then we have"
    },
    {
      "page_no": 41,
      "bbox": [
        301.01904296875,
        742.3324584960938,
        310.98162841796875,
        752.2950439453125
      ],
      "text": "41"
    },
    {
      "page_no": 42,
      "bbox": [
        135.39700317382812,
        74.24199676513672,
        249.61062622070312,
        93.02112579345703
      ],
      "text": "• Set Condition. Si+1 ⊆[i]"
    },
    {
      "page_no": 42,
      "bbox": [
        135.3970184326172,
        93.34398651123047,
        268.53253173828125,
        110.62909698486328
      ],
      "text": "• Budget Condition. |Si+1| ≤k"
    },
    {
      "page_no": 42,
      "bbox": [
        135.39700317382812,
        111.32269287109375,
        494.5580139160156,
        132.16712951660156
      ],
      "text": "• Value Condition. fSi,i(Si+1) ≥(1 −1/e) · (1 −θ)i+1(1 −γ)i+1opti+1 −(i + 1) · ϵ0"
    },
    {
      "page_no": 42,
      "bbox": [
        107.99993896484375,
        140.49005126953125,
        235.50181579589844,
        150.5381622314453
      ],
      "text": "Proof. Proof of Set Condition."
    },
    {
      "page_no": 42,
      "bbox": [
        107.99993133544922,
        156.8890380859375,
        481.3016052246094,
        175.6691436767578
      ],
      "text": "Note that Si ⊆[i −1], by using the way we construct Si+1, then it is obvious that Si+1 ⊆[i]."
    },
    {
      "page_no": 42,
      "bbox": [
        107.99993896484375,
        173.3525390625,
        222.9483642578125,
        183.3151397705078
      ],
      "text": "Proof of Budget Condition."
    },
    {
      "page_no": 42,
      "bbox": [
        107.99993896484375,
        189.666015625,
        498.19659423828125,
        206.9511260986328
      ],
      "text": "Note that |Si| ≤k, by using the way we construct Si+1, then it is straightforward that |Si+1| ≤k."
    },
    {
      "page_no": 42,
      "bbox": [
        107.99993896484375,
        206.1295166015625,
        216.4925994873047,
        216.0921173095703
      ],
      "text": "Proof of Value Condition."
    },
    {
      "page_no": 42,
      "bbox": [
        107.53193664550781,
        222.6084747314453,
        177.5889129638672,
        232.57107543945312
      ],
      "text": "We can show that"
    },
    {
      "page_no": 42,
      "bbox": [
        156.2369384765625,
        241.77001953125,
        281.1800231933594,
        259.05511474609375
      ],
      "text": "fSi,i(Si+1) ≥efSi,i(Si+1) −ϵ0"
    },
    {
      "page_no": 42,
      "bbox": [
        204.8749542236328,
        258.85205078125,
        271.093017578125,
        276.13714599609375
      ],
      "text": "≥efSi,i(Si) −ϵ0"
    },
    {
      "page_no": 42,
      "bbox": [
        204.8749542236328,
        275.93408203125,
        455.26904296875,
        343.66015625
      ],
      "text": "≥(1 −θ) · efSi−1,i−1(Si) −ϵ0\n≥(1 −θ) · ((1 −1/e)(1 −θ)i(1 −γ)iopti −i · ϵ0) −ϵ0\n≥(1 −θ) · ((1 −1/e)(1 −θ)i(1 −γ)i+1opti+1 −i · ϵ0) −ϵ0\n≥(1 −1/e) · (1 −θ)i+1(1 −γ)i+1opti+1 −(i + 1) · ϵ0."
    },
    {
      "page_no": 42,
      "bbox": [
        107.6419677734375,
        343.340576171875,
        504.0038146972656,
        386.1210632324219
      ],
      "text": "where the first step follows from Universal Approximate Condition, the second step follows from\nUniversal Monotone Condition, the third step follows from Universal Dynamic Condition 1, the\nforth step follows from Value Condition, the fifth step follows from Universal Dynamic Condition\n2, and the last step follows from simple algebra."
    },
    {
      "page_no": 42,
      "bbox": [
        107.69100189208984,
        392.5474548339844,
        224.45266723632812,
        402.51007080078125
      ],
      "text": "Thus, we complete the proof."
    },
    {
      "page_no": 42,
      "bbox": [
        108.0,
        418.53851318359375,
        215.5363311767578,
        428.5010986328125
      ],
      "text": "D.14\nTheoretical Result"
    },
    {
      "page_no": 42,
      "bbox": [
        107.53199768066406,
        439.25445556640625,
        461.4831237792969,
        449.2170715332031
      ],
      "text": "We first give the guarantee of our full-knowledge version (without cache size limitation)."
    },
    {
      "page_no": 42,
      "bbox": [
        108.0,
        454.65826416015625,
        504.0029296875,
        486.474609375
      ],
      "text": "Lemma D.31 (Formal version of Lemma 3.1). Under the mild assumption, let k denote any target\nsize. If we greedily compute the attention score based on full information, then we can find the set Si\nsuch that"
    },
    {
      "page_no": 42,
      "bbox": [
        238.88099670410156,
        495.783935546875,
        373.12158203125,
        513.1340942382812
      ],
      "text": "f(Si) ≥(1 −1/e) · (1 −α)opti,"
    },
    {
      "page_no": 42,
      "bbox": [
        108.0,
        515.1640014648438,
        240.7104034423828,
        532.4600830078125
      ],
      "text": "where α ∈(0, 1) are parameters."
    },
    {
      "page_no": 42,
      "bbox": [
        108.0,
        543.218017578125,
        503.9990234375,
        564.3496704101562
      ],
      "text": "Proof. The proof follows from using Theorem D.17, Corollary D.18, Lemma D.29 with choosing\nθ = γ = α/(10n)."
    },
    {
      "page_no": 42,
      "bbox": [
        107.6709976196289,
        582.28125,
        503.997314453125,
        603.22802734375
      ],
      "text": "Next, we show the guarantee for our robust and approximate greedy eviction policy algorithm\n(Algorithm 2)."
    },
    {
      "page_no": 42,
      "bbox": [
        107.6709976196289,
        608.78271484375,
        505.7454833984375,
        649.6571044921875
      ],
      "text": "Theorem D.32 (Formal version of Theorem 4.4). Under the mild assumption, let k denote the budget\nof space limitation. If for each token, we greedily compute the attention score based on top-k choice,\nthen we can show the set eSi we generate each for token i ∈[n] satisfy that"
    },
    {
      "page_no": 42,
      "bbox": [
        229.71200561523438,
        653.9249267578125,
        382.2964782714844,
        673.7100830078125
      ],
      "text": "f(eSi) ≥(1 −1/e) · (1 −α)opti −β,"
    },
    {
      "page_no": 42,
      "bbox": [
        108.00006103515625,
        673.3049926757812,
        269.5634460449219,
        690.60107421875
      ],
      "text": "where α ∈(0, 1), β > 0 are parameters."
    },
    {
      "page_no": 42,
      "bbox": [
        108.0,
        701.3590087890625,
        503.9990234375,
        723.9846801757812
      ],
      "text": "Proof. The proof follows from using Theorem D.17, Corollary D.18, Lemma D.30 with choosing\nϵ0 = β/(10n) and θ = γ = α/(10n)."
    },
    {
      "page_no": 42,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "42"
    },
    {
      "page_no": 43,
      "bbox": [
        108.0,
        74.31652069091797,
        391.17694091796875,
        84.27912139892578
      ],
      "text": "D.15\nExtended Related Work for Theoretical Attention Problems"
    },
    {
      "page_no": 43,
      "bbox": [
        107.69100189208984,
        91.97538757324219,
        504.35113525390625,
        287.54205322265625
      ],
      "text": "The static attention computation is asking the following question that given Q, K, V ∈Rn×d, the\ngoal is to D−1 exp(QK⊤)V where D = diag(exp(QK⊤)1n). [98] studied the static attention\ncomputation from both algorithm and hardness. On the positive, they provide an almost linear time\nalgorithm to approximately compute the attention matrix. On the negative side, assuming a strong\nexponential time hypothesis (SETH), they prove a hardness result. Their hardness result is, unless\nSETH fails, there is no algorithm that runs in truly subquadratic time to approximately compute\nthe attention matrix. Further, [100] considers the dynamic of attention computation problem. They\nalso provide both algorithmic results and hardness results. In the work of [103], they consider\nthe sparsification of attention matrix construction. In particular, they assume that situation that\nd ≫n, and show how to sparsify the columns of matrix Q. [103] provides two algorithms, one\nis a randomized algorithm, and the other is a deterministic algorithm. Differential privacy is a\nfamous and textbook topic in graduate school, recently the work of [101] shows how to give a\ndifferentially private algorithm for computing the attention matrix. For a given A ∈Rn×d and\nvector b ∈Rn, [104] formulates and studies exponential regression minx ∥exp(Ax) −b∥2. Then\n[105] considers the normalization factor in exponential regression and defines the softmax regression\nproblem minx ∥⟨exp(Ax), 1n⟩−1 exp(Ax) −b∥2. [107] moves the scaling factor from exp(Ax) to\nb and defines a rescaled softmax regression problem minx ∥exp(Ax) −⟨exp(Ax), 1n⟩· b∥2."
    },
    {
      "page_no": 43,
      "bbox": [
        108.00003051757812,
        292.85748291015625,
        220.97592163085938,
        302.820068359375
      ],
      "text": "D.16\nSparsity Preserving"
    },
    {
      "page_no": 43,
      "bbox": [
        108.0,
        312.8895568847656,
        503.99932861328125,
        358.5361022949219
      ],
      "text": "Recall that in Figure 2, we observe that even when trained densely, the attention matrices of LLMs\nare over 95% sparse at inference time. Only 5% of the KV cache is sufficient for decoding the same\noutput token at each generation step. Here, we provide some formal formulations for sparsity.\nDefinition D.33. Suppose the following conditions"
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        367.8900146484375,
        199.4436492919922,
        386.68109130859375
      ],
      "text": "• Let S0 ⊂[m]."
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        386.02801513671875,
        196.1966552734375,
        403.3240966796875
      ],
      "text": "• Let k = |S0|."
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        404.1670227050781,
        374.0885009765625,
        421.4631042480469
      ],
      "text": "• Let τ ∈(0, 1) denote a threshold for truncating the value."
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        422.3050231933594,
        411.1806335449219,
        439.6011047363281
      ],
      "text": "• Let α ∈(0, 1) denote a fraction of mass (larger than τ) outside S0."
    },
    {
      "page_no": 43,
      "bbox": [
        135.39697265625,
        437.8750305175781,
        260.15264892578125,
        458.7751159667969
      ],
      "text": "• Let mapping D : Rd →Rm\n≥0."
    },
    {
      "page_no": 43,
      "bbox": [
        135.39698791503906,
        458.60003662109375,
        383.2746276855469,
        479.5111083984375
      ],
      "text": "• For each x ∈Rd, D(x) ∈Rm is a vector that has length m."
    },
    {
      "page_no": 43,
      "bbox": [
        107.1729736328125,
        481.6170349121094,
        404.01861572265625,
        498.9131164550781
      ],
      "text": "We say the distribution D is (α, τ, k)-good if the following conditions hold"
    },
    {
      "page_no": 43,
      "bbox": [
        135.39697265625,
        499.825439453125,
        285.87890625,
        520.8461303710938
      ],
      "text": "• For all x ∈Rd, S0 ⊂suppτ(D(x))"
    },
    {
      "page_no": 43,
      "bbox": [
        108.0,
        518.9994506835938,
        504.00189208984375,
        557.0606079101562
      ],
      "text": "• For all x ∈Rd, | suppτ(D(x))\\S0| ≤α · k\nClaim D.34. Suppose we sample n points {x1, x2, · · · , xn} ⊂Rd from (α, τ, k)-good distribution\nuniformly at random, then we have"
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        566.510986328125,
        236.4234619140625,
        585.2901000976562
      ],
      "text": "• S0 ⊆∩i∈[n] suppτ(xi)"
    },
    {
      "page_no": 43,
      "bbox": [
        135.39700317382812,
        585.7509765625,
        284.9738464355469,
        603.0360717773438
      ],
      "text": "• |(∪i∈[n] suppτ(D(x)))\\S0| ≤αkn"
    },
    {
      "page_no": 43,
      "bbox": [
        108.0,
        609.9609985351562,
        359.5019226074219,
        628.7510986328125
      ],
      "text": "Proof. Since for all i ∈[n], we have S0 ⊆suppτ(D(xi)), thus"
    },
    {
      "page_no": 43,
      "bbox": [
        258.33795166015625,
        625.4259643554688,
        353.66448974609375,
        644.205078125
      ],
      "text": "S0 ⊆∩i∈[n] suppτ(xi)."
    },
    {
      "page_no": 43,
      "bbox": [
        107.69090270996094,
        646.5254516601562,
        261.6728210449219,
        656.488037109375
      ],
      "text": "Therefore we proved the first property."
    },
    {
      "page_no": 43,
      "bbox": [
        107.53190612792969,
        662.7479858398438,
        392.68817138671875,
        680.0330810546875
      ],
      "text": "We know that for all i ∈[n], we have | suppτ(D(x))\\S0| ≤αkn. Thus"
    },
    {
      "page_no": 43,
      "bbox": [
        173.2178955078125,
        687.2319946289062,
        296.9899597167969,
        704.51708984375
      ],
      "text": "|(∪i∈[n] suppτ(D(xi)))\\S0| ≤"
    },
    {
      "page_no": 43,
      "bbox": [
        299.7508544921875,
        677.2696533203125,
        314.1468200683594,
        694.8289794921875
      ],
      "text": "n\nX"
    },
    {
      "page_no": 43,
      "bbox": [
        300.49285888671875,
        687.2319946289062,
        438.4782409667969,
        708.4514770507812
      ],
      "text": "i=1\n| suppτ(D(xi)))\\S0| ≤n · αk"
    },
    {
      "page_no": 43,
      "bbox": [
        107.6907958984375,
        712.4444580078125,
        323.9488830566406,
        722.4070434570312
      ],
      "text": "Therefore, we finish the proof for the second property."
    },
    {
      "page_no": 43,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "43"
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        74.31652069091797,
        252.82627868652344,
        84.27912139892578
      ],
      "text": "D.17\nDefinition of Loss Function"
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        94.30728149414062,
        505.74664306640625,
        162.67262268066406
      ],
      "text": "In this section, we follow the theoretical softmax regression literature [105] and define a number\nof functions to make the calculations of gradient and Hessian convenient. We also proposed a new\npenalty term (ℓ1 type sparsity penalty, see Definition D.41) into the final loss function, which is not\nstudied in previous work [104, 105, 107, 110, 132, 133]. We first provide some function definitions.\nDefinition D.35 (Function u, [105]). Given matrix A ∈Rn×d, let function u : Rd →Rn be defined\nas follows"
    },
    {
      "page_no": 44,
      "bbox": [
        270.333984375,
        170.78805541992188,
        341.6674499511719,
        180.7506561279297
      ],
      "text": "u(x) := exp(Ax)"
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        188.31031799316406,
        505.65350341796875,
        217.5871124267578
      ],
      "text": "Definition D.36 (Function α, see Definition 5.4 in [105] as an example). We define u(x) as Defini-\ntion D.35. Then we define α : Rd →R as follows"
    },
    {
      "page_no": 44,
      "bbox": [
        266.8110046386719,
        218.05393981933594,
        345.189453125,
        235.4041290283203
      ],
      "text": "α(x) := ⟨u(x), 1n⟩"
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        235.99203491210938,
        504.0043029785156,
        256.797607421875
      ],
      "text": "Definition D.37 (Function f, see Definition 5.1 in [105] as an example). Provided that the following\nconditions are true"
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        267.1820068359375,
        281.078369140625,
        277.4046630859375
      ],
      "text": "• We define u(x) as Definition D.35."
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        285.5570068359375,
        279.2947082519531,
        295.7796630859375
      ],
      "text": "• We define α(x) as Definition D.36"
    },
    {
      "page_no": 44,
      "bbox": [
        107.72100830078125,
        303.32501220703125,
        298.0882568359375,
        324.2370910644531
      ],
      "text": "Let function f : Rd →Rn be defined as follows"
    },
    {
      "page_no": 44,
      "bbox": [
        262.0470275878906,
        322.9234619140625,
        349.95458984375,
        335.0090637207031
      ],
      "text": "f(x) := α(x)−1u(x)."
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        342.62255859375,
        504.00323486328125,
        363.4476013183594
      ],
      "text": "Definition D.38 (Function c, see Definition 5.5 in [105] as an example). Provided that the following\nconditions are true"
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        371.6014404296875,
        236.20765686035156,
        391.12811279296875
      ],
      "text": "• Given a vector b ∈Rn."
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        392.20703125,
        299.67626953125,
        402.4296875
      ],
      "text": "• Let f(x) be defined as Definition D.38."
    },
    {
      "page_no": 44,
      "bbox": [
        107.4419937133789,
        409.97503662109375,
        306.41192626953125,
        430.8871154785156
      ],
      "text": "Then, let function c : Rd →Rn defined as follows"
    },
    {
      "page_no": 44,
      "bbox": [
        108.0,
        433.93804931640625,
        505.3865966796875,
        478.4070739746094
      ],
      "text": "• c(x) := f(x) −b.\nDefinition D.39 (Loss function Lexp, see Definition 5.3 in [105] as an example). We define Lexp :\nRd →R"
    },
    {
      "page_no": 44,
      "bbox": [
        254.11099243164062,
        477.3166198730469,
        357.8916015625,
        496.2250671386719
      ],
      "text": "Lexp(x) := 0.5 · ∥c(x)∥2\n2."
    },
    {
      "page_no": 44,
      "bbox": [
        108.00001525878906,
        496.3343505859375,
        335.0376281738281,
        515.8610229492188
      ],
      "text": "Definition D.40 (Loss function Lreg). Given A ∈Rn×d."
    },
    {
      "page_no": 44,
      "bbox": [
        107.72096252441406,
        512.7449340820312,
        304.7272033691406,
        533.64501953125
      ],
      "text": "Let function Lreg : Rd →R be defined as follows"
    },
    {
      "page_no": 44,
      "bbox": [
        240.87896728515625,
        534.099609375,
        370.6270446777344,
        553.008056640625
      ],
      "text": "Lreg(x) := 0.5 · ∥diag(w)Ax∥2\n2"
    },
    {
      "page_no": 44,
      "bbox": [
        107.531982421875,
        559.1854248046875,
        393.6866760253906,
        591.1730346679688
      ],
      "text": "We define a novel penalty function\nDefinition D.41 (Implicitly controlling the sparsity). Given A ∈Rn×d."
    },
    {
      "page_no": 44,
      "bbox": [
        107.17300415039062,
        590.2659301757812,
        145.260009765625,
        600.228515625
      ],
      "text": "We define"
    },
    {
      "page_no": 44,
      "bbox": [
        249.44700622558594,
        608.0939331054688,
        362.5555725097656,
        625.3790283203125
      ],
      "text": "Lsparse(x) := ∥exp(Ax)∥1."
    },
    {
      "page_no": 44,
      "bbox": [
        107.69097900390625,
        631.556396484375,
        505.65264892578125,
        667.1195678710938
      ],
      "text": "Then it is obvious that we have\nClaim D.42. Given A ∈Rn×d. Let u be defined as Definition D.35. Let α be defined as Defini-\ntion D.36."
    },
    {
      "page_no": 44,
      "bbox": [
        107.1729965209961,
        673.5460205078125,
        140.27871704101562,
        683.5086059570312
      ],
      "text": "We have"
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        693.8389282226562,
        258.6734619140625,
        711.1890869140625
      ],
      "text": "• Lsparse(x) = ⟨exp(Ax), 1n⟩"
    },
    {
      "page_no": 44,
      "bbox": [
        135.39700317382812,
        712.2139282226562,
        241.68447875976562,
        729.5640869140625
      ],
      "text": "• Lsparse(x) = ⟨u(x), 1n⟩"
    },
    {
      "page_no": 44,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "44"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39700317382812,
        74.40748596191406,
        219.06346130371094,
        85.20047760009766
      ],
      "text": "• Lsparse(x) = α(x)"
    },
    {
      "page_no": 45,
      "bbox": [
        107.67098999023438,
        97.58502197265625,
        505.16644287109375,
        118.63308715820312
      ],
      "text": "Proof. The proof is trivially following from the definition of u(x) (see Definition D.35) and α(x)\n(see Definition D.36)."
    },
    {
      "page_no": 45,
      "bbox": [
        107.69100189208984,
        128.49307250976562,
        505.1664733886719,
        167.66615295410156
      ],
      "text": "The final loss function can be defined as follows. Intuitively, we can write attention D−1 exp(QK⊤)\ninto n subproblems where each subproblem can be viewed as one softmax problem.\nDefinition D.43. If the following conditions hold"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39703369140625,
        177.95904541015625,
        281.33837890625,
        189.41664123535156
      ],
      "text": "• We define Lexp as Definition D.39."
    },
    {
      "page_no": 45,
      "bbox": [
        135.39703369140625,
        196.33203125,
        279.9443664550781,
        207.7886505126953
      ],
      "text": "• We define Lreg as Definition D.40."
    },
    {
      "page_no": 45,
      "bbox": [
        135.3970184326172,
        214.70501708984375,
        290.55035400390625,
        226.16163635253906
      ],
      "text": "• We define Lsparse as Definition D.41."
    },
    {
      "page_no": 45,
      "bbox": [
        107.44200897216797,
        235.0460205078125,
        211.62640380859375,
        245.26866149902344
      ],
      "text": "Then we define L function"
    },
    {
      "page_no": 45,
      "bbox": [
        221.12100219726562,
        253.10604858398438,
        390.8815612792969,
        263.8164367675781
      ],
      "text": "L(x) := Lexp(x) + Lsparse(x) + Lreg(x)."
    },
    {
      "page_no": 45,
      "bbox": [
        107.99996948242188,
        277.3755187988281,
        176.15411376953125,
        287.3381042480469
      ],
      "text": "D.18\nGradient"
    },
    {
      "page_no": 45,
      "bbox": [
        107.99996948242188,
        297.44146728515625,
        485.43292236328125,
        322.26507568359375
      ],
      "text": "Next, we show the gradient of Lexp.\nLemma D.44 (Gradient, Lemma 5.6 in [105]). Provided that the following conditions are true"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39698791503906,
        328.8520202636719,
        327.27667236328125,
        349.7630920410156
      ],
      "text": "• Given matrix A ∈Rn×d and a vector b ∈Rn."
    },
    {
      "page_no": 45,
      "bbox": [
        135.39700317382812,
        347.2250061035156,
        413.9026794433594,
        369.63006591796875
      ],
      "text": "• Let A∗,i ∈Rn denote the i-th column of matrix A, for every i ∈[d]."
    },
    {
      "page_no": 45,
      "bbox": [
        135.39700317382812,
        369.2129821777344,
        281.78436279296875,
        379.4356384277344
      ],
      "text": "• We define α(x) as Definition D.36."
    },
    {
      "page_no": 45,
      "bbox": [
        135.3970184326172,
        387.5859680175781,
        281.3243713378906,
        397.8086242675781
      ],
      "text": "• We define f(x) as Definition D.37."
    },
    {
      "page_no": 45,
      "bbox": [
        135.39703369140625,
        405.9589538574219,
        279.6864013671875,
        416.1816101074219
      ],
      "text": "• We define c(x) as Definition D.38."
    },
    {
      "page_no": 45,
      "bbox": [
        135.39706420898438,
        424.3309631347656,
        294.7814025878906,
        435.3013916015625
      ],
      "text": "• We define Lexp(x) as Definition D.39."
    },
    {
      "page_no": 45,
      "bbox": [
        135.3970947265625,
        442.7039489746094,
        271.44622802734375,
        460.0000305175781
      ],
      "text": "• Let ◦denote hadamard product."
    },
    {
      "page_no": 45,
      "bbox": [
        107.39208221435547,
        463.0449523925781,
        211.369873046875,
        480.3410339355469
      ],
      "text": "For every i ∈[d], we have"
    },
    {
      "page_no": 45,
      "bbox": [
        135.3970947265625,
        483.3859558105469,
        170.43516540527344,
        493.5250244140625
      ],
      "text": "• Part 1."
    },
    {
      "page_no": 45,
      "bbox": [
        264.235107421875,
        499.125,
        307.5665588378906,
        509.0876159667969
      ],
      "text": "d exp(Ax)"
    },
    {
      "page_no": 45,
      "bbox": [
        278.62799072265625,
        505.615966796875,
        384.3244934082031,
        523.408447265625
      ],
      "text": "dxi\n= exp(Ax) ◦A∗,i"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39697265625,
        534.0549926757812,
        170.43605041503906,
        544.1940307617188
      ],
      "text": "• Part 2."
    },
    {
      "page_no": 45,
      "bbox": [
        252.0169677734375,
        549.4808959960938,
        317.0164489746094,
        566.8310546875
      ],
      "text": "d⟨exp(Ax), 1n⟩"
    },
    {
      "page_no": 45,
      "bbox": [
        277.2439880371094,
        556.2860107421875,
        397.0444641113281,
        574.0784912109375
      ],
      "text": "dxi\n= ⟨exp(Ax), A∗,i⟩"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39700317382812,
        584.7250366210938,
        170.4360809326172,
        594.8640747070312
      ],
      "text": "• Part 3."
    },
    {
      "page_no": 45,
      "bbox": [
        252.10000610351562,
        599.5054931640625,
        287.6890563964844,
        611.591064453125
      ],
      "text": "dα(x)−1"
    },
    {
      "page_no": 45,
      "bbox": [
        262.8689880371094,
        603.47900390625,
        396.96246337890625,
        625.3854370117188
      ],
      "text": "dxi\n= −α(x)−1 · ⟨f(x), A∗,i⟩"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39700317382812,
        636.031982421875,
        170.4360809326172,
        646.1710205078125
      ],
      "text": "• Part 4."
    },
    {
      "page_no": 45,
      "bbox": [
        212.59100341796875,
        651.7720947265625,
        237.5194549560547,
        661.7346801757812
      ],
      "text": "df(x)"
    },
    {
      "page_no": 45,
      "bbox": [
        217.78199768066406,
        651.7720947265625,
        276.48345947265625,
        676.0554809570312
      ],
      "text": "dxi\n= dc(x)"
    },
    {
      "page_no": 45,
      "bbox": [
        257.56500244140625,
        658.2620239257812,
        435.9685974121094,
        676.0554809570312
      ],
      "text": "dxi\n= −⟨f(x), A∗,i⟩· f(x) + f(x) ◦A∗,i"
    },
    {
      "page_no": 45,
      "bbox": [
        135.39706420898438,
        686.7020263671875,
        170.43614196777344,
        696.841064453125
      ],
      "text": "• Part 5."
    },
    {
      "page_no": 45,
      "bbox": [
        186.80006408691406,
        702.4411010742188,
        225.1845245361328,
        713.1514892578125
      ],
      "text": "dLexp(x)"
    },
    {
      "page_no": 45,
      "bbox": [
        198.718994140625,
        707.0864868164062,
        462.2639465332031,
        728.6800537109375
      ],
      "text": "dxi\n= A⊤\n∗,i · (f(x)(f(x) −b)⊤f(x) + diag(f(x))(f(x) −b))"
    },
    {
      "page_no": 45,
      "bbox": [
        301.01904296875,
        742.3324584960938,
        310.98162841796875,
        752.2950439453125
      ],
      "text": "45"
    },
    {
      "page_no": 46,
      "bbox": [
        108.0,
        74.31652069091797,
        170.62490844726562,
        84.27912139892578
      ],
      "text": "D.19\nHessian"
    },
    {
      "page_no": 46,
      "bbox": [
        108.0,
        94.38246154785156,
        503.9959716796875,
        129.6455841064453
      ],
      "text": "Here, we compute the Hessian for several functions.\nLemma D.45 (Hessian of u(x), Lemma 5.9 in [105]). Provided that the following conditions are\ntrue"
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        137.50035095214844,
        250.89366149902344,
        157.0270538330078
      ],
      "text": "• Given a matrix A ∈Rn×d."
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970184326172,
        154.41497802734375,
        413.07568359375,
        176.82106018066406
      ],
      "text": "• For every i ∈[d], let A∗,i ∈Rn denote the i-th column of matrix A."
    },
    {
      "page_no": 46,
      "bbox": [
        135.39703369140625,
        176.3299560546875,
        271.4461669921875,
        193.6260528564453
      ],
      "text": "• Let ◦denote hadamard product."
    },
    {
      "page_no": 46,
      "bbox": [
        107.44203186035156,
        196.376953125,
        231.1976318359375,
        213.6730499267578
      ],
      "text": "Then, we have, for each i ∈[d]"
    },
    {
      "page_no": 46,
      "bbox": [
        135.39703369140625,
        216.4249267578125,
        170.4361114501953,
        226.56399536132812
      ],
      "text": "• Part 1."
    },
    {
      "page_no": 46,
      "bbox": [
        257.1720275878906,
        228.45394897460938,
        304.9734802246094,
        242.0315399169922
      ],
      "text": "d2 exp(Ax)"
    },
    {
      "page_no": 46,
      "bbox": [
        273.2229919433594,
        238.5599365234375,
        391.3875427246094,
        257.6354064941406
      ],
      "text": "dx2\ni\n= A∗,i ◦u(x) ◦A∗,i"
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970184326172,
        267.5489501953125,
        170.43609619140625,
        277.6880187988281
      ],
      "text": "• Part 2."
    },
    {
      "page_no": 46,
      "bbox": [
        256.7320251464844,
        279.5780029296875,
        304.5325012207031,
        293.1556091308594
      ],
      "text": "d2 exp(Ax)"
    },
    {
      "page_no": 46,
      "bbox": [
        265.64599609375,
        289.6839904785156,
        391.8275451660156,
        308.46307373046875
      ],
      "text": "dxidxj\n= A∗,j ◦u(x) ◦A∗,i"
    },
    {
      "page_no": 46,
      "bbox": [
        108.00003051757812,
        313.3769836425781,
        450.573974609375,
        323.51605224609375
      ],
      "text": "Lemma D.46 (Lemma 5.10 in [105]). Provided that the following conditions are true"
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970489501953,
        333.4239807128906,
        281.785400390625,
        343.6466369628906
      ],
      "text": "• We define α(x) as Definition D.36."
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970489501953,
        348.10797119140625,
        413.07574462890625,
        370.5140380859375
      ],
      "text": "• For every i ∈[d], let A∗,i ∈Rn denote the i-th column of matrix A."
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970947265625,
        370.0229797363281,
        271.44622802734375,
        387.3190612792969
      ],
      "text": "• Let ◦denote hadamard product."
    },
    {
      "page_no": 46,
      "bbox": [
        107.44209289550781,
        390.07098388671875,
        164.71707153320312,
        400.0335693359375
      ],
      "text": "Then, we have"
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970947265625,
        410.11798095703125,
        170.43617248535156,
        420.2570495605469
      ],
      "text": "• Part 1."
    },
    {
      "page_no": 46,
      "bbox": [
        264.7601013183594,
        424.3886413574219,
        294.6175537109375,
        435.72564697265625
      ],
      "text": "d2α(x)"
    },
    {
      "page_no": 46,
      "bbox": [
        271.8389892578125,
        432.2530212402344,
        384.3024597167969,
        451.3284606933594
      ],
      "text": "dx2\ni\n= ⟨u(x), A∗,i ◦A∗,i⟩"
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        461.2420349121094,
        170.4360809326172,
        471.381103515625
      ],
      "text": "• Part 2."
    },
    {
      "page_no": 46,
      "bbox": [
        264.2619934082031,
        475.5126953125,
        384.8004455566406,
        502.1570739746094
      ],
      "text": "d2α(x)\ndxidxj\n= ⟨u(x), A∗,i ◦A∗,j⟩"
    },
    {
      "page_no": 46,
      "bbox": [
        135.39698791503906,
        512.4400024414062,
        170.43606567382812,
        522.5790405273438
      ],
      "text": "• Part 3."
    },
    {
      "page_no": 46,
      "bbox": [
        268.0489807128906,
        526.710693359375,
        297.9074401855469,
        538.0466918945312
      ],
      "text": "d2α(x)"
    },
    {
      "page_no": 46,
      "bbox": [
        275.12799072265625,
        532.728515625,
        381.011962890625,
        551.6206665039062
      ],
      "text": "dx2\n= A⊤diag(u(x))A"
    },
    {
      "page_no": 46,
      "bbox": [
        108.0,
        562.7465209960938,
        250.75411987304688,
        572.7091064453125
      ],
      "text": "D.20\nHessian is Positive Definite"
    },
    {
      "page_no": 46,
      "bbox": [
        108.0,
        582.7522583007812,
        504.00396728515625,
        629.0701293945312
      ],
      "text": "It is well known that in literature [104, 105, 107], the Hessian H of loss function can be written as\nA⊤(B(x) + W 2)A for some matrix function B(x) ∈Rn×n (for example see explanation in Section\n5.10 in [105]). In this section, we show that Hessian is positive definite.\nLemma D.47. If the following conditions hold"
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        636.8394165039062,
        243.42166137695312,
        656.3660888671875
      ],
      "text": "• Given matrix A ∈Rn×d."
    },
    {
      "page_no": 46,
      "bbox": [
        135.3970184326172,
        657.3690185546875,
        303.9923095703125,
        668.3394775390625
      ],
      "text": "• We define Lsparse(x) as Definition D.41."
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        675.6690063476562,
        292.64495849609375,
        686.6384887695312
      ],
      "text": "• We define Lsparse(x) Definition D.40."
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        693.968017578125,
        292.64495849609375,
        704.9384765625
      ],
      "text": "• We define Lsparse(x) Definition D.39."
    },
    {
      "page_no": 46,
      "bbox": [
        135.39700317382812,
        712.2680053710938,
        325.52362060546875,
        723.2374877929688
      ],
      "text": "• Let L(x) = Lexp(x) + Lsparse(x) + Lreg(x)."
    },
    {
      "page_no": 46,
      "bbox": [
        301.0179443359375,
        742.3324584960938,
        310.98052978515625,
        752.2950439453125
      ],
      "text": "46"
    },
    {
      "page_no": 47,
      "bbox": [
        135.39700317382812,
        72.89347076416016,
        331.27044677734375,
        84.97906494140625
      ],
      "text": "• Let A⊤(B(x) + W 2)A be the Hessian of L(x)"
    },
    {
      "page_no": 47,
      "bbox": [
        135.39698791503906,
        88.69709777832031,
        503.9994812011719,
        115.3434829711914
      ],
      "text": "• Let W = diag(w) ∈Rn×n. Let W 2 ∈Rn×n denote the matrix that i-th diagonal entry is\nw2\ni,i."
    },
    {
      "page_no": 47,
      "bbox": [
        135.39698791503906,
        122.91900634765625,
        359.17864990234375,
        133.8884735107422
      ],
      "text": "• Let σmin(A) denote the minimum singular value of A."
    },
    {
      "page_no": 47,
      "bbox": [
        135.39698791503906,
        141.05401611328125,
        245.5471649169922,
        151.2766571044922
      ],
      "text": "• Let l > 0 denote a scalar."
    },
    {
      "page_no": 47,
      "bbox": [
        107.44198608398438,
        160.44500732421875,
        164.7169647216797,
        170.40760803222656
      ],
      "text": "Then, we have"
    },
    {
      "page_no": 47,
      "bbox": [
        135.39698791503906,
        178.72265625,
        343.87884521484375,
        199.72608947753906
      ],
      "text": "• Part 1. If all i ∈[n], w2\ni ≥20 + l/σmin(A)2, then"
    },
    {
      "page_no": 47,
      "bbox": [
        299.2049560546875,
        194.93963623046875,
        315.9934997558594,
        206.27561950683594
      ],
      "text": "d2L"
    },
    {
      "page_no": 47,
      "bbox": [
        299.74798583984375,
        202.80401611328125,
        349.3603820800781,
        224.0451202392578
      ],
      "text": "dx2 ⪰l · Id"
    },
    {
      "page_no": 47,
      "bbox": [
        135.39697265625,
        227.61572265625,
        388.04681396484375,
        248.61915588378906
      ],
      "text": "• Part 2 If all i ∈[n], w2\ni ≥200 · exp(R2) + l/σmin(A)2, then"
    },
    {
      "page_no": 47,
      "bbox": [
        194.3039093017578,
        241.34210205078125,
        453.5622863769531,
        262.74017333984375
      ],
      "text": "(1 −1/10) · (B(x) + W 2) ⪯W 2 ⪯(1 + 1/10) · (B(x) + W 2)"
    },
    {
      "page_no": 47,
      "bbox": [
        107.99981689453125,
        268.56109619140625,
        503.997802734375,
        289.60906982421875
      ],
      "text": "Proof. The entire proof framework follows from [104, 105, 107], in the next few paragraphs, we\nmainly explain the difference."
    },
    {
      "page_no": 47,
      "bbox": [
        107.69100189208984,
        296.03546142578125,
        383.7077331542969,
        307.4920654296875
      ],
      "text": "The B(x) based on Lsparse is diag(u(x)). Note that it is obvious that"
    },
    {
      "page_no": 47,
      "bbox": [
        272.83502197265625,
        311.2959899902344,
        339.1676025390625,
        328.5810852050781
      ],
      "text": "diag(u(x)) ⪰0."
    },
    {
      "page_no": 47,
      "bbox": [
        108.00001525878906,
        326.8884582519531,
        273.2196960449219,
        336.85107421875
      ],
      "text": "From the upper bound size, we know that"
    },
    {
      "page_no": 47,
      "bbox": [
        249.5740203857422,
        342.14898681640625,
        361.9264831542969,
        375.3000793457031
      ],
      "text": "diag(u(x)) ⪯∥u(x)∥∞· In\n⪯exp(R2)"
    },
    {
      "page_no": 47,
      "bbox": [
        107.64096069335938,
        373.6064453125,
        391.69464111328125,
        383.5690612792969
      ],
      "text": "where the last step follows from Proof of Part 0 in Lemma 7.2 in [105]."
    },
    {
      "page_no": 47,
      "bbox": [
        107.69096374511719,
        389.9954528808594,
        503.9989929199219,
        410.8670654296875
      ],
      "text": "To prove Part 1, following from [104, 105], we only use the lower bound of diag(u(x)). By putting\nthings together, we get our results."
    },
    {
      "page_no": 47,
      "bbox": [
        107.69100189208984,
        417.2924499511719,
        505.6557922363281,
        427.3386535644531
      ],
      "text": "To prove Part 2, we follow from [107] and use both the upper bound and lower bound of diag(u(x))."
    },
    {
      "page_no": 47,
      "bbox": [
        108.0,
        457.60650634765625,
        221.55374145507812,
        467.569091796875
      ],
      "text": "D.21\nHessian is Lipschitz"
    },
    {
      "page_no": 47,
      "bbox": [
        108.0,
        477.6724548339844,
        295.1573181152344,
        501.4560852050781
      ],
      "text": "In this section, we show Hessian is Lipschitz.\nLemma D.48. If the following conditions hold"
    },
    {
      "page_no": 47,
      "bbox": [
        135.39700317382812,
        509.4614562988281,
        373.2896728515625,
        521.5470581054688
      ],
      "text": "• Let H(x) = A⊤(B(x) + W 2)A denote the Hessian of L."
    },
    {
      "page_no": 47,
      "bbox": [
        107.4420166015625,
        530.1900024414062,
        164.7169952392578,
        540.152587890625
      ],
      "text": "Then, we have"
    },
    {
      "page_no": 47,
      "bbox": [
        135.3970184326172,
        546.22509765625,
        318.1060485839844,
        566.8770751953125
      ],
      "text": "• ∥H(x) −H(y)∥≤n2 exp(40R2)∥x −y∥2"
    },
    {
      "page_no": 47,
      "bbox": [
        107.99998474121094,
        572.697998046875,
        504.0010986328125,
        593.7460327148438
      ],
      "text": "Proof. The entire proof framework follows from [104, 105, 107], in the next few paragraphs, we\nmainly explain the difference."
    },
    {
      "page_no": 47,
      "bbox": [
        108.0,
        600.1714477539062,
        381.803466796875,
        611.712646484375
      ],
      "text": "Note that the B(x) based on Lexp + Lreg have been proved by [105]"
    },
    {
      "page_no": 47,
      "bbox": [
        107.531982421875,
        616.5604248046875,
        383.1622619628906,
        628.0170288085938
      ],
      "text": "We only need to prove B(x) based on Lsparse and add them together."
    },
    {
      "page_no": 47,
      "bbox": [
        107.99998474121094,
        632.948486328125,
        324.5096740722656,
        644.4060668945312
      ],
      "text": "Note that B(x) based on Lsparse is in fact diag(u(x))."
    },
    {
      "page_no": 47,
      "bbox": [
        108.00001525878906,
        649.3374633789062,
        272.39276123046875,
        659.300048828125
      ],
      "text": "Using Lemma 7.2 in [105], we know that"
    },
    {
      "page_no": 47,
      "bbox": [
        107.6409912109375,
        664.5980224609375,
        419.5760498046875,
        706.01806640625
      ],
      "text": "∥diag(u(x)) −diag(u(y))∥≤∥u(x) −u(y)∥2\n≤2√nR exp(R2)∥x −y∥2\nwhere the last step follows from Part 1 in Lemma 7.2 in [105]."
    },
    {
      "page_no": 47,
      "bbox": [
        107.69099426269531,
        712.4444580078125,
        319.2567138671875,
        722.4070434570312
      ],
      "text": "Thus, putting things together, we complete the proof."
    },
    {
      "page_no": 47,
      "bbox": [
        301.0190124511719,
        742.3324584960938,
        310.9815979003906,
        752.2950439453125
      ],
      "text": "47"
    },
    {
      "page_no": 48,
      "bbox": [
        108.0,
        74.31652069091797,
        238.878662109375,
        84.27912139892578
      ],
      "text": "D.22\nGreedy Type Algorithm"
    },
    {
      "page_no": 48,
      "bbox": [
        108.0,
        95.04478454589844,
        503.9989013671875,
        115.95407104492188
      ],
      "text": "In this section, we propose a greedy-type algorithm (based on the approximate Newton method) to\nsolve the optimization problem."
    },
    {
      "page_no": 48,
      "bbox": [
        107.64099884033203,
        131.2235107421875,
        262.0409851074219,
        141.27706909179688
      ],
      "text": "Algorithm 4 A greedy type algorithm."
    },
    {
      "page_no": 48,
      "bbox": [
        112.97894287109375,
        145.89939880371094,
        505.7441101074219,
        209.0620880126953
      ],
      "text": "1: procedure OURITERATIVEMETHOD(A ∈Rn×d, b ∈Rn, w ∈Rn, ϵ, δ)\n2:\nInitialize x0\n3:\nT ←log(∥x0 −x∗∥2/ϵ)\n▷Let T denote the number of iterations.\n4:\nfor t = 0 →T do\n5:\nD ←Bdiag(xt) + diag(w ◦w)"
    },
    {
      "page_no": 48,
      "bbox": [
        108.49591064453125,
        205.33697509765625,
        372.0328674316406,
        295.6531066894531
      ],
      "text": "6:\neD ←SUBSAMPLE(D, A, ϵ1 = Θ(1), δ1 = δ/T)\n7:\nCompute gradient g exactly\n8:\nGet the approximate Hessian eH by computing A⊤eDA\n9:\nUpdate xt+1 by using the Newton step xt + eH−1g\n10:\nend for\n11:\nex ←xT +1\n12:\nreturn ex\n13: end procedure"
    },
    {
      "page_no": 48,
      "bbox": [
        107.6709976196289,
        316.80841064453125,
        364.213623046875,
        336.3350830078125
      ],
      "text": "Theorem D.49. Given matrix A ∈Rn×d, b ∈Rn, and w ∈Rn."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39695739746094,
        338.8544921875,
        315.11468505859375,
        350.9400939941406
      ],
      "text": "• We use x∗to denote the optimal solution of"
    },
    {
      "page_no": 48,
      "bbox": [
        268.35498046875,
        359.89105224609375,
        378.9060363769531,
        381.1070861816406
      ],
      "text": "min\nx∈Rd Lexp + Lsparse + Lreg"
    },
    {
      "page_no": 48,
      "bbox": [
        143.86495971679688,
        384.54901123046875,
        159.36676025390625,
        394.5115966796875
      ],
      "text": "that"
    },
    {
      "page_no": 48,
      "bbox": [
        153.8279571533203,
        402.3714904785156,
        368.4757385253906,
        436.1790771484375
      ],
      "text": "– g(x∗) = 0d, where g denotes the gradient function.\n– ∥x∗∥2 ≤R."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39697265625,
        433.19305419921875,
        386.8116149902344,
        453.8450927734375
      ],
      "text": "• Suppose that R ≥10, M = exp(Θ(R2 + log n)), and l > 0."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39695739746094,
        455.7080078125,
        434.3365478515625,
        473.00408935546875
      ],
      "text": "• Assume that ∥A∥≤R. Here ∥A∥denotes the spectral norm of matrix A."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39688110351562,
        474.8139343261719,
        504.0032958984375,
        504.56707763671875
      ],
      "text": "• Suppose that b ≥0n and ∥b∥1 ≤1. Here 0n denotes a length-n vector where all the entries\nare zeros. (Here b ≥0n denotes bi ≥0 for all i ∈[n])"
    },
    {
      "page_no": 48,
      "bbox": [
        135.39703369140625,
        501.27862548828125,
        503.9966125488281,
        526.0686645507812
      ],
      "text": "• Assume that w2\ni ≥200 · exp(R2) + l/σmin(A)2 for all i ∈[n]. Here σmin(A) denotes the\nsmallest singular value of matrix A."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39697265625,
        533.6685180664062,
        424.0796203613281,
        553.7960815429688
      ],
      "text": "• Let x0 denote an starting/initial point such that M∥x0 −x∗∥2 ≤0.1l."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39697265625,
        554.1649780273438,
        369.25054931640625,
        571.4610595703125
      ],
      "text": "• We use to ϵ ∈(0, 0.1) represent our accuracy parameter."
    },
    {
      "page_no": 48,
      "bbox": [
        135.39697265625,
        573.3240356445312,
        346.56005859375,
        590.6201171875
      ],
      "text": "• We use δ ∈(0, 0.1) to represent failure probability."
    },
    {
      "page_no": 48,
      "bbox": [
        107.44197082519531,
        594.4769897460938,
        256.0938720703125,
        604.4395751953125
      ],
      "text": "There is a randomized algorithm that"
    },
    {
      "page_no": 48,
      "bbox": [
        135.39697265625,
        614.29150390625,
        281.29510498046875,
        634.4190673828125
      ],
      "text": "• runs log(∥x0 −x∗∥2/ϵ) iterations"
    },
    {
      "page_no": 48,
      "bbox": [
        135.39695739746094,
        634.7880249023438,
        167.1086883544922,
        644.9270629882812
      ],
      "text": "• spend"
    },
    {
      "page_no": 48,
      "bbox": [
        252.11795043945312,
        652.6166381835938,
        395.7498474121094,
        671.5240478515625
      ],
      "text": "O((nnz(A) + dω) · poly(log(n/δ))"
    },
    {
      "page_no": 48,
      "bbox": [
        143.86492919921875,
        673.6680297851562,
        215.94435119628906,
        683.630615234375
      ],
      "text": "time per iteration,"
    },
    {
      "page_no": 48,
      "bbox": [
        135.3969268798828,
        689.2130126953125,
        323.6923522949219,
        710.1240844726562
      ],
      "text": "• and finally outputs a vector ex ∈Rd such that"
    },
    {
      "page_no": 48,
      "bbox": [
        267.14190673828125,
        710.4335327148438,
        380.71759033203125,
        731.05810546875
      ],
      "text": "Pr[∥ex −x∗∥2 ≤ϵ] ≥1 −δ."
    },
    {
      "page_no": 48,
      "bbox": [
        301.0189208984375,
        742.3324584960938,
        310.98150634765625,
        752.2950439453125
      ],
      "text": "48"
    },
    {
      "page_no": 49,
      "bbox": [
        108.0,
        74.23101806640625,
        503.99896240234375,
        95.27908325195312
      ],
      "text": "Proof. The proof framework follows from approximate Newton (second order method) literature\n[134, 135, 136, 137, 138, 139, 140, 141, 104, 105, 107, 110, 111, 142, 143, 144, 145]."
    },
    {
      "page_no": 49,
      "bbox": [
        108.0,
        101.70448303222656,
        471.95361328125,
        111.66708374023438
      ],
      "text": "Following from Lemma D.47, we know the Hessian of the loss function is positive definite."
    },
    {
      "page_no": 49,
      "bbox": [
        108.0,
        118.09346008300781,
        386.4191589355469,
        128.13963317871094
      ],
      "text": "Following from Lemma D.48, we know the Hessian of L is Lipschitz."
    },
    {
      "page_no": 49,
      "bbox": [
        108.0,
        134.48146057128906,
        419.4705810546875,
        144.44406127929688
      ],
      "text": "Following Section 9 in [105], by running Algorithm 4, we complete the proof."
    },
    {
      "page_no": 49,
      "bbox": [
        107.53199768066406,
        170.57606506347656,
        504.003173828125,
        206.09666442871094
      ],
      "text": "We remark that ω denotes the exponent of matrix multiplication (i.e., nω is the time of multiplying\nan n × n matrix with another n × n matrix). The most naive algorithm gives ω = 3. Currently, the\nstate-of-the-art algorithm gives ω = 2.373."
    },
    {
      "page_no": 49,
      "bbox": [
        301.01898193359375,
        742.3324584960938,
        310.9815673828125,
        752.2950439453125
      ],
      "text": "49"
    }
  ],
  "pictures": [
    {
      "page_no": 2,
      "bbox": [
        150.0,
        182.0,
        161.0,
        204.0
      ],
      "xref": 0,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk1_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        152.0,
        184.0,
        164.0,
        206.0
      ],
      "xref": 4,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk2_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        173.0,
        182.0,
        185.0,
        204.0
      ],
      "xref": 5,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk3_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        176.0,
        184.0,
        187.0,
        206.0
      ],
      "xref": 6,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk4_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        196.0,
        182.0,
        207.0,
        204.0
      ],
      "xref": 7,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk5_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        198.0,
        184.0,
        209.0,
        206.0
      ],
      "xref": 8,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk6_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        219.0,
        221.0,
        230.0,
        243.0
      ],
      "xref": 9,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk7_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        216.0,
        182.0,
        228.0,
        204.0
      ],
      "xref": 10,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk8_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        219.0,
        184.0,
        230.0,
        206.0
      ],
      "xref": 11,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p2_blk9_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        206.19300842285156,
        68.45915222167969,
        312.99542236328125,
        148.3553466796875
      ],
      "xref": 0,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk1_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        313.5828857421875,
        71.27903747558594,
        407.8133850097656,
        144.24305725097656
      ],
      "xref": 1,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk2_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        424.1451110839844,
        68.45917510986328,
        489.70697021484375,
        134.1385498046875
      ],
      "xref": 2,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk3_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        225.93206787109375,
        76.44878387451172,
        280.80194091796875,
        120.39169311523438
      ],
      "xref": 3,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk4_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        279.0,
        86.0,
        291.0,
        121.0
      ],
      "xref": 4,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk5_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        240.0,
        118.0,
        278.0,
        131.0
      ],
      "xref": 5,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk6_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        418.3879089355469,
        134.60850524902344,
        496.7566833496094,
        146.00546264648438
      ],
      "xref": 11,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p4_blk7_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        396.0,
        413.0,
        403.0,
        427.0
      ],
      "xref": 12,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk1_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        360.0,
        355.0,
        368.0,
        369.0
      ],
      "xref": 13,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk2_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        362.0,
        356.0,
        369.0,
        370.0
      ],
      "xref": 17,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk3_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        375.0,
        355.0,
        383.0,
        369.0
      ],
      "xref": 18,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk4_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        377.0,
        356.0,
        384.0,
        370.0
      ],
      "xref": 19,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk5_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        418.0,
        383.0,
        426.0,
        397.0
      ],
      "xref": 20,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk6_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        403.0,
        355.0,
        410.0,
        369.0
      ],
      "xref": 21,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk7_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        404.0,
        356.0,
        412.0,
        370.0
      ],
      "xref": 22,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk8_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        416.0,
        355.0,
        424.0,
        369.0
      ],
      "xref": 27,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk9_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        418.0,
        356.0,
        425.0,
        370.0
      ],
      "xref": 28,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk10_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        366.0,
        413.0,
        374.0,
        427.0
      ],
      "xref": 35,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk11_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        368.0,
        415.0,
        376.0,
        429.0
      ],
      "xref": 38,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk12_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        381.0,
        413.0,
        389.0,
        427.0
      ],
      "xref": 39,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk13_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        383.0,
        415.0,
        390.0,
        429.0
      ],
      "xref": 40,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk14_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        424.0,
        440.0,
        431.0,
        454.0
      ],
      "xref": 41,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk15_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        409.0,
        413.0,
        416.0,
        427.0
      ],
      "xref": 42,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk16_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        410.0,
        415.0,
        418.0,
        429.0
      ],
      "xref": 43,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk17_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        422.0,
        413.0,
        430.0,
        427.0
      ],
      "xref": 48,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk18_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        424.0,
        415.0,
        431.0,
        429.0
      ],
      "xref": 49,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk19_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        397.0,
        415.0,
        405.0,
        429.0
      ],
      "xref": 55,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk20_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        362.0,
        300.0,
        369.0,
        314.0
      ],
      "xref": 60,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk21_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        363.0,
        301.0,
        371.0,
        315.0
      ],
      "xref": 64,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk22_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        377.0,
        300.0,
        384.0,
        314.0
      ],
      "xref": 65,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk23_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        378.0,
        301.0,
        386.0,
        315.0
      ],
      "xref": 66,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk24_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        391.0,
        300.0,
        399.0,
        314.0
      ],
      "xref": 67,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk25_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        392.0,
        301.0,
        400.0,
        315.0
      ],
      "xref": 68,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk26_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        406.0,
        324.0,
        413.0,
        339.0
      ],
      "xref": 69,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk27_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        404.0,
        300.0,
        412.0,
        314.0
      ],
      "xref": 70,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk28_crop.png"
    },
    {
      "page_no": 6,
      "bbox": [
        406.0,
        301.0,
        413.0,
        315.0
      ],
      "xref": 71,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p6_blk29_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        241.46145629882812,
        302.35687255859375,
        410.81805419921875
      ],
      "xref": 3,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk1_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        108.0,
        241.46145629882812,
        302.35687255859375,
        410.81805419921875
      ],
      "xref": 4,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk2_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 5,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk3_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 6,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk4_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 7,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk5_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 8,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk6_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 9,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk7_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        135.41964721679688,
        255.17129516601562,
        297.51812744140625,
        385.0113525390625
      ],
      "xref": 10,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk8_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        302.35687255859375,
        241.46148681640625,
        503.1654052734375,
        400.3341064453125
      ],
      "xref": 11,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk9_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        336.2281799316406,
        255.17129516601562,
        499.13311767578125,
        384.20489501953125
      ],
      "xref": 12,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk10_crop.png"
    },
    {
      "page_no": 26,
      "bbox": [
        336.2281799316406,
        255.17129516601562,
        499.13311767578125,
        384.20489501953125
      ],
      "xref": 13,
      "image_path": "../data/parsed_documents/2306.14048/images/2306.14048_p26_blk11_crop.png"
    }
  ],
  "tables": [
    {
      "page_no": 1,
      "index": 1,
      "flavor": "stream",
      "nrows": 26,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p1_table1_stream.csv"
    },
    {
      "page_no": 1,
      "index": 2,
      "flavor": "stream",
      "nrows": 11,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p1_table2_stream.csv"
    },
    {
      "page_no": 2,
      "index": 1,
      "flavor": "lattice",
      "nrows": 11,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p2_table1_lattice.csv"
    },
    {
      "page_no": 2,
      "index": 2,
      "flavor": "lattice",
      "nrows": 11,
      "ncols": 10,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p2_table2_lattice.csv"
    },
    {
      "page_no": 2,
      "index": 3,
      "flavor": "lattice",
      "nrows": 11,
      "ncols": 10,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p2_table3_lattice.csv"
    },
    {
      "page_no": 2,
      "index": 4,
      "flavor": "lattice",
      "nrows": 11,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p2_table4_lattice.csv"
    },
    {
      "page_no": 3,
      "index": 1,
      "flavor": "stream",
      "nrows": 19,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p3_table1_stream.csv"
    },
    {
      "page_no": 4,
      "index": 1,
      "flavor": "stream",
      "nrows": 24,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p4_table1_stream.csv"
    },
    {
      "page_no": 5,
      "index": 1,
      "flavor": "stream",
      "nrows": 55,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p5_table1_stream.csv"
    },
    {
      "page_no": 6,
      "index": 1,
      "flavor": "stream",
      "nrows": 13,
      "ncols": 12,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p6_table1_stream.csv"
    },
    {
      "page_no": 6,
      "index": 2,
      "flavor": "stream",
      "nrows": 42,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p6_table2_stream.csv"
    },
    {
      "page_no": 7,
      "index": 1,
      "flavor": "stream",
      "nrows": 7,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p7_table1_stream.csv"
    },
    {
      "page_no": 7,
      "index": 2,
      "flavor": "stream",
      "nrows": 17,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p7_table2_stream.csv"
    },
    {
      "page_no": 8,
      "index": 1,
      "flavor": "stream",
      "nrows": 20,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p8_table1_stream.csv"
    },
    {
      "page_no": 8,
      "index": 2,
      "flavor": "stream",
      "nrows": 61,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p8_table2_stream.csv"
    },
    {
      "page_no": 8,
      "index": 3,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p8_table3_stream.csv"
    },
    {
      "page_no": 9,
      "index": 1,
      "flavor": "stream",
      "nrows": 8,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p9_table1_stream.csv"
    },
    {
      "page_no": 9,
      "index": 2,
      "flavor": "stream",
      "nrows": 17,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p9_table2_stream.csv"
    },
    {
      "page_no": 10,
      "index": 1,
      "flavor": "stream",
      "nrows": 57,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p10_table1_stream.csv"
    },
    {
      "page_no": 11,
      "index": 1,
      "flavor": "stream",
      "nrows": 19,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p11_table1_stream.csv"
    },
    {
      "page_no": 11,
      "index": 2,
      "flavor": "stream",
      "nrows": 51,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p11_table2_stream.csv"
    },
    {
      "page_no": 12,
      "index": 1,
      "flavor": "stream",
      "nrows": 53,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p12_table1_stream.csv"
    },
    {
      "page_no": 13,
      "index": 1,
      "flavor": "stream",
      "nrows": 53,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p13_table1_stream.csv"
    },
    {
      "page_no": 14,
      "index": 1,
      "flavor": "stream",
      "nrows": 52,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p14_table1_stream.csv"
    },
    {
      "page_no": 15,
      "index": 1,
      "flavor": "stream",
      "nrows": 52,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p15_table1_stream.csv"
    },
    {
      "page_no": 16,
      "index": 1,
      "flavor": "stream",
      "nrows": 38,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p16_table1_stream.csv"
    },
    {
      "page_no": 16,
      "index": 2,
      "flavor": "stream",
      "nrows": 52,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p16_table2_stream.csv"
    },
    {
      "page_no": 17,
      "index": 1,
      "flavor": "stream",
      "nrows": 52,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p17_table1_stream.csv"
    },
    {
      "page_no": 18,
      "index": 1,
      "flavor": "stream",
      "nrows": 23,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p18_table1_stream.csv"
    },
    {
      "page_no": 19,
      "index": 1,
      "flavor": "stream",
      "nrows": 39,
      "ncols": 18,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p19_table1_stream.csv"
    },
    {
      "page_no": 20,
      "index": 1,
      "flavor": "stream",
      "nrows": 28,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p20_table1_stream.csv"
    },
    {
      "page_no": 20,
      "index": 2,
      "flavor": "stream",
      "nrows": 15,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p20_table2_stream.csv"
    },
    {
      "page_no": 21,
      "index": 1,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p21_table1_stream.csv"
    },
    {
      "page_no": 21,
      "index": 2,
      "flavor": "stream",
      "nrows": 36,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p21_table2_stream.csv"
    },
    {
      "page_no": 22,
      "index": 1,
      "flavor": "stream",
      "nrows": 23,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p22_table1_stream.csv"
    },
    {
      "page_no": 23,
      "index": 1,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p23_table1_stream.csv"
    },
    {
      "page_no": 23,
      "index": 2,
      "flavor": "stream",
      "nrows": 13,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p23_table2_stream.csv"
    },
    {
      "page_no": 24,
      "index": 1,
      "flavor": "stream",
      "nrows": 8,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p24_table1_stream.csv"
    },
    {
      "page_no": 24,
      "index": 2,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p24_table2_stream.csv"
    },
    {
      "page_no": 25,
      "index": 1,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p25_table1_stream.csv"
    },
    {
      "page_no": 26,
      "index": 1,
      "flavor": "lattice",
      "nrows": 1,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p26_table1_lattice.csv"
    },
    {
      "page_no": 26,
      "index": 2,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p26_table2_lattice.csv"
    },
    {
      "page_no": 27,
      "index": 1,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p27_table1_stream.csv"
    },
    {
      "page_no": 27,
      "index": 2,
      "flavor": "stream",
      "nrows": 16,
      "ncols": 6,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p27_table2_stream.csv"
    },
    {
      "page_no": 28,
      "index": 1,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table1_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 2,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table2_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 3,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table3_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 4,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table4_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 5,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table5_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 6,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table6_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 7,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table7_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 8,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table8_lattice.csv"
    },
    {
      "page_no": 28,
      "index": 9,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p28_table9_lattice.csv"
    },
    {
      "page_no": 29,
      "index": 1,
      "flavor": "stream",
      "nrows": 69,
      "ncols": 12,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p29_table1_stream.csv"
    },
    {
      "page_no": 30,
      "index": 1,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p30_table1_stream.csv"
    },
    {
      "page_no": 31,
      "index": 1,
      "flavor": "stream",
      "nrows": 62,
      "ncols": 20,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p31_table1_stream.csv"
    },
    {
      "page_no": 32,
      "index": 1,
      "flavor": "stream",
      "nrows": 74,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p32_table1_stream.csv"
    },
    {
      "page_no": 33,
      "index": 1,
      "flavor": "stream",
      "nrows": 30,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p33_table1_stream.csv"
    },
    {
      "page_no": 34,
      "index": 1,
      "flavor": "stream",
      "nrows": 20,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p34_table1_stream.csv"
    },
    {
      "page_no": 35,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p35_table1_lattice.csv"
    },
    {
      "page_no": 35,
      "index": 2,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p35_table2_lattice.csv"
    },
    {
      "page_no": 36,
      "index": 1,
      "flavor": "stream",
      "nrows": 25,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p36_table1_stream.csv"
    },
    {
      "page_no": 37,
      "index": 1,
      "flavor": "stream",
      "nrows": 35,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p37_table1_stream.csv"
    },
    {
      "page_no": 38,
      "index": 1,
      "flavor": "stream",
      "nrows": 7,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p38_table1_stream.csv"
    },
    {
      "page_no": 39,
      "index": 1,
      "flavor": "stream",
      "nrows": 13,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p39_table1_stream.csv"
    },
    {
      "page_no": 39,
      "index": 2,
      "flavor": "stream",
      "nrows": 17,
      "ncols": 13,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p39_table2_stream.csv"
    },
    {
      "page_no": 40,
      "index": 1,
      "flavor": "stream",
      "nrows": 7,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p40_table1_stream.csv"
    },
    {
      "page_no": 41,
      "index": 1,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 10,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p41_table1_stream.csv"
    },
    {
      "page_no": 41,
      "index": 2,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p41_table2_stream.csv"
    },
    {
      "page_no": 42,
      "index": 1,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p42_table1_stream.csv"
    },
    {
      "page_no": 42,
      "index": 2,
      "flavor": "stream",
      "nrows": 15,
      "ncols": 12,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p42_table2_stream.csv"
    },
    {
      "page_no": 43,
      "index": 1,
      "flavor": "stream",
      "nrows": 31,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p43_table1_stream.csv"
    },
    {
      "page_no": 43,
      "index": 2,
      "flavor": "stream",
      "nrows": 26,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p43_table2_stream.csv"
    },
    {
      "page_no": 44,
      "index": 1,
      "flavor": "stream",
      "nrows": 17,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p44_table1_stream.csv"
    },
    {
      "page_no": 45,
      "index": 1,
      "flavor": "stream",
      "nrows": 6,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p45_table1_stream.csv"
    },
    {
      "page_no": 46,
      "index": 1,
      "flavor": "stream",
      "nrows": 57,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p46_table1_stream.csv"
    },
    {
      "page_no": 47,
      "index": 1,
      "flavor": "stream",
      "nrows": 16,
      "ncols": 11,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p47_table1_stream.csv"
    },
    {
      "page_no": 48,
      "index": 1,
      "flavor": "stream",
      "nrows": 24,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p48_table1_stream.csv"
    },
    {
      "page_no": 49,
      "index": 1,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2306.14048/2306.14048_p49_table1_stream.csv"
    }
  ]
}