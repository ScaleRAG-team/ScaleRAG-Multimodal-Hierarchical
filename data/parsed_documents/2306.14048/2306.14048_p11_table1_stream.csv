"","and memory-efficient exact attention with io-awareness. Advances in Neural Information"
"","Processing Systems, 35:16344–16359, 2022."
"","[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with"
"","sparse transformers. arXiv preprint arXiv:1904.10509, 2019."
"[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,",""
"","Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking"
"","attention with performers. arXiv preprint arXiv:2009.14794, 2020."
"[11] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers",""
"","are rnns: Fast autoregressive transformers with linear attention.
In International conference on"
"","machine learning, pages 5156–5165. PMLR, 2020."
"[12] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint",""
"","arXiv:1911.02150, 2019."
"[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam",""
"","Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:"
"","Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."
"[14]","Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens."
"","arXiv preprint arXiv:2304.08467, 2023."
"[15] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence",""
"","Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds,"
