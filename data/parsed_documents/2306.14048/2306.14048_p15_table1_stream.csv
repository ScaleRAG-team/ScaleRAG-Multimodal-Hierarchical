"[73] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for",""
"","multi-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019."
"[74]","Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny"
"","Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint"
"","arXiv:2201.11903, 2022."
"[75]","Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang,"
"","Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and"
"","beyond. arXiv preprint arXiv:2304.13712, 2023."
"[76]","Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of"
"","deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,"
"","2018."
"[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,",""
"","Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified"
"","text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020."
"","[78] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al."
"","Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
"[79] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-",""
"","wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language"
"","models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
"[80] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,",""
"","Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A"
"","176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,"
"","2022."
"[81]","Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi,"
"","Sanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020."
"[82] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,",""
"","and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint"
"","arXiv:1908.03265, 2019."
"[83] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the",""
"","difficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020."
"[84] Dušan Variš and Ondˇrej Bojar. Sequence length is a domain: Length-based overfitting in",""
"","transformer models. arXiv preprint arXiv:2109.07276, 2021."
"[85] Wancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved",""
"","calibration for the transformer architecture. arXiv preprint arXiv:2102.11402, 2021."
"[86] Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural",""
"","machine translation. arXiv preprint arXiv:2008.07772, 2020."
"[87] Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit",""
"","Cheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper
transformers on small"
"","datasets. arXiv preprint arXiv:2012.15355, 2020."
"[88] Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit:",""
"","Learning to initialize neural networks for stable and efficient training. Advances in Neural"
"","Information Processing Systems, 34:16410–16422, 2021."
"[89]","Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati,"
"","Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive"
"","gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022."
"","[90] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei."
"","Deepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022."
"[91] Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao,",""
"","and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. arXiv"
"","preprint arXiv:2208.01313, 2022."
"[92]","Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint"
"","arXiv:1711.05101, 2017."
