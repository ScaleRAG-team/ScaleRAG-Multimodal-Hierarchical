"References"
"[1] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-"
"Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for"
"dialog applications. arXiv preprint arXiv:2201.08239, 2022."
"[2] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with"
"large language models.
In 27th International Conference on Intelligent User Interfaces, pages"
"841–852, 2022."
"[3]
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani"
"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large"
"language models. arXiv preprint arXiv:2206.07682, 2022."
"[4] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B"
"Hashimoto. Benchmarking large language models for news summarization. arXiv preprint"
"arXiv:2301.13848, 2023."
"[5] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm"
"Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling"
"transformer inference. arXiv preprint arXiv:2211.05102, 2022."
"[6] Laszlo A Belady, Robert A Nelson, and Gerald S Shedler. An anomaly in space-time char-"
"acteristics of certain programs running in a paging machine. Communications of the ACM,"
"12(6):349–353, 1969."
"[7] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
transformer."
"arXiv preprint arXiv:2001.04451, 2020."
"[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
Flashattention: Fast"
"and memory-efficient exact attention with io-awareness. Advances in Neural Information"
"Processing Systems, 35:16344–16359, 2022."
"[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with"
"sparse transformers. arXiv preprint arXiv:1904.10509, 2019."
"[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,"
"Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking"
"attention with performers. arXiv preprint arXiv:2009.14794, 2020."
"[11] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers"
"are rnns: Fast autoregressive transformers with linear attention.
In International conference on"
"machine learning, pages 5156–5165. PMLR, 2020."
"[12] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint"
"arXiv:1911.02150, 2019."
"[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam"
"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:"
"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."
"[14]
Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens."
"arXiv preprint arXiv:2304.08467, 2023."
"[15] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence"
"Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds,"
"Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot"
"language model evaluation.
In Zenodo. https://doi.org/10.5281/zenodo.5371628, September"
"2021."
"[16] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,"
"Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of"
"language models. arXiv preprint arXiv:2211.09110, 2022."
"[17] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng"
"Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed"
"inference: Enabling efficient inference of transformer models at unprecedented scale. arXiv"
"preprint arXiv:2207.00032, 2022."
