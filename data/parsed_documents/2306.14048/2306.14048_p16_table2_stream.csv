"[93] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu."
"international
Texygen: A benchmarking platform for text generation models.
In The 41st"
"ACM SIGIR conference on research & development in information retrieval, pages 1097â€“1100,"
"2018."
"[94] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws."
"arXiv preprint arXiv:2212.09720, 2022."
"[95] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,"
"and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint"
"arXiv:2307.03172, 2023."
"[96] Pranjal Awasthi and Anupam Gupta.
Improving length-generalization in transformers via task"
"hinting. arXiv preprint arXiv:2310.00726, 2023."
"[97] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating trans-"
"formers via kernel density estimation. arXiv preprint arXiv:2302.02451, 2023."
"arXiv preprint
[98]
Josh Alman and Zhao Song.
Fast attention requires bounded entries."
"arXiv:2302.13214, 2023."
"[99] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the perfor-"
"mance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023."
"[100]
Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic"
"attention maintenance in large language models. arXiv preprint arXiv:2304.02207, 2023."
"arXiv
[101] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation."
"preprint arXiv:2305.04701, 2023."
"[102] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations"
"of transformers. arXiv preprint arXiv:2306.02896, 2023."
"[103] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention"
"arxiv preprint: arxiv
sparsification algorithms for over-parameterized feature dimension."
"2304.03426, 2023."
"[104] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression"
"problems. arXiv preprint, 2303.15725, 2023."
"[105] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression."
"arXiv preprint arXiv:2304.10411, 2023."
"[106] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers"
"via sketches for polynomial kernels. arXiv preprint arXiv:2310.01655, 2023."
"[107] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions"
"regression. arXiv preprint arXiv:2305.00660, 2023."
"[108]
Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir"
"arXiv preprint
Zandieh.
Hyperattention:
Long-context
attention in near-linear
time."
"arXiv:2310.05869, 2023."
"[109] Timothy Chu, Zhao Song, and Chiwun Yang. How to protect copyright data in optimization of"
"large language models? arXiv preprint arXiv:2308.12247, 2023."
"[110] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction for balancing the trade-"
"off between creativity and reality in large language models. arXiv preprint arXiv:2306.02295,"
"2023."
"[111] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformu-"
"lating single layer attention in llm based on tensor and svm trick, and solving it
in matrix"
"multiplication time. arXiv preprint arXiv:2309.07418, 2023."
"[112] Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. arXiv"
"preprint arXiv:2204.13807, 2022."
"[113] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention"
"computation. arXiv preprint arXiv:2307.08045, 2023."
"[114] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the"
"optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680,"
"2023."
