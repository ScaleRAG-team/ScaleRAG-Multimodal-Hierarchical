"","1
2"
"ABC, 2023, USA","Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan
, Lei Yang
, Yuxiong He"
"quantization accuracy. The fundamental challenge of quantization","(a) Baseline vs. Blocked Quantization
(b) Quantization Error"
"accuracy lies in the vast difference in number ranges and granu-","Baseline"
"","500"
"larity between high precision and low precision data (Eg. FP32/16",""
"vs. INT8). Some related work [41] propose to filter the outliers in",""
"","400"
"data to mitigate the gap in numerical ranges. Yet their accuracy",""
"hinges on the quality of outlier filtering and it brings extra filtering","Euclidean Distance"
"","300"
"overhead. Dettmers et al. [8] proposes to use block based quantiza-",""
"tion on optimizer states to improve the quantization accuracy yet",""
"","200"
"it requires changes to the model structure thus limits its usability.",""
"Starting from 1-bit SGD of error-",""
"Gradient Compression:","1
16
64
5122048"
"compensation compression [31], gradient compression has been","Nubmer of Blocks"
"pushed to an extreme direction of using just a single bit. To deal",""
"with non-linear gradient-based optimizers like Adam or Lamb, 1-bit",""
"","Figure 2: Illustration & example of block based quantization"
"quantization algorithms like 1-bit Adam [35] and 1-bit Lamb [18]",""
"","vs. baseline"
"are proposed, which achieve extreme efficient gradient communica-",""
"tion in distributed training. However, 1-bit Adam/LAMB cannot be",""
"directly applicable to ZeRO-3. The main reason is 1-bit Adam/Lamb",""
"assumes each GPU has the full view of optimizer states (OS) for",""
"the model, but ZeRO-3 splits it across all the GPUs in use. There-",""
"fore, it is infeasible to directly apply existing gradient compression",""
"techniques at ZeRO-3 and we need to design our own.",""
"ZeRO Communication Reduction: To reduce expensive cross-",""
"node communication, recent optimization on ZeRO-3, such as MiCS",""
"[40], trades on-device memory for communication. In MiCS, the",""
"GPU cluster is divided into sub-groups, and model states are par-",""
"titioned within a sub-group but replicated across sub-groups. By",""
"keeping the sub-group size small, MiCS can either leverage high",""
"bandwidth intra-node interconnect, or use hierarchical communi-",""
"cation to lower the communication volume. â„ğ‘ğ‘ in ZeRO++ adopts",""
"a similar approach of trading memory for less communication. The",""
"","Figure 3:
hpZ removes cross node traffic in backward all-"
"key difference is that â„ğ‘ğ‘ only do secondary partition on weights,",""
"","gather by holding secondary weight partitions in on-device"
"while keeping all other model states partitioned across all GPUs.",""
"","memory."
"This allows hpZ to achieve significant communication reduction",""
"without the massive memory overhead of MiCS.",""
"","While this reduces the communication volume of the all-gather"
"3
DESIGN",""
"","by 2x, doing so naively results in two major issues:
i) the lower-"
"In this section, we elaborate on the design of our three key op-",""
"","ing of precision results in significant accuracy degradation during"
"timizations in ZeRO++ introduced in Section 1 for reducing the",""
"","training as discussed in 2.3 , and ii) the quantization and dequanti-"
"communication overhead of ZeRO: i) Quantized Weight Commu-",""
"","zation overhead negates any throughput gain from communication"
"ii) Hierarchical Partitioning for ZeRO
nication for ZeRO (ğ‘ğ‘¤ğ‘ ),",""
"","volume reduction. We discuss the optimized implementation of"
"(â„ğ‘ğ‘ ), and iii) Quantized Gradient communication for ZeRO (ğ‘ğ‘”ğ‘ ).",""
"","ğ‘ğ‘¤ğ‘ to minimize the quantization and dequantization overhead in"
"After that, we discuss the end-to-end impact of these optimizations",""
"","Section 4. Here, we primarily focus on design choices to mitigate"
"to reduce to total communication volume of ZeRO.",""
"","accuracy degradation."
"","ğ‘ğ‘¤ğ‘ uses blocked based quantization to improve the quanti-"
"","zation accuracy. As illustrated in Figure 2, each weight tensor is"
"3.1
Quantized Weight Communication for",""
"","divided into smaller chunks, and converted into INT8 by symmetric"
"ZeRO (ğ‘ğ‘¤ğ‘ )","quantization, using an independent quantization scaling coefficient."
"As discussed in Section 2.2, ZeRO partitions the model weights","By keeping the quantization granularity small, we significantly"
"across all the ranks (i.e., GPUs) and fetches the FP16 weights layer-","mitigate the gap in number ranges and granularity."
"by-layer right before they are needed in computation via all-gather","We show an example of the quantization error of performing"
"for the forward and backward of each training iteration. To reduce","block based quantization vs. the non-blocked quantization baseline"
"the communication overhead of
forward all-gather on weights,","in Figure 2(a). Fig. 2(b) shows a case study of weights quantization"
"ğ‘ğ‘¤ğ‘ , quantizes FP16 weights to INT8 right during the all-gather,","on BERT model, where block based quantization reduces the quan-"
"and dequantizes them back to FP16 on the receiver side, and then","tization error by 3x. More in-depth convergence evaluations are"
"conducts layer computation.","shown in Sec. 5."
