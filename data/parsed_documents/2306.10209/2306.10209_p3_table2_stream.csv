"system throughput. Finally, we conduct convergence evaluation in-",""
"","Algorithm 1: ZeRO algorithm"
"dicating that ZeRO++ has negligible impact on model convergence",""
"","Input
:𝑚𝑜𝑑𝑒𝑙,𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒"
"and maintains similar model training accuracy as ZeRO baseline.",""
"","Output :𝑚𝑜𝑑𝑒𝑙"
"The main contributions of this paper are as follows:",""
"","1 while 𝑚𝑜𝑑𝑒𝑙 not converged do"
"","𝑎𝑙𝑙_𝑔𝑎𝑡ℎ𝑒𝑟 _𝑃𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 (𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒 );"
"• Blocked quantized weights (𝑞𝑤𝑍 ) reduces communication","2"
"volume of all-gather of weights by 50%.","𝑚𝑜𝑑𝑒𝑙 .𝑓 𝑜𝑟 𝑤𝑎𝑟𝑑 ();
3"
"","𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 (𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒 );"
"• Hierarchical partitioning of model weights (ℎ𝑝𝑍 ) completely","4"
"eliminates inter-node all-gather communication in backward","𝑎𝑙𝑙_𝑔𝑎𝑡ℎ𝑒𝑟 _𝑃𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 (𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒 );
5"
"propagation.","𝑚𝑜𝑑𝑒𝑙 .𝑏𝑎𝑐𝑘𝑤𝑎𝑟𝑑 ();
6"
"","𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 (𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒 );"
"• Novel, all-to-all quantized gradient reduction collective (𝑞𝑔𝑍 )","7"
"reduces gradient communication by 75% comparing with","𝑟𝑒𝑑𝑢𝑐𝑒_𝑠𝑐𝑎𝑡𝑡𝑒𝑟 _𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡𝑠 (𝑤𝑜𝑟𝑙𝑑𝑆𝑖𝑧𝑒 );
8"
"reduce-scatter.","𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟 .𝑠𝑡𝑒𝑝 ();
9"
"• Optimized Integration of each of the above techniques into","10
end while"
"existing ZeRO implementation, that enables communication","11 Return: 𝑚𝑜𝑑𝑒𝑙"
"and computation overlapping, and leverages custom high",""
"performance CUDA kernels for quantization, dequantiza-",""
"tion, as well as operator fusion (section 4). Our implementa-",""
"","2.2
ZeRO Optimizer"
"tion translates the 4x communication volume reduction of",""
"","ZeRO is a memory-optimized solution for data parallel training."
"ZeRO++ into real throughput improvement.",""
"","ZeRO partitions and distributes all model states (i.e., parameters,"
"• Extensive experiments shows that i) over 45% of sustained",""
"","gradients, optimizer states) among GPUs in use and recollects model"
"peak throughput even at small batch sizes, ii) up to 2.4x end-",""
"","states only when the layer needs to be computed. There are three"
"to-end system improvement over ZeRO, and iii) achieving",""
"","different stages for using ZeRO to optimize on-device memory"
"similar throughput in low-bandwidth cluster compared to",""
"","usage. In ZeRO stage 1 (ZeRO-1), only optimizer states are split and"
"baseline in high-bandwidth cluster. In addition, we present",""
"","spread across all GPUs in use. ZeRO stage 2 (ZeRO-2) partitions"
"performance breakdown and analysis of diffrent components",""
"","both optimizer states and gradients, where ZeRO stage 3 (ZeRO-3)"
"of ZeRO++.Our end-to-end training shows that ZeRO++ does",""
"","splits all three components of model states as parameters, gradients,"
"not affect model convergence.",""
"","and optimizer states."
"• ZeRO++ is open-sourced and released as part of https://",""
"","ZeRO-3 is the most memory efficient solution for model training"
"github.com/microsoft/DeepSpeed",""
"","at large scale, but at the cost of more collective communications. Al-"
"","gorithm 1 illustrates the high-level pseudocode for ZeRO-3. During"
"2
BACKGROUND AND RELATED WORK",""
"","model training, ZeRO-3 lazy-schedules the fetching of parameters"
"2.1
Data, Model and 3D parallelism","until the computation needs to happen on a particular layer. Before"
"Data parallelism (DP), pipeline parallelism (PP), and tensor paral-","forward propagation, ZeRO launches an all-gather to collect the"
