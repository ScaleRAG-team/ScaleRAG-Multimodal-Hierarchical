"ZeRO++: Extremely Efficient Collective Communication for Giant Model Training","ABC, 2023, USA"
"3.2
Hierarchical Partitioning for ZeRO (â„ğ‘ğ‘ )",""
"ZeRO-3 partitions all its model states across all its ranks, resulting",""
"in communication collectives that span all the GPUs. With â„ğ‘ğ‘ ,",""
"we notice that it is possible to have different partitioning for dif-",""
"ferent model states,
limiting the communication collectives to a",""
"subset of the GPUs. Given that on modern GPU clusters, intra-node",""
"communication bandwidth is significantly higher than inter-node",""
"communication bandwidth, this presents opportunities to reduce",""
"the inter-node communication.",""
"More specifically, in â„ğ‘ğ‘ , we eliminate the inter-node all-gather",""
"","Figure 4: Per-device memory consumption analysis of stan-"
"during the backward pass by holding secondary FP16 weights par-",""
"","dard data parallel (DP), ZeRO stage 3 (ZeRO-3) and proposed"
"tition within each node. We do this by creating a hierarchical parti-",""
"","hierarchical partitioning of ZeRO parameters (â„ğ‘ğ‘ ). ğ¾ de-"
"tioning strategy consisting of two partitions: first, all model states",""
"","notes the memory multiplier of optimizer states, ğ‘€ repre-"
"are partitioned globally across all devices as in ZeRO-3, which we",""
"","sents the number of trainable parameters, ğ‘ƒ is the data paral-"
"call primary partition. Second, a secondary copy of FP16 parame-",""
"","lel group size or world size, and ğ›¼ is the number of secondary"
"ters is partitioned at the sub-global level (e,.g., compute node, see",""
"","groups or ratio of world size to the number of ranks in the"
"figure 3), which we call secondary partition. This secondary copy of",""
"","secondary group. A typical real world scenario example is"
"FP16 parameters is replicated across multiple secondary partitions.",""
"","provided in the last column. We assume a model size of 100B"
"Consider a 64-node cluster, each node with 8 GPUs. Model",""
"","trained on 1024 V100 GPU DGX cluster (64 compute nodes,"
"weights are partitioned in two stages: i) across all 512 GPUs that we",""
"","16 GPUs per node)."
"call primary partition, and ii) the same weights are also partitioned",""
"within a compute node across 8 GPUs, that we call secondary par-",""
"tition. In this example, for the secondary partition, each compute",""
"","parameters partitioned across a subset of devices as long as model"
"node in the cluster holds a full replica of FP16 weights partitioned",""
"","parameters fit."
"among the 8 GPUs within the node, and there are 64 of such replicas",""
"","Figure 4 provides a concrete memory usage estimate of a typical"
"in total.",""
"","large language model of size of 100B parameters, with primary"
"","group size of 1024 GPUs and secondary group size of 16 GPUs"
"3.2.1
A training iteration with hpZ. During the forward pass of a","(e.g., DGX-2 V100 node). As shown in Figure 4, with our proposed"
"training iteration, we all-gather weights based on the primary parti-",""
"","method, â„ğ‘ğ‘ consumes 8.9ğ‘¥ more memory than ZeRO-3, our ap-"
"tion across all GPUs. However, once the weights are consumed dur-",""
"","proach is still 114ğ‘¥ less memory requirement than standard DP."
"ing the forward pass, they are partitioned based on the secondary",""
"","This marginal
increase in memory usage is compensated for by"
"partition. Given the temporal consistency of model parameters be-",""
"","efficient
intra-node communication schedule. By eliminating or"
"tween forward and backward passes, when the weights are needed",""
"","reducing inter-node communication for backward pass, â„ğ‘ğ‘ re-"
"again during the backward pass, we all-gather weights based on this",""
"","duces the end-to-end communication of ZeRO by 1.5ğ‘¥, while still"
"secondary group. Note that when the secondary partitioning is set",""
"","supporting model training with hundreds of billions of parameters."
"to be a compute node, this avoids any inter-node communication",""
"for this all-gather. Finally, at the end of the iteration, during the",""
"","3.3
Quantized Gradients Communication for"
"optimizer step, all the model states, as well as the primary copy of",""
"","ZeRO (ğ‘ğ‘”ğ‘ )"
"the fp16 parameter are updated based on the primary partition. hpZ",""
"","In this section, we propose a novel quantized reduce-scatter algo-"
"makes two changes to baseline ZeRO pseudocode in Algorithm 1:",""
"","rithm called qgZ based on all-to-all collectives that enables a 4x"
"i) in line 4, parameter partitioning is based on secondary group size,",""
"","communication volume reduction of gradient reduce-scatter by re-"
"ii) parameter all-gather preceding backward pass in line 5 is also",""
"","placing FP16 with INT4 quantized data, while overcoming precision"
"based on secondary group size.",""
"","loss challenges described in Section 1, as well as numerous system"
"Our design of â„ğ‘ğ‘ is flexible to support any secondary group",""
"","challenges that we will outline in this section."
"size. The group size controls how many ranks (i.e., GPUs) are in the",""
"","qgZ leverages all-to-all collectives to implement quantized reduce-"
"secondary partition. It is also a measure of memory-communication",""
"","scatter which includes three major components: 1) all-to-all-based"
"trade-off of â„ğ‘ğ‘ . Simply put, by default, â„ğ‘ğ‘ secondary partition is",""
"","implementation of quantized gradient reduce-scatter, 2) reducing"
"node-based (recall intra-node bandwidth is multiple factors of inter-",""
"","communication volume with hierarchical collectives, 3) tensor slice"
"node bandwidth for current and future hardware configurations)",""
"","reordering for correct gradient placement. We talk about each of"
"but can be extended to support multiple compute nodes as needed.",""
"","them step-by-step."
"3.2.2
Memory Usage Analysis. By design, â„ğ‘ğ‘ trades memory for","3.3.1
All-to-all based implementation. A naive approach towards"
"communication efficiency. It is important to analyze this tradeoff.","quantized reduce-scatter, while avoiding precision loss due to re-"
"Recall that standard data parallel DNN (DP) replicates model param-","duction is to apply quantization and dequantization to a ring-based"
"eters across data parallel ranks, ZeRO-3 on the other hand partitions","reduce-scatter directly as shown on the left of Figure 5. We can"
"parameter across data parallel ranks. A midway approach is model","inject quantization and dequantization on each GPU. Once a GPU"
