"",""
"Time","ğ‘â„ğ‘¢ğ‘›ğ‘˜ğµğ‘ğ‘ ğ‘’ â† (ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ¼ ğ· % ğ‘›ğ‘œğ‘‘ğ‘’ğ‘†ğ‘–ğ‘§ğ‘’) âˆ— ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ ;
6"
"","7 Return: ğ‘ğ‘–ğ‘ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’ğ‘‚ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ + ğ‘â„ğ‘¢ğ‘›ğ‘˜ğµğ‘ğ‘ ğ‘’ + ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘‚ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡;"
"Figure 10: Pipelining and overlapping intra-node communi-",""
"cation with inter-node communication in ğ‘ğ‘”ğ‘ .",""
"","4.2
CUDA Kernels"
"","As existing quantization implementations are unable to capture"
"","the combination of data mapping and high throughput necessary"
"","to minimize kernel overhead, we implement and optimize custom"
"quantization kernel is non-blocking and we further avoid operations","CUDA kernels to implement these primitives. In particular, these"
"that involve explicit/implicit CUDA synchronization (e.g. tensor","kernels aim to (1) saturate device memory bandwidth and (2) mini-"
"concatenation), making the quantization a non-blocking operation","mize the total traffic via fusion."
"that can be launched asynchronously.",""
"","Maximizing Bandwidth Utilization: A core quantization and"
"With this two features, as ZeRO fetch parameters for each layer,","dequantization library of composable operators was developed as"
"the communication of the current layer and the quantization of the","the foundation for ZeRO++. The core primitives leverage efficient"
"next layer can be launched at the same time on different CUDA","vectorized memory accesses at the maximum granularity a given"
"streams. When the quantized data are needed for the next layer,","GPU architecture supports.
In order to satisfy the alignment re-"
"ZeRO++ synchronizes the quantization stream to make sure the","quirements these instructions have, model state is partitioned such"
"quantized data are ready. This approach hides the quantization cost","that quantization granularities will be 16B aligned. Additionally,"
"of the next layer under the communication time span of the current","we leverage instruction level parallelism to overlap multiple mem-"
"layer which hides the quantization overhead.","ory transactions with each other. In practice, the combination of"
"","vectorized accesses and instruction level parallelism enables the"
"","quantization library to achieve full GPU memory bandwidth uti-"
"4.1.2
Hierarchical Collectives for Gradient Communication. As dis-",""
"","lization."
"cussed in Sec. 3.3.2, our all-to-all based gradient communication is",""
"broken into two stages: first intra-node communication followed","Minimizing Total Traffic: Multiple techniques are used to re-"
"","duce the total memory traffic for quantization kernels. First, the"
"by inter-node communication. The inter-node communication de-",""
"","size of each quantization block is tuned so as to express sufficient"
"pends on the results of the intra-node communication, therefore,",""
"","parallelism to schedule across a GPUâ€™s streaming multiprocessors"
"with a naive implementation, inter-nodes links are idle during intra-",""
"","and cache values not quantized yet in the register file while cal-"
"node communication and vice versa. To reduce latency by leverag-",""
"","culating the quantization scale and offset
for the block. Second,"
"ing both inter-node and intra-node links in parallel, we chunk our",""
"","we fuse tensor reshaping and quantization into the same kernel to"
"input gradient tensor and pipeline transfer between intra-node com-",""
"","avoid redundantly loading data from global memory. For example,"
"munication and inter-node communication. As shown in Figure 10,",""
"","the tensor slice reordering (i.e., orange arrow-lines in Figure 9) is"
"compared with ""no pipeline"" case on the top, simply adopting a ""2-",""
"","realized within a fused quantization and remapping kernel.This"
"stage pipeline"" transfer achieves the amount of end-to-end latency",""
"","fused kernel achieves the same level of performance as a single"
"reduction shown as the red arrow-line in Figure 10. By overlapping",""
"","quantization kernel working with contiguous data. Finally, we fuse"
"intra-node and inter-node communication, the end-to-end latency",""
"","sequential dequantization, reduction, and quantization operations"
"of gradient communication is significantly reduced.",""
"","into single kernel
implementation, which reduces total memory"
"Doing this pipeline correctly has implications on our tensor slice",""
"","traffic by 9x in ğ‘ğ‘”ğ‘ ."
"reordering process. The more pipeline stages we have, the more",""
"fine-grained tensor slices are needed for reordering. Therefore,",""
"we also propose a generalized tensor slices reordering scheme as","5
EVALUATION"
"algorithm 2, which covers both w/ and w/o pipelining data transfer","In this section, we perform three sets of evaluations for ZeRO++."
"cases. Here stages refer to the number of pipeline stages we have,","First, we perform end-to-end evaluations showing : i) scalability"
"nodeSize is the number of GPUs per node and nodes is the number","evaluation on up to 384 GPUs ,
ii) speedup over state-of-the-art"
"of nodes.","(SOTA) baseline across models ranging from 10-138B parameters,"
"Next, we discuss how we optimize our CUDA kernels to further","and iii)
throughput comparisons for cluster setting with varied"
"reduce all quantization related overhead.","cross-node bandwidth. Second, we perform throughput analysis"
