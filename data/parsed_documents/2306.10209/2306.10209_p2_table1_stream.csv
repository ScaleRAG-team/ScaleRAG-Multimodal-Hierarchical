"","1
2"
"ABC, 2023, USA","Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan
, Lei Yang
, Yuxiong He"
"On one hand, clusters with low-bandwidth is common in major-","block-based quantization. Thus, we implement highly optimized"
"ity of cloud computing environments. Although high performance","quantization CUDA kernels from scratch."
"",""
"","Hierarchical Weight Partition for ZeRO (hpZ) Second, to re-"
"NVLink [25] and NVSwitch [26] as intra-node interconnects, cross-","duce communication overhead of all-gather on weights during back-"
"node links are often less than 100Gbps ethernet which makes it","ward, we trade GPU memory for communication. More specifically,"
"the communication bottleneck. As shown in Figure 1(a), the per","instead of spreading whole model weights across all the machines,"
"GPU throughput on low bandwidth clusters is only half of that with","we maintain a full model copy within each machine. At the expense"
"high-bandwidth clusters.","of higher memory overhead, this allows us to replace the expensive"
"On the other hand, even on high-bandwidth clusters, when run-","cross-machine all-gather on weights with intra-machine all-gather,"
"ning on thousands of GPUs, the batch size per GPU is limited by","which is substantially faster due to much higher intra-machine"
"the maximum global batch size that can be used during the train-","communication bandwidth."
"",""
"","Quantized Gradient Communication for ZeRO (qgZ) Third,"
"words, as global batch size cannot be increased indefinitely without","reducing communication cost of gradients using reduce-scatter is"
"slowing down model convergence, training on thousands of GPUs","even more challenging. Directly applying quantization to reduce"
"forces the batch size per GPU to be very small, which reduces the","communication volume is infeasible. The main issue is, even by"
"compute-to-communication ratio and thus creates a communica-","incorporating block-based quantization to reduce-scatter opera-"
"tion bottleneck. As shown in Figure 1(b), the per GPU throughput","tion,
it will still significantly hurt model training accuracy. The"
"is heavily impacted by small batch size per GPU, which is a result","key reason behind is quantization will decrease value precision."
"of communication bottleneck.","And reduction on low-precision values will accumulate and amplify"
"However, rare efforts have been made to optimize end-to-end","the errors. Therefore, we propose a novel and much more efficient"
"communication efficiency for ZeRO. There are many previous work","gradient communication paradigm as a general replacement of"
"on reducing communication overhead in distributed model training,","reduce-scatter collective, where the gradients are compressed using"
"such as 1-bit LAMB [18], 1-bit Adam [35] and other error compen-","block-based INT4 quantization during the communication to re-"
"sation compression techniques for gradient averaging [1, 12, 31, 34].","duce the communication volume, but the full precision is recovered"
"However, none of them can work with ZeRO as they all assume","before the reduction operator to preserve training accuracy. We"
"model state replication, while model states are partitioned in ZeRO.","call this 𝑞𝑔𝑍 , and is designed to i) overcome significant accuracy"
"We start from scratch and provide an end-to-end system for reduc-","loss that would result
from low-precision reduction if we were"
"ing all communication overhead in ZeRO training.","to simply implement reduce-scatter in INT4/INT8, and ii) avoid"
"","accuracy degradation and significant latency overhead of a long"
"","sequence of quantization and dequantization steps needed by a"
"1.2
ZeRO++","ring [23] or tree [5, 37] based reduce-scatter (e.g., left of Figure 5),"
"In this paper, we present a novel system of communication opti-","even if we did the reductions in full-precision. Furthermore, 𝑞𝑔𝑍"
"mizations collectively called ZeRO++ that offers dramatic commu-","leverages the hierarchical nature of modern GPU clusters, where"
"nication volume reduction for ZeRO. Below we discuss the main","intra-node bandwidth is significantly higher than inter-node, to"
"communication overheads in ZeRO,
followed by three different","first reduce gradients within a node before doing cross-node re-"
"communication optimizations in ZeRO++ that address them.","duction to minimize inter-node communication volume, resulting"
"Assume the model size as 𝑀. During the forward pass, ZeRO","in 2/4x communication volume reduction (INT8/4) compared to"
"[29] conducts an all-gather operation to collect all the parameters","FP16 reduce-scatter. We further reduce end-to-end latency of 𝑞𝑔𝑍"
"layers.
In the backward pass,
(𝑀) needed to train for all model","by pipelining intra-node and inter-node communication and con-"
"ZeRO re-collects parameters (𝑀) with all-gather first, then each","ducting CUDA kernel fusion."
"",""
"","Communication Volume Reduction By incorporating all"
"scatter function to aggregate and redistribute gradients (𝑀) across","three components above, we reduce the cross-node communication"
"accelerators. In total, ZeRO has a total communication volume of","for forward
volume from 3𝑀 down to 0.75𝑀. More specifically,"
"3𝑀, spreads evenly across 2 all-gather and 1 reduce-scatter.","all-gather operation on model weights, by applying INT8 quanti-"
"To reduce these communication overheads, ZeRO++ has three","zation, we reduce the communication size from 𝑀 to 0.5𝑀. Dur-"
"sets of communication optimizations, targeting each of the above","ing backward all-gather on weights, with our secondary copy of"
"mentioned three communication collectives respectively:","model parameters, we reduce the communication size from 𝑀 to"
"",""
"Quantized Weight Communication for ZeRO (qwZ) First, in",""
"order to reduce parameter communication volume during forward","novel all-to-all based INT4 reduce-scatter, we reduce cross-node"
"all-gather, we adopt quantization on weights to shrink down each","communication from 𝑀 to 0.25𝑀. Thus,
in total, we reduce 3𝑀"
"model parameter from FP16 (2 bytes) to INT8 (1 byte) data type","communication to 0.75𝑀."
"",""
"","Evaluation We implemented ZeRO++ and performed extensive"
"by half. However, naively conducting quantization on weights may","evaluation demonstrating three key results: i) scalability of GPT-3"
"lose model training accuracy. In order to preserve decent model","like models on up to 384 GPUs achieving over 45% of sustained peak"
"training precision, we adopt block-based quantization [8], which","throughput,
ii) consistent speedup of up to 2.4x over ZeRO [29]"
"conducts independent quantization on each subset of model param-","baseline across models ranging from 10-138B parameters, and iii)"
"eters. There is no existing implementation for high performance","comparing with baseline in 4x higher bandwidth cluster, ZeRO++"
