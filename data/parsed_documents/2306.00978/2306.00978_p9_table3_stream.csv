"quantization). Our method significantly improves the captioning quality compared to the round-to-nearest (RTN) baseline. We color the","","","","","",""
"text to show the correct or wrong captions.","","","","","",""
"to provide accurate and efficient quantization. We perform","OPT (Wiki PPLâ†“)","1.3B","2.7B","6.7B","13B","30B"
"experiments with the OpenFlamingo-9B model (Awadalla","","","","","",""
"","FP16","14.62","12.47","10.86","10.13","9.56"
"et al., 2023) (an open-source reproduction of (Alayrac et al.,","","","","","",""
"","RTN","10476","193210","7622","17564","8170"
"2022)) on COCO captioning (Chen et al., 2015) dataset (Ta-","","","","","",""
"","GPTQ","46.67","28.15","16.65","16.74","11.75"
"ble 6). We measured the average performance of 5k samples","","","","","",""
"","AWQ +GPTQ","35.71","25.70","15.71","13.25","11.38"
"under different few-shot settings. We only quantize the lan-","","","","","",""
"guage part of the model since it dominates the model size.","","","","","",""
