"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration",""
"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,","Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,"
"Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,","A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,"
"et al. Training language models to follow instructions","Bhosale, S., et al. Llama 2: Open foundation and fine-"
"with human feedback. Advances in Neural Information","tuned chat models.
arXiv preprint arXiv:2307.09288,"
"Processing Systems, 35:27730–27744, 2022.","2023b."
"","Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,"
"Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,",""
"","L., Gomez, A. N., Kaiser, Ł., and Polosukhin,
I. At-"
"D. nuqmm: Quantized matmul for efficient inference of",""
"","tention is all you need. Advances in neural information"
"large-scale generative language models. arXiv preprint",""
"","processing systems, 30, 2017."
"arXiv:2206.09557, 2022.",""
"","Wang, H., Zhang, Z.,
and Han, S.
Spatten:
Efficient"
"Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap-",""
"","sparse
attention architecture with cascade
token and"
"pelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and",""
"","head pruning.
CoRR, abs/2012.09852, 2020.
URL"
"Launay, J. The refinedweb dataset for falcon llm: out-",""
"","https://arxiv.org/abs/2012.09852."
"performing curated corpora with web data, and web data",""
"only. arXiv preprint arXiv:2306.01116, 2023.","Wang, K., Liu, Z., Lin, Y., Lin,
J., and Han, S.
HAQ:"
"","Hardware-Aware Automated Quantization with Mixed"
"Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,",""
"","Precision.
In CVPR, 2019."
"Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,",""
"A., et al. Multitask prompted training enables zero-shot","Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,"
"task generalization.
arXiv preprint arXiv:2110.08207,","B., Du, N., Dai, A. M., and Le, Q. V.
Finetuned lan-"
"2021.","arXiv preprint
guage models are zero-shot
learners."
"","arXiv:2109.01652, 2021."
"Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow,",""
"","Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang,"
"D., Castagn´e, R., Luccioni, A. S., Yvon, F., Gall´e, M.,",""
"","Q., Yu, F., and Liu, X. Outlier suppression: Pushing"
"et al. Bloom: A 176b-parameter open-access multilingual",""
"","the limit of low-bit transformer language models, 2022a."
"language model. arXiv preprint arXiv:2211.05100, 2022.",""
"","URL https://arxiv.org/abs/2209.13325."
"Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,",""
"","Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang,"
"D. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E.,",""
"","Q., Yu, F., and Liu, X. Outlier suppression: Pushing"
"et al.
High-throughput generative inference of
large",""
"","the limit of low-bit transformer language models. arXiv"
"arXiv preprint
language models with a
single gpu.",""
"","preprint arXiv:2209.13325, 2022b."
"arXiv:2303.06865, 2023.",""
"","Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and"
"Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,","Liu, X. Outlier suppression+: Accurate quantization of"
"Batra, D., Parikh, D., and Rohrbach, M. Towards vqa","large language models by equivalent and optimal shifting"
"models that can read.
In Proceedings of the IEEE/CVF","and scaling. arXiv preprint arXiv:2304.09145, 2023."
"conference on computer vision and pattern recognition,",""
"","Xiao, G., Lin,
J., Seznec, M., Demouth,
J.,
and Han,"
"pp. 8317–8326, 2019.",""
"","S.
Smoothquant: Accurate and efficient post-training"
"Taori,
R., Gulrajani,
I.,
Zhang,
T., Dubois, Y.,
Li,","quantization for large language models. arXiv preprint"
"X., Guestrin, C.,
Liang,
P.,
and Hashimoto,
T. B.","arXiv:2211.10438, 2022."
"Stanford
alpaca:
An
instruction-following
llama",""
"","Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and"
"https://github.com/tatsu-lab/
model.",""
"","He, Y. Zeroquant: Efficient and affordable post-training"
"stanford_alpaca, 2023.",""
"","quantization for
large-scale transformers, 2022. URL"
"","https://arxiv.org/abs/2206.01861."
"Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate",""
"language and compiler for tiled neural network computa-",""
"","Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,"
"tions.
In Proceedings of the 3rd ACM SIGPLAN Interna-",""
"","X.,
and Wang, L.
Mm-vet:
Evaluating large multi-"
"tional Workshop on Machine Learning and Programming",""
"","modal models for integrated capabilities. arXiv preprint"
"Languages, pp. 10–19, 2019.",""
"","arXiv:2308.02490, 2023."
"Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,","Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li,"
"M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,","H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-"
"Azhar, F., et al. Llama: Open and efficient foundation lan-","tuning of language models with zero-init attention. arXiv"
"guage models. arXiv preprint arXiv:2302.13971, 2023a.","preprint arXiv:2303.16199, 2023."
