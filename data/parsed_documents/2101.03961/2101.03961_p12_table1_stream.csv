"tokens passed between the layers.
In this section, we consider the scaling properties on a"
"step-basis and a time-basis with a ﬁxed computational budget."
"3.1 Scaling Results on a Step-Basis"
"Figure 4 demonstrates consistent scaling beneﬁts with the number of experts when training"
"all models for a ﬁxed number of steps. We observe a clear trend: when keeping the FLOPS"
"per
token ﬁxed, having more parameters
(experts)
speeds up training.
The
left Figure"
"demonstrates consistent
scaling properties
(with ﬁxed FLOPS per
token) between sparse"
"model parameters and test loss. This reveals the advantage of scaling along this additional"
"axis of
sparse model parameters. Our
right Figure measures
sample eﬃciency of a dense"
"model variant and four FLOP-matched sparse variants. We ﬁnd that increasing the number"
"of experts leads to more sample eﬃcient models. Our Switch-Base 64 expert model achieves"
"the same performance of
the T5-Base model at
step 60k at
step 450k, which is a
7.5x"
"speedup in terms of
step time.
In addition, consistent with the ﬁndings of Kaplan et al."
"(2020), we ﬁnd that
larger models are also more sample eﬃcient—learning more quickly"
"for a ﬁxed number of observed tokens."
