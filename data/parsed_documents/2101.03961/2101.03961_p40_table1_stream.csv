"Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N"
"Gomez,
Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In Advances"
"in neural
information processing systems, pages 5998–6008, 2017."
"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bow-"
"man. Glue: A multi-task benchmark and analysis platform for natural
language under-"
"standing. arXiv preprint arXiv:1804.07461, 2018."
"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix"
"Hill, Omer Levy, and Samuel Bowman.
Superglue: A stickier benchmark for general-"
"purpose language understanding systems.
In Advances in Neural Information Processing"
"Systems, pages 3266–3280, 2019."
"Shibo Wang and Pankaj Kanwar. Bﬂoat16: The secret to high performance on cloud tpus."
"Google Cloud Blog, 2019."
"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,"
"Aditya Barua, and Colin Raﬀel. mt5: A massively multilingual pre-trained text-to-text"
"transformer. arXiv preprint arXiv:2010.11934, 2020."
"Zhilin Yang, Zihang Dai, Yiming Yang,
Jaime Carbonell, Ruslan Salakhutdinov,
and"
"Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding,"
"2020."
"Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-"
"ago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al.
Big bird:"
"Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020."
