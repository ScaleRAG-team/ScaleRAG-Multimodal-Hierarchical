"Editor: Alexander Clark"
"Abstract"
"In deep learning, models
typically reuse
the
same parameters
for
all
inputs.
Mixture"
"of Experts
(MoE) models defy this and instead select diﬀerent parameters
for
each in-"
"coming example.
The result
is a sparsely-activated model—with an outrageous number"
"of parameters—but a constant computational cost. However, despite several notable suc-"
"cesses of MoE, widespread adoption has been hindered by complexity, communication costs,"
"and training instability. We address these with the introduction of the Switch Transformer."
"We simplify the MoE routing algorithm and design intuitive improved models with reduced"
"communication and computational costs. Our proposed training techniques mitigate the"
"instabilities, and we show large sparse models may be trained, for the ﬁrst time, with lower"
"precision (bﬂoat16)
formats. We design models based oﬀ T5-Base and T5-Large (Raﬀel"
"et al., 2019) to obtain up to 7x increases in pre-training speed with the same computational"
"resources. These improvements extend into multilingual settings where we measure gains"
"over the mT5-Base version across all 101 languages. Finally, we advance the current scale"
"of
language models by pre-training up to trillion parameter models on the “Colossal Clean"
"Crawled Corpus”, and achieve a 4x speedup over the T5-XXL model.12"
"Keywords: mixture-of-experts, natural language processing, sparsity, large-scale machine"
"learning, distributed computing"
