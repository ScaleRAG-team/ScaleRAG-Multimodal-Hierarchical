"Initialization Baseline (no distillation)",""
"Init. non-expert weights from teacher
223M","-1.639"
"Table 6: Distilling Switch Transformers for Language Modeling.","Initializing T5-Base with"
"","the non-expert weights from Switch-Base and using a loss from a mixture of teacher"
"","and ground-truth labels obtains the best performance. We can distill 30% of the"
"","performance improvement of a large sparse model with 100x more parameters back"
"","into a small dense model. For a ﬁnal baseline, we ﬁnd no improvement of T5-Base"
"initialized with the expert weights, but trained normally without distillation.",""
