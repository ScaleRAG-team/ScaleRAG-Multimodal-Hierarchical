"Guillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and"
"Herv´e J´egou. Large memory layers with product keys. In Advances in Neural Information"
"Processing Systems, pages 8548–8559, 2019."
"Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris"
"Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models"
"better. arXiv preprint arXiv:2107.06499, 2021."
"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping"
"Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models"
"with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668,"
"2020."
"Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David"
"Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh,
et al."
"Mixed precision training. arXiv preprint arXiv:1710.03740, 2017."
"Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details,
just
the"
"summary!
topic-aware convolutional neural networks for extreme summarization. arXiv"
"preprint arXiv:1808.08745, 2018."
"Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela."
"Adversarial nli: A new benchmark for natural
language understanding.
arXiv preprint"
"arXiv:1910.14599, 2019."
"Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´e Susano Pinto,"
"Sylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert"
"models. arXiv preprint arXiv:2009.13239, 2020."
"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language"
"understanding by generative pre-training, 2018."
"Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael"
"Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning"
"with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019."
"Samyam Rajbhandari, Jeﬀ Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory opti-"
"mization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054,"
"2019."
"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
Squad:
100,000+"
"questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016."
"Prajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models."
"In International Conference on Learning Representations, 2018."
"Herbert Robbins.
Some aspects of
the sequential design of experiments.
Bulletin of
the"
"American Mathematical Society, 58(5):527–535, 1952."
