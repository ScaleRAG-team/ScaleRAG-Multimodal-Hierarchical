"MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",""
"• Differential learning rates: Since the backbone model","However, in real-world scenarios, sampling from language"
"is already well-trained and the MEDUSA heads need","models is often employed to generate diverse responses,"
"more training, we can use separate learning rates for","and the temperature parameter is used merely to modulate"
"them to enable faster convergence of MEDUSA heads","the “creativity” of the response. Therefore, higher temper-"
"while preserving the backbone model’s capability.","atures should result in more opportunities for the original"
"","model to accept the draft model’s output. We ascertain that"
"the beginning of
• Heads warmup: Noticing that at",""
"","it is typically unnecessary to match the distribution of the"
"training, the MEDUSA heads have a large loss, which",""
"","original model. Thus, we propose employing a typical ac-"
"leads to a large gradient and may distort the backbone",""
"","ceptance scheme to select plausible candidates rather than"
"model’s parameters. Following the idea from Kumar",""
"","using rejection sampling. This approach draws inspiration"
"et al. (2022), we can employ a two-stage training pro-",""
"","from truncation sampling studies (Hewitt et al., 2022) (refer"
"cess.
In the first stage, we only train the MEDUSA",""
"","to Appendix A for an in-depth explanation). Our objective"
"heads as MEDUSA-1. In the second stage, we train the",""
"","is to choose candidates that are typical, meaning they are"
"backbone model and MEDUSA heads together with a",""
"","not exceedingly improbable to be produced by the original"
"warmup strategy. Specifically, we first train the back-",""
"","model. We use the prediction probability from the original"
"bone model for a few epochs, then train the MEDUSA",""
"","model as a natural gauge for this and establish a threshold"
"heads together with the backbone model. Besides this",""
"","based on the prediction distribution to determine acceptance."
"simple strategy, we can also use a more sophisticated",""
"","Specifically, given x1, x2, · · ·
, xn as context, when eval-"
"warmup strategy by gradually increasing the weight λ0",""
"","uating the candidate sequence (xn+1, xn+2, · · ·
, xn+K+1)"
"of the backbone model’s loss. We find both strategies",""
"","(composed by top predictions of the original language model"
"work well in practice.",""
"","head and MEDUSA heads), we consider the condition"
