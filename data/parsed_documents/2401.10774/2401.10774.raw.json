{
  "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
  "authors": [
    "Tianle Cai",
    "Yuhong Li",
    "Zhengyang Geng",
    "Hongwu Peng",
    "Jason D. Lee",
    "Deming Chen",
    "Tri Dao"
  ],
  "source_path": "../data/pdf/2401.10774.pdf",
  "page_count": 27,
  "processed_pages": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27
  ],
  "counts": {
    "texts": 616,
    "pictures": 3,
    "tables": 42
  },
  "stats_per_page": [
    {
      "page": 1,
      "text_blocks": 13,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 2,
      "text_blocks": 28,
      "layout_blocks": 2,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 2,
      "tables_found": 0
    },
    {
      "page": 3,
      "text_blocks": 14,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 4,
      "text_blocks": 19,
      "layout_blocks": 1,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 1,
      "tables_found": 1
    },
    {
      "page": 5,
      "text_blocks": 16,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 6,
      "text_blocks": 21,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 7,
      "text_blocks": 48,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 8,
      "text_blocks": 55,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 9,
      "text_blocks": 26,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 10,
      "text_blocks": 25,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 11,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 12,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 13,
      "text_blocks": 11,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 14,
      "text_blocks": 17,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 15,
      "text_blocks": 43,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 16,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 17,
      "text_blocks": 20,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 18,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 19,
      "text_blocks": 22,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 20,
      "text_blocks": 22,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 21,
      "text_blocks": 22,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 22,
      "text_blocks": 12,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 23,
      "text_blocks": 16,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 24,
      "text_blocks": 22,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    },
    {
      "page": 25,
      "text_blocks": 18,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 3
    },
    {
      "page": 26,
      "text_blocks": 15,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 1
    },
    {
      "page": 27,
      "text_blocks": 27,
      "layout_blocks": 0,
      "xobjects_found": 0,
      "xobjects_exported": 0,
      "reused_exported": 0,
      "rasterized": 0,
      "tables_found": 2
    }
  ],
  "texts": [
    {
      "page_no": 1,
      "bbox": [
        73.98899841308594,
        89.9119873046875,
        523.2542724609375,
        122.19116973876953
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads"
    },
    {
      "page_no": 1,
      "bbox": [
        66.09500122070312,
        157.32049560546875,
        529.12353515625,
        170.8991241455078
      ],
      "text": "Tianle Cai * 1 2 Yuhong Li * 3 Zhengyang Geng 4 Hongwu Peng 5 Jason D. Lee 1 Deming Chen 3 Tri Dao 1 2"
    },
    {
      "page_no": 1,
      "bbox": [
        150.197998046875,
        193.9931640625,
        194.68328857421875,
        205.9483642578125
      ],
      "text": "Abstract"
    },
    {
      "page_no": 1,
      "bbox": [
        75.00700378417969,
        212.20730590820312,
        271.1732482910156,
        628.6958618164062
      ],
      "text": "Large Language Models (LLMs) employ auto-\nregressive decoding that requires sequential com-\nputation, with each step reliant on the previous\none’s output. This creates a bottleneck as each\nstep necessitates moving the full model param-\neters from High-Bandwidth Memory (HBM) to\nthe accelerator’s cache. While methods such as\nspeculative decoding have been suggested to ad-\ndress this issue, their implementation is impeded\nby the challenges associated with acquiring and\nmaintaining a separate draft model. In this pa-\nper, we present MEDUSA, an efficient method\nthat augments LLM inference by adding extra\ndecoding heads to predict multiple subsequent\ntokens in parallel. Using a tree-based attention\nmechanism, MEDUSA constructs multiple can-\ndidate continuations and verifies them simulta-\nneously in each decoding step. By leveraging\nparallel processing, MEDUSA substantially re-\nduces the number of decoding steps required. We\npresent two levels of fine-tuning procedures for\nMEDUSA to meet the needs of different use cases:\nMEDUSA-1: MEDUSA is directly fine-tuned on\ntop of a frozen backbone LLM, enabling lossless\ninference acceleration. MEDUSA-2: MEDUSA\nis fine-tuned together with the backbone LLM,\nenabling better prediction accuracy of MEDUSA\nheads and higher speedup but needing a special\ntraining recipe that preserves the model’s capabil-\nities. Moreover, we propose several extensions\nthat improve or expand the utility of MEDUSA,\nincluding a self-distillation to handle situations\nwhere no training data is available and a typical\nacceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate"
    },
    {
      "page_no": 1,
      "bbox": [
        55.215999603271484,
        636.9796752929688,
        290.9273681640625,
        677.3764038085938
      ],
      "text": "*Equal contribution\n1Princeton University\n2Together AI\n3University of Illinois Urbana-Champaign 4Carnegie Mellon Uni-\nversity 5University of Connecticut. Correspondence to: Tianle Cai\n<tianle.cai@princeton.edu>, Yuhong Li <leeyh@illinois.edu>."
    },
    {
      "page_no": 1,
      "bbox": [
        55.1619987487793,
        684.2979125976562,
        289.7516784667969,
        717.2273559570312
      ],
      "text": "Proceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s)."
    },
    {
      "page_no": 1,
      "bbox": [
        327.0069885253906,
        195.53829956054688,
        523.1683349609375,
        265.3520812988281
      ],
      "text": "MEDUSA on models of various sizes and train-\ning procedures. Our experiments demonstrate\nthat MEDUSA-1 can achieve over 2.2× speedup\nwithout compromising generation quality, while\nMEDUSA-2 further improves the speedup to 2.3-\n2.8×."
    },
    {
      "page_no": 1,
      "bbox": [
        307.44000244140625,
        291.99114990234375,
        384.2760925292969,
        303.94635009765625
      ],
      "text": "1. Introduction"
    },
    {
      "page_no": 1,
      "bbox": [
        307.11199951171875,
        312.9093017578125,
        543.181640625,
        538.1400756835938
      ],
      "text": "The recent advancements in Large Language Models\n(LLMs) have demonstrated that the quality of language\ngeneration significantly improves with an increase in model\nsize, reaching billions of parameters (Brown et al., 2020;\nChowdhery et al., 2022; Zhang et al., 2022; Hoffmann et al.,\n2022; OpenAI, 2023; Google, 2023; Touvron et al., 2023).\nHowever, this growth has led to an increase in inference\nlatency, which poses a significant challenge in practical ap-\nplications. From a system perspective, LLM inference is\npredominantly memory-bandwidth-bound (Shazeer, 2019;\nKim et al., 2023), with the main latency bottleneck stem-\nming from accelerators’ memory bandwidth rather than\narithmetic computations. This bottleneck is inherent to\nthe sequential nature of auto-regressive decoding, where\neach forward pass requires transferring the complete model\nparameters from High-Bandwidth Memory (HBM) to the\naccelerator’s cache. This process, which generates only a\nsingle token, underutilizes the arithmetic computation po-\ntential of modern accelerators, leading to inefficiency."
    },
    {
      "page_no": 1,
      "bbox": [
        307.0820007324219,
        546.0352783203125,
        543.0979614257812,
        687.5800170898438
      ],
      "text": "To address this, one approach to speed up LLM inference\ninvolves increasing the arithmetic intensity (the ratio of total\nfloating-point operations (FLOPs) to total data movement)\nof the decoding process and reducing the number of decod-\ning steps. In line with this idea, speculative decoding has\nbeen proposed (Leviathan et al., 2022; Chen et al., 2023;\nXia et al., 2023; Miao et al., 2023). This method uses a\nsmaller draft model to generate a token sequence, which is\nthen refined by the original, larger model for acceptable con-\ntinuation. However, obtaining an appropriate draft model\nremains challenging, and it’s even harder to integrate the\ndraft model into a distributed system (Chen et al., 2023)."
    },
    {
      "page_no": 1,
      "bbox": [
        307.44000244140625,
        695.5466918945312,
        543.0934448242188,
        717.4920043945312
      ],
      "text": "Instead of using a separate draft model to sequentially gen-\nerate candidate outputs, in this paper, we revisit and re-"
    },
    {
      "page_no": 1,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "1"
    },
    {
      "page_no": 1,
      "bbox": [
        10.940000534057617,
        214.47998046875,
        37.619998931884766,
        560.0
      ],
      "text": "arXiv:2401.10774v3  [cs.LG]  14 Jun 2024"
    },
    {
      "page_no": 2,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 2,
      "bbox": [
        55.439998626708984,
        69.39230346679688,
        291.0934143066406,
        222.89212036132812
      ],
      "text": "fine the concept of using multiple decoding heads on top\nof the backbone model to expedite inference (Stern et al.,\n2018). We find that when applied effectively, this tech-\nnique can overcome the challenges of speculative decoding,\nallowing for seamless integration into existing LLM sys-\ntems. Specifically, we introduce MEDUSA, a method that\nenhances LLM inference by integrating additional decoding\nheads to concurrently predict multiple tokens. These heads\nare fine-tuned in a parameter-efficient manner and can be\nadded to any existing model. With no requirement for a\ndraft model, MEDUSA offers easy integration into current\nLLM systems, including those in distributed environments,\nensuring a user-friendly experience."
    },
    {
      "page_no": 2,
      "bbox": [
        54.97200012207031,
        230.93081665039062,
        291.0979309082031,
        456.01806640625
      ],
      "text": "We further enhance MEDUSA with two key insights. Firstly,\nthe current approach of generating a single candidate con-\ntinuation at each decoding step leads to inefficient use of\ncomputational resources. To address this, we propose gener-\nating multiple candidate continuations using the MEDUSA\nheads and verifying them concurrently through a simple\nadjustment to the attention mask. Secondly, we can reuse\nthe rejection sampling scheme as used in speculative de-\ncoding (Leviathan et al., 2022; Chen et al., 2023) to gener-\nate consistent responses with the same distribution as the\noriginal model. However, it cannot further enhance the\nacceleration rate. Alternatively, we introduce a typical ac-\nceptance scheme that selects reasonable candidates from the\nMEDUSA head outputs. We use temperature as a threshold\nto manage deviation from the original model’s predictions,\nproviding an efficient alternative to the rejection sampling\nmethod. Our results suggest that the proposed typical ac-\nceptance scheme can accelerate the decoding speed further\nwhile maintaining a similar generation quality."
    },
    {
      "page_no": 2,
      "bbox": [
        55.082000732421875,
        464.0644226074219,
        291.09796142578125,
        713.0780029296875
      ],
      "text": "To equip LLMs with predictive MEDUSA heads, we propose\ntwo distinct fine-tuning procedures tailored to various sce-\nnarios. For situations with limited computational resources\nor when the objective is to incorporate MEDUSA into an\nexisting model without affecting its performance, we recom-\nmend MEDUSA-1. This method requires minimal memory\nand can be further optimized with quantization techniques\nakin to those in QLoRA (Dettmers et al., 2023), without\ncompromising the generation quality due to the fixed back-\nbone model. However, in MEDUSA-1, the full potential of\nthe backbone model is not utilized. We can further fine-tune\nit to enhance the prediction accuracy of MEDUSA heads,\nwhich can directly lead to a greater speedup. Therefore,\nwe introduce MEDUSA-2, which is suitable for scenarios\nwith ample computational resources or for direct Super-\nvised Fine-Tuning (SFT) from a base model. The key to\nMEDUSA-2 is a training protocol that enables joint training\nof the MEDUSA heads and the backbone model without\ncompromising the model’s next-token prediction capabil-\nity and output quality. We propose different strategies for\nobtaining the training datasets depending on the model’s"
    },
    {
      "page_no": 2,
      "bbox": [
        307.11199951171875,
        69.42604064941406,
        543.0928955078125,
        151.16110229492188
      ],
      "text": "training recipe and dataset availability. When the model is\nfine-tuned on a public dataset, it can be directly used for\nMEDUSA. If the dataset is unavailable or the model un-\nderwent a Reinforcement Learning with Human Feedback\n(RLHF) (Ouyang et al., 2022) process, we suggest a self-\ndistillation approach to generate a training dataset for the\nMEDUSA heads."
    },
    {
      "page_no": 2,
      "bbox": [
        307.0820007324219,
        159.1201629638672,
        543.0911254882812,
        276.6910705566406
      ],
      "text": "Our experiments primarily focus on scenarios with a batch\nsize of one, which is representative of the use case where\nLLMs are locally hosted for personal use. We test MEDUSA\non models of varying sizes and training settings, including\nVicuna-7B, 13B (trained with a public dataset), Vicuna-\n33B (Chiang et al., 2023) (trained with a private dataset1),\nand Zephyr-7B (trained with both supervised fine-tuning and\nalignment). MEDUSA can achieve a speedup of 2.3 to 2.8\ntimes across different prompt types without compromising\non the quality of generation."
    },
    {
      "page_no": 2,
      "bbox": [
        399.40252685546875,
        441.0746765136719,
        513.82763671875,
        460.96697998046875
      ],
      "text": "It is difficult\nIt is difficult not ✅ \nIt' difficult a ❌\nIt is' not ❌  ..."
    },
    {
      "page_no": 2,
      "bbox": [
        335.04931640625,
        367.4275817871094,
        365.4898376464844,
        373.71630859375
      ],
      "text": "Transformer"
    },
    {
      "page_no": 2,
      "bbox": [
        341.82177734375,
        374.46392822265625,
        358.71466064453125,
        380.7526550292969
      ],
      "text": "Layers"
    },
    {
      "page_no": 2,
      "bbox": [
        336.0314636230469,
        398.8565979003906,
        364.5033264160156,
        405.14532470703125
      ],
      "text": "Embedding"
    },
    {
      "page_no": 2,
      "bbox": [
        338.84600830078125,
        328.4931335449219,
        361.68316650390625,
        334.7818603515625
      ],
      "text": "LM Head"
    },
    {
      "page_no": 2,
      "bbox": [
        330.95208740234375,
        308.322265625,
        367.23712158203125,
        321.6473388671875
      ],
      "text": "❄️/🔥\nOriginal Model"
    },
    {
      "page_no": 2,
      "bbox": [
        407.79486083984375,
        356.6385192871094,
        447.5304870605469,
        362.92724609375
      ],
      "text": "Medusa Head 1"
    },
    {
      "page_no": 2,
      "bbox": [
        407.79486083984375,
        375.402099609375,
        447.5304870605469,
        381.6908264160156
      ],
      "text": "Medusa Head 2"
    },
    {
      "page_no": 2,
      "bbox": [
        407.79486083984375,
        394.1656799316406,
        447.5304870605469,
        400.45440673828125
      ],
      "text": "Medusa Head 3"
    },
    {
      "page_no": 2,
      "bbox": [
        355.4986877441406,
        342.5658264160156,
        449.5166931152344,
        355.79150390625
      ],
      "text": "🔥Medusa Heads\nLast Hidden"
    },
    {
      "page_no": 2,
      "bbox": [
        490.2960205078125,
        356.6385192871094,
        509.519287109375,
        362.92724609375
      ],
      "text": "is, ', the"
    },
    {
      "page_no": 2,
      "bbox": [
        491.3074951171875,
        328.4931335449219,
        508.5135803222656,
        334.7818603515625
      ],
      "text": "It, I, As"
    },
    {
      "page_no": 2,
      "bbox": [
        485.50250244140625,
        375.402099609375,
        514.3181762695312,
        381.6908264160156
      ],
      "text": "difficult, is, '"
    },
    {
      "page_no": 2,
      "bbox": [
        482.5926513671875,
        394.1656799316406,
        517.2221069335938,
        400.45440673828125
      ],
      "text": "not, difficult, a"
    },
    {
      "page_no": 2,
      "bbox": [
        320.3902587890625,
        442.0128479003906,
        380.14288330078125,
        455.337890625
      ],
      "text": "What will happen if\nMedusa meets a llama?"
    },
    {
      "page_no": 2,
      "bbox": [
        474.6914367675781,
        314.4204406738281,
        525.59521484375,
        320.70916748046875
      ],
      "text": "🔝Top-k Predictions"
    },
    {
      "page_no": 2,
      "bbox": [
        320.2070007324219,
        432.63104248046875,
        529.7404174804688,
        438.9197692871094
      ],
      "text": "📝Input\n📜Candidates\n✍🏻Single step prediction"
    },
    {
      "page_no": 2,
      "bbox": [
        306.93798828125,
        481.1529235839844,
        542.93359375,
        599.9364013671875
      ],
      "text": "Figure 1. MEDUSA introduces multiple heads on top of the last\nhidden states of the LLM, enabling the prediction of several sub-\nsequent tokens in parallel (Section 2.1.1). During inference, each\nhead generates multiple top predictions for its designated posi-\ntion. These predictions are assembled into candidates, which are\nprocessed in parallel using a tree-based attention mechanism (Sec-\ntion 2.1.2). The final step is to verify the candidates and accept a\ncontinuation. Besides the standard rejection sampling scheme, a\ntypical acceptance scheme (Section 2.3.1) can also be used here\nto select reasonable continuations, and the longest accepted candi-\ndate prefix will be used for the next decoding phase."
    },
    {
      "page_no": 2,
      "bbox": [
        307.44000244140625,
        621.6751708984375,
        386.475830078125,
        633.63037109375
      ],
      "text": "2. Methodology"
    },
    {
      "page_no": 2,
      "bbox": [
        307.44000244140625,
        642.744384765625,
        543.0936279296875,
        688.52001953125
      ],
      "text": "MEDUSA follows the same framework as speculative decod-\ning, where each decoding step primarily consists of three\nsubsteps: (1) generating candidates, (2) processing candi-\ndates, and (3) accepting candidates. For MEDUSA, (1) is"
    },
    {
      "page_no": 2,
      "bbox": [
        307.44000244140625,
        696.7556762695312,
        541.439453125,
        717.2273559570312
      ],
      "text": "1Upon contacting the authors, this version is experimental and\nused some different data than Vicuna 7B and 13B."
    },
    {
      "page_no": 2,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "2"
    },
    {
      "page_no": 3,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 3,
      "bbox": [
        55.439998626708984,
        69.54341888427734,
        290.68792724609375,
        151.16110229492188
      ],
      "text": "achieved by MEDUSA heads, (2) is realized by tree attention,\nand since MEDUSA heads are on top of the original model,\nthe logits calculated in (2) can be used for substep (1) for\nthe next decoding step. The final step (3) can be realized by\neither rejection sampling (Leviathan et al., 2022; Chen et al.,\n2023) or typical acceptance (Section 2.3.1). The overall\npipeline is illustrated in Figure 1."
    },
    {
      "page_no": 3,
      "bbox": [
        55.082000732421875,
        159.05630493164062,
        291.1857604980469,
        252.755859375
      ],
      "text": "In this section, we first introduce the key components of\nMEDUSA, including MEDUSA heads, and tree attention.\nThen, we present two levels of fine-tuning procedures for\nMEDUSA to meet the needs of different use cases. Fi-\nnally, we propose two extensions to MEDUSA, including\nself-distillation and typical acceptance, to handle situations\nwhere no training data is available for MEDUSA and to\nimprove the efficiency of the decoding process, respectively."
    },
    {
      "page_no": 3,
      "bbox": [
        55.439998626708984,
        267.8594970703125,
        145.96018981933594,
        277.82208251953125
      ],
      "text": "2.1. Key Components"
    },
    {
      "page_no": 3,
      "bbox": [
        55.68899917602539,
        286.60345458984375,
        154.4263916015625,
        296.5660705566406
      ],
      "text": "2.1.1. MEDUSA HEADS"
    },
    {
      "page_no": 3,
      "bbox": [
        55.439998626708984,
        305.3276062011719,
        291.09344482421875,
        482.591064453125
      ],
      "text": "In speculative decoding, subsequent tokens are predicted by\nan auxiliary draft model. This draft model must be small yet\neffective enough to generate continuations that the original\nmodel will accept. Fulfilling these requirements is a chal-\nlenging task, and existing approaches (Spector & Re, 2023;\nMiao et al., 2023) often resort to separately pre-training\na smaller model. This pre-training process demands sub-\nstantial additional computational resources. For example,\nin (Miao et al., 2023), a reported 275 NVIDIA A100 GPU\nhours were used. Additionally, separate pre-training can po-\ntentially create a distribution shift between the draft model\nand the original model, leading to continuations that the\noriginal model may not favor. Chen et al. (2023) have also\nhighlighted the complexities of serving multiple models in\na distributed environment."
    },
    {
      "page_no": 3,
      "bbox": [
        54.275001525878906,
        490.55291748046875,
        291.1875,
        707.3020629882812
      ],
      "text": "To streamline and democratize the acceleration of LLM in-\nference, we take inspiration from Stern et al. (2018), which\nutilizes parallel decoding for tasks such as machine transla-\ntion and image super-resolution. MEDUSA heads are addi-\ntional decoding heads appended to the last hidden states of\nthe original model. Specifically, given the original model’s\nlast hidden states ht at position t, we add K decoding heads\nto ht. The k-th head is used to predict the token in the\n(t + k + 1)-th position of the next tokens (the original lan-\nguage model head is used to predict the (t + 1)-th position).\nThe prediction of the k-th head is denoted as p(k)\nt\n, represent-\ning a distribution over the vocabulary, while the prediction\nof the original model is denoted as p(0)\nt . Following the ap-\nproach of Stern et al. (2018), we utilize a single layer of\nfeed-forward network with a residual connection for each\nhead. We find that this simple design is sufficient to achieve\nsatisfactory performance. The definition of the k-th head is\noutlined as:"
    },
    {
      "page_no": 3,
      "bbox": [
        319.6130065917969,
        87.88838195800781,
        529.2695922851562,
        103.74956512451172
      ],
      "text": "p(k)\nt\n= softmax\n\u0010\nW (k)\n2\n·\n\u0010\nSiLU(W (k)\n1\n· ht) + ht\n\u0011\u0011\n,"
    },
    {
      "page_no": 3,
      "bbox": [
        380.3450012207031,
        108.53593444824219,
        529.2695922851562,
        124.61559295654297
      ],
      "text": "where W (k)\n2\n∈Rd×V , W (k)\n1\n∈Rd×d."
    },
    {
      "page_no": 3,
      "bbox": [
        307.1910095214844,
        141.17295837402344,
        543.0970458984375,
        226.80404663085938
      ],
      "text": "d is the output dimension of the LLM’s last hidden layer\nand V is the vocabulary size. We initialize W (k)\n2\nidentically\nto the original language model head, and W (k)\n1\nto zero. This\naligns the initial prediction of MEDUSA heads with that of\nthe original model. The SiLU activation function (Elfwing\net al., 2017) is employed following the Llama models (Tou-\nvron et al., 2023)."
    },
    {
      "page_no": 3,
      "bbox": [
        307.11199951171875,
        234.84938049316406,
        543.09375,
        388.19805908203125
      ],
      "text": "Unlike a draft model, MEDUSA heads are trained in conjunc-\ntion with the original backbone model, which can remain\nfrozen during training (MEDUSA-1) or be trained together\n(MEDUSA-2). This method allows for fine-tuning large mod-\nels even on a single GPU, taking advantage of the powerful\nbase model’s learned representations. Furthermore, it en-\nsures that the distribution of the MEDUSA heads aligns with\nthat of the original model, thereby mitigating the distribution\nshift problem. Additionally, since the new heads consist of\njust a single layer akin to the original language model head,\nMEDUSA does not add complexity to the serving system\ndesign and is friendly to distributed settings. We will discuss\nthe training recipe for MEDUSA heads in Section 2.2."
    },
    {
      "page_no": 3,
      "bbox": [
        307.6889953613281,
        401.9284362792969,
        410.7242736816406,
        411.89105224609375
      ],
      "text": "2.1.2. TREE ATTENTION"
    },
    {
      "page_no": 3,
      "bbox": [
        307.0820007324219,
        420.6345520019531,
        543.1842651367188,
        718.9860229492188
      ],
      "text": "Through MEDUSA heads, we obtain probability predictions\nfor the subsequent K+1 tokens. These predictions enable us\nto create length-K + 1 continuations as candidates. While\nthe speculative decoding studies (Leviathan et al., 2022;\nChen et al., 2023) suggest sampling a single continuation\nas the candidate, leveraging multiple candidates during de-\ncoding can enhance the expected acceptance length within a\ndecoding step. Nevertheless, more candidates can also raise\ncomputational demands. To strike a balance, we employ\na tree-structured attention mechanism to process multiple\ncandidates concurrently. This attention mechanism diverges\nfrom the traditional causal attention paradigm. Within this\nframework, only tokens from the same continuation are\nregarded as historical data. Drawing inspiration from the\nconcept of embedding graph structures into attention as\nproposed in the graph neural network domain (Ying et al.,\n2021), we incorporate the tree structure into our attention\nmask, visualized in Figure 2. Remarkably, similar ideas\nhave also been explored in independent works like Miao\net al. (2023); Spector & Re (2023), where they follow a\nbottom-up approach and construct the tree by merging mul-\ntiple candidates generated by a draft model. In our method,\nwe instead take a top-down approach to build the tree thanks\nto the structure of candidates generated by MEDUSA heads.\nFor a given k-th head, its top-sk predictions serve as the"
    },
    {
      "page_no": 3,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "3"
    },
    {
      "page_no": 4,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 4,
      "bbox": [
        54.9379997253418,
        225.38198852539062,
        290.9321594238281,
        322.0193786621094
      ],
      "text": "Figure 2. We demonstrates the use of tree attention to process mul-\ntiple candidates concurrently. As exemplified, the top-2 predictions\nfrom the first MEDUSA head and the top-3 from the second result\nin a total of 2 × 3 = 6 candidates. Each of these candidates\ncorresponds to a distinct branch within the tree structure. To guar-\nantee that each token only accesses its predecessors, we devise\nan attention mask that exclusively permits attention flow from the\ncurrent token back to its antecedent tokens. The positional indices\nfor positional encoding are adjusted in line with this structure."
    },
    {
      "page_no": 4,
      "bbox": [
        55.082000732421875,
        345.6269226074219,
        291.0961608886719,
        555.0859375
      ],
      "text": "basis for candidate formation, where sk is a designated\nhyperparameter. These candidates are established by de-\ntermining the Cartesian product of the top-sk predictions\nfrom each head. For instance, in Figure 2, with s1 = 2 and\ns2 = 3, each first head prediction can be succeeded by any\nprediction from the second head. This leads to a tree struc-\nture where sk branches exist at the k-th level (considering a\nvirtual root as the 0-level, in practice, this 0-level is for the\nprediction of the language model head of the original model,\nwhich can be sampled independently). Within this tree, only\na token’s predecessors are seen as historical context, and our\nattention mask ensures that the attention is only applied on a\ntoken’s predecessors. By employing this mask and properly\nsetting the positional indices for positional encoding, we\ncan process numerous candidates simultaneously without\nthe need to expand the batch size. The cumulative number\nof new tokens is calculated as PK\nk=1\nQk\ni=1 si."
    },
    {
      "page_no": 4,
      "bbox": [
        55.082000732421875,
        556.1689453125,
        291.0911560058594,
        625.862060546875
      ],
      "text": "In this section, we demonstrate the most simple and regular\nway to construct the tree structure by taking the Cartesian\nproduct. However, it is possible to construct the tree struc-\nture in a more sophisticated way and exploit the unbalanced\naccuracy of different top predictions of different heads. We\nwill discuss this in Section 2.3.3."
    },
    {
      "page_no": 4,
      "bbox": [
        55.44000244140625,
        640.9415283203125,
        154.32876586914062,
        650.9041137695312
      ],
      "text": "2.2. Training Strategies"
    },
    {
      "page_no": 4,
      "bbox": [
        55.082000732421875,
        659.6092529296875,
        289.7930908203125,
        717.4920043945312
      ],
      "text": "At the most basic level, we can train MEDUSA heads by\nfreezing the backbone model and fine-tuning MEDUSA\nheads. However, training the backbone in conjunction with\nthe MEDUSA heads can significantly enhance the accuracy\nof the MEDUSA heads. Depending on the computational"
    },
    {
      "page_no": 4,
      "bbox": [
        307.44000244140625,
        69.39230346679688,
        543.1838989257812,
        91.36188507080078
      ],
      "text": "resources and the specific reqirements of the use case, we\npropose two levels of training strategies for MEDUSA heads."
    },
    {
      "page_no": 4,
      "bbox": [
        307.11199951171875,
        99.28030395507812,
        543.1859130859375,
        169.09408569335938
      ],
      "text": "In this section, we assume the availability of a training\ndataset that aligns with the target model’s output distribution.\nThis could be the dataset used for Supervised Fine-Tuning\n(SFT) of the target model. We will discuss eliminating the\nneed for such a dataset using a self-distillation approach in\nSection 2.3.2."
    },
    {
      "page_no": 4,
      "bbox": [
        307.6889953613281,
        182.82447814941406,
        474.8256530761719,
        192.78707885742188
      ],
      "text": "2.2.1. MEDUSA-1: FROZEN BACKBONE"
    },
    {
      "page_no": 4,
      "bbox": [
        307.0820007324219,
        201.5219268798828,
        542.690185546875,
        334.6990661621094
      ],
      "text": "To train MEDUSA heads with a frozen backbone model, we\ncan use the cross-entropy loss between the prediction of\nMEDUSA heads and the ground truth. Specifically, given\nthe ground truth token yt+k+1 at position t + k + 1, the\nloss for the k-th head is Lk = −log p(k)\nt\n(yt+k+1) where\np(k)\nt\n(y) denotes the probability of token y predicted by the\nk-th head. We also observe that Lk is larger when k is larger,\nwhich is reasonable since the prediction of the k-th head is\nmore uncertain when k is larger. Therefore, we can add a\nweight λk to Lk to balance the loss of different heads. And\nthe total MEDUSA loss is:"
    },
    {
      "page_no": 4,
      "bbox": [
        344.5479736328125,
        353.0747985839844,
        395.1078796386719,
        364.6614990234375
      ],
      "text": "LMEDUSA-1 ="
    },
    {
      "page_no": 4,
      "bbox": [
        397.9239807128906,
        343.08673095703125,
        412.3199462890625,
        360.8659362792969
      ],
      "text": "K\nX"
    },
    {
      "page_no": 4,
      "bbox": [
        397.87298583984375,
        350.36474609375,
        542.107421875,
        374.4965515136719
      ],
      "text": "k=1\n−λk log p(k)\nt\n(yt+k+1).\n(1)"
    },
    {
      "page_no": 4,
      "bbox": [
        307.44000244140625,
        389.43792724609375,
        541.4446411132812,
        519.1830444335938
      ],
      "text": "In practice, we set λk as the k-th power of a constant like\n0.8. Since we only use the backbone model for providing\nthe hidden states, we can use a quantized version of the\nbackbone model to reduce the memory consumption. This\nintroduces a more democratized way to accelerate LLM\ninference, as with the quantization, MEDUSA can be trained\nfor a large model on a single consumer GPU similar to\nQLoRA (Dettmers et al., 2023). The training only takes\na few hours (e.g., 5 hours for MEDUSA-1 on Vicuna 7B\nmodel with a single NVIDIA A100 PCIE GPU to train on\n60k ShareGPT samples)."
    },
    {
      "page_no": 4,
      "bbox": [
        307.6889953613281,
        532.9134521484375,
        460.6607360839844,
        542.8760375976562
      ],
      "text": "2.2.2. MEDUSA-2: JOINT TRAINING"
    },
    {
      "page_no": 4,
      "bbox": [
        307.1310119628906,
        551.6185302734375,
        543.1787109375,
        609.345458984375
      ],
      "text": "To further improve the accuracy of MEDUSA heads, we can\ntrain MEDUSA heads together with the backbone model.\nHowever, this requires a special training recipe to preserve\nthe backbone model’s next-token prediction capability and\noutput quality. To achieve this, we propose three strategies:"
    },
    {
      "page_no": 4,
      "bbox": [
        318.8970031738281,
        624.6414184570312,
        541.4429321289062,
        696.1500244140625
      ],
      "text": "• Combined loss:\nTo keep the backbone model’s\nnext-token prediction capability, we need to add the\ncross-entropy loss of the backbone model LLM =\n−log p(0)\nt (yt+1) to the MEDUSA loss. We also add\na weight λ0 to balance the loss of the backbone model\nand the MEDUSA heads. Therefore, the total loss is:"
    },
    {
      "page_no": 4,
      "bbox": [
        366.1919860839844,
        707.145751953125,
        542.1074829101562,
        718.7315063476562
      ],
      "text": "LMEDUSA-2 = LLM + λ0LMEDUSA-1.\n(2)"
    },
    {
      "page_no": 4,
      "bbox": [
        295.7750244140625,
        732.4114379882812,
        300.7563171386719,
        742.3740234375
      ],
      "text": "4"
    },
    {
      "page_no": 5,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 5,
      "bbox": [
        66.89700317382812,
        69.453369140625,
        289.6083679199219,
        127.25106811523438
      ],
      "text": "• Differential learning rates: Since the backbone model\nis already well-trained and the MEDUSA heads need\nmore training, we can use separate learning rates for\nthem to enable faster convergence of MEDUSA heads\nwhile preserving the backbone model’s capability."
    },
    {
      "page_no": 5,
      "bbox": [
        66.89700317382812,
        136.13441467285156,
        291.0947265625,
        313.6370849609375
      ],
      "text": "• Heads warmup: Noticing that at the beginning of\ntraining, the MEDUSA heads have a large loss, which\nleads to a large gradient and may distort the backbone\nmodel’s parameters. Following the idea from Kumar\net al. (2022), we can employ a two-stage training pro-\ncess. In the first stage, we only train the MEDUSA\nheads as MEDUSA-1. In the second stage, we train the\nbackbone model and MEDUSA heads together with a\nwarmup strategy. Specifically, we first train the back-\nbone model for a few epochs, then train the MEDUSA\nheads together with the backbone model. Besides this\nsimple strategy, we can also use a more sophisticated\nwarmup strategy by gradually increasing the weight λ0\nof the backbone model’s loss. We find both strategies\nwork well in practice."
    },
    {
      "page_no": 5,
      "bbox": [
        55.439998626708984,
        327.67828369140625,
        290.68695068359375,
        385.53607177734375
      ],
      "text": "Putting these strategies together, we can train MEDUSA\nheads together with the backbone model without hurting\nthe backbone model’s capability. Moreover, this recipe can\nbe applied together with Supervised Fine-Tuning (SFT),\nenabling us to get a model with native MEDUSA support."
    },
    {
      "page_no": 5,
      "bbox": [
        55.688995361328125,
        399.2664489746094,
        261.5733642578125,
        409.22906494140625
      ],
      "text": "2.2.3. HOW TO SELECT THE NUMBER OF HEADS"
    },
    {
      "page_no": 5,
      "bbox": [
        55.13100051879883,
        417.9308166503906,
        291.1830139160156,
        499.6130676269531
      ],
      "text": "Empirically, we found that five heads are sufficient at most.\nTherefore, we recommend training with five heads and refer-\nring to the strategy described in Section 2.3.3 to determine\nthe optimal configuration of the tree attention. With opti-\nmized tree attention, sometimes three or four heads may\nbe enough for inference. In this case, we can ignore the\nredundant heads without overhead."
    },
    {
      "page_no": 5,
      "bbox": [
        55.439998626708984,
        514.6925048828125,
        118.82205200195312,
        524.6550903320312
      ],
      "text": "2.3. Extensions"
    },
    {
      "page_no": 5,
      "bbox": [
        55.68899917602539,
        533.4354248046875,
        181.4175567626953,
        543.3980102539062
      ],
      "text": "2.3.1. TYPICAL ACCEPTANCE"
    },
    {
      "page_no": 5,
      "bbox": [
        55.191001892089844,
        552.1643676757812,
        291.1858825683594,
        717.468017578125
      ],
      "text": "In speculative decoding papers (Leviathan et al., 2022; Chen\net al., 2023), authors employ rejection sampling to yield di-\nverse outputs that align with the distribution of the original\nmodel. However, subsequent implementations (Joao Gante,\n2023; Spector & Re, 2023) reveal that this sampling strategy\nresults in diminished efficiency as the sampling tempera-\nture increases. Intuitively, this can be comprehended in the\nextreme instance where the draft model is the same as the\noriginal one: Using greedy decoding, all output of the draft\nmodel will be accepted, therefore maximizing the efficiency.\nConversely, rejection sampling introduces extra overhead,\nas the draft model and the original model are sampled in-\ndependently. Even if their distributions align perfectly, the\noutput of the draft model may still be rejected."
    },
    {
      "page_no": 5,
      "bbox": [
        307.11199951171875,
        69.45993041992188,
        543.185791015625,
        318.5340881347656
      ],
      "text": "However, in real-world scenarios, sampling from language\nmodels is often employed to generate diverse responses,\nand the temperature parameter is used merely to modulate\nthe “creativity” of the response. Therefore, higher temper-\natures should result in more opportunities for the original\nmodel to accept the draft model’s output. We ascertain that\nit is typically unnecessary to match the distribution of the\noriginal model. Thus, we propose employing a typical ac-\nceptance scheme to select plausible candidates rather than\nusing rejection sampling. This approach draws inspiration\nfrom truncation sampling studies (Hewitt et al., 2022) (refer\nto Appendix A for an in-depth explanation). Our objective\nis to choose candidates that are typical, meaning they are\nnot exceedingly improbable to be produced by the original\nmodel. We use the prediction probability from the original\nmodel as a natural gauge for this and establish a threshold\nbased on the prediction distribution to determine acceptance.\nSpecifically, given x1, x2, · · · , xn as context, when eval-\nuating the candidate sequence (xn+1, xn+2, · · · , xn+K+1)\n(composed by top predictions of the original language model\nhead and MEDUSA heads), we consider the condition"
    },
    {
      "page_no": 5,
      "bbox": [
        381.91400146484375,
        338.81182861328125,
        532.179931640625,
        349.90594482421875
      ],
      "text": "poriginal(xn+k|x1, x2, · · · , xn+k−1) >"
    },
    {
      "page_no": 5,
      "bbox": [
        316.7030029296875,
        353.7558288574219,
        532.1796264648438,
        364.8499450683594
      ],
      "text": "min (ϵ, δ exp (−H(poriginal(·|x1, x2, · · · , xn+k−1)))) ,"
    },
    {
      "page_no": 5,
      "bbox": [
        307.0820007324219,
        384.3558044433594,
        543.0975952148438,
        550.0950317382812
      ],
      "text": "where H(·) denotes the entropy function, and ϵ, δ are the\nhard threshold and the entropy-dependent threshold respec-\ntively. This criterion is adapted from Hewitt et al. (2022)\nand rests on two observations: (1) tokens with relatively\nhigh probability are meaningful, and (2) when the distribu-\ntion’s entropy is high, various continuations may be deemed\nreasonable. During decoding, every candidate is evaluated\nusing this criterion, and a prefix of the candidate is accepted\nif it satisfies the condition. To guarantee the generation of\nat least one token at each step, we apply greedy decoding\nfor the first token and unconditionally accept it while em-\nploying typical acceptance for subsequent tokens. The final\nprediction for the current step is determined by the longest\naccepted prefix among all candidates."
    },
    {
      "page_no": 5,
      "bbox": [
        307.0820007324219,
        557.990234375,
        543.09716796875,
        675.6250610351562
      ],
      "text": "Examining this scheme leads to several insights. Firstly,\nwhen the temperature is set to 0, it reverts to greedy decod-\ning, as only the most probable token possesses non-zero\nprobability. As the temperature surpasses 0, the outcome\nof greedy decoding will consistently be accepted with ap-\npropriate ϵ, δ, since those tokens have the maximum prob-\nability, yielding maximal speedup. Likewise, in general\nscenarios, an increased temperature will correspondingly\nresult in longer accepted sequences, as corroborated by our\nexperimental findings."
    },
    {
      "page_no": 5,
      "bbox": [
        307.44000244140625,
        683.5455322265625,
        541.4370727539062,
        717.468017578125
      ],
      "text": "Empirically, we verify that typical acceptance can achieve\na better speedup while maintaining a similar generation\nquality as shown in Figure 5."
    },
    {
      "page_no": 5,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "5"
    },
    {
      "page_no": 6,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 6,
      "bbox": [
        55.68899917602539,
        69.46748352050781,
        169.93849182128906,
        79.43008422851562
      ],
      "text": "2.3.2. SELF-DISTILLATION"
    },
    {
      "page_no": 6,
      "bbox": [
        55.082000732421875,
        88.1964340209961,
        291.0986022949219,
        217.63406372070312
      ],
      "text": "In Section 2.2, we assume the existence of a training dataset\nthat matches the target model’s output distribution. However,\nthis is not always the case. For example, the model owners\nmay only release the model without the training data, or the\nmodel may have gone through a Reinforcement Learning\nwith Human Feedback (RLHF) procedure, which makes the\noutput distribution of the model different from the training\ndataset. To tackle this issue, we propose an automated self-\ndistillation pipeline to use the model itself to generate the\ntraining dataset for MEDUSA heads, which matches the\noutput distribution of the model."
    },
    {
      "page_no": 6,
      "bbox": [
        55.082000732421875,
        225.56675720214844,
        291.1822204589844,
        367.0740661621094
      ],
      "text": "The dataset generation process is straightforward. We first\ntake a public seed dataset from a domain similar to the target\nmodel; for example, using the ShareGPT (ShareGPT, 2023)\ndataset for chat models. Then, we simply take the prompts\nfrom the dataset and ask the model to reply to the prompts.\nIn order to obtain multi-turn conversation samples, we can\nsequentially feed the prompts from the seed dataset to the\nmodel. Or, for models like Zephyr 7B (Tunstall et al., 2023),\nwhich are trained on both roles of the conversation, they\nhave the ability to self-talk, and we can simply feed the\nfirst prompt and let the model generate multiple rounds of\nconversation."
    },
    {
      "page_no": 6,
      "bbox": [
        55.439998626708984,
        374.96929931640625,
        291.1870422363281,
        504.5580749511719
      ],
      "text": "For MEDUSA-1, this dataset is sufficient for training\nMEDUSA heads. However, for MEDUSA-2, we observe\nthat solely using this dataset for training the backbone and\nMEDUSA heads usually leads to a lower generation quality.\nIn fact, even without training MEDUSA heads, training the\nbackbone model with this dataset will lead to performance\ndegradation. This suggests that we also need to use the\noriginal model’s probability prediction instead of using the\nground truth token as the label for the backbone model, sim-\nilar to classic knowledge distillation works (Kim & Rush,\n2016). Concretely, the loss for the backbone model is:"
    },
    {
      "page_no": 6,
      "bbox": [
        107.62799835205078,
        516.1477661132812,
        237.25457763671875,
        531.4015502929688
      ],
      "text": "LLM-distill = KL(p(0)\noriginal,t||p(0)\nt ),"
    },
    {
      "page_no": 6,
      "bbox": [
        55.082000732421875,
        543.0407104492188,
        289.43731689453125,
        568.0280151367188
      ],
      "text": "where p(0)\noriginal,t denotes the probability distribution of the\noriginal model’s prediction at position t."
    },
    {
      "page_no": 6,
      "bbox": [
        55.082000732421875,
        576.0477294921875,
        291.09796142578125,
        717.4920043945312
      ],
      "text": "However, naively, to obtain the original model’s probability\nprediction, we need to maintain two models during training,\nincreasing the memory requirements. To further alleviate\nthis issue, we propose a simple yet effective way to exploit\nthe self-distillation setup. We can use a parameter-efficient\nadapter like LoRA (Hu et al., 2021) for fine-tuning the back-\nbone model. In this way, the original model is simply the\nmodel with the adapter turned off. Therefore, the distillation\ndoes not require additional memory consumption. Together,\nthis self-distillation pipeline can be used to train MEDUSA-2\nwithout hurting the backbone model’s capability and intro-\nduce almost no additional memory consumption. Lastly,"
    },
    {
      "page_no": 6,
      "bbox": [
        307.44000244140625,
        69.49776458740234,
        541.4425659179688,
        115.29605102539062
      ],
      "text": "one tip about using self-distillation is that it is preferable to\nuse LoRA without quantization in this case, otherwise, the\nteacher model will be the quantized model, which may lead\nto a lower generation quality."
    },
    {
      "page_no": 6,
      "bbox": [
        307.6889953613281,
        129.0264434814453,
        502.0738220214844,
        150.94406127929688
      ],
      "text": "2.3.3. SEARCHING FOR THE OPTIMIZED TREE\nCONSTRUCTION"
    },
    {
      "page_no": 6,
      "bbox": [
        307.0820007324219,
        159.55831909179688,
        543.1849365234375,
        253.28207397460938
      ],
      "text": "In Section 2.1.2, we present the simplest way to construct\nthe tree structure by taking the Cartesian product. However,\nwith a fixed budget for the number of total nodes in the\ntree, a regular tree structure may not be the best choice.\nIntuitively, those candidates composed of the top predictions\nof different heads may have different accuracies. Therefore,\nwe can leverage an estimation of the accuracy to construct\nthe tree structure."
    },
    {
      "page_no": 6,
      "bbox": [
        307.44000244140625,
        261.2185363769531,
        541.4421997070312,
        394.5290832519531
      ],
      "text": "Specifically, we can use a calibration dataset and calculate\nthe accuracies of the top predictions of different heads. Let\na(i)\nk denote the accuracy of the i-th top prediction of the k-th\nhead2. Assuming the accuracies are independent, we can\nestimate the accuracy of a candidate sequence composed\nby the top [i1, i2, · · · , ik] predictions of different heads as\nQk\nj=1 a(ij)\nj\n. Let I denote the set of all possible combinations\nof [i1, i2, · · · , ik] and each element of I can be mapped to\na node of the tree (not only leaf nodes but all nodes are\nincluded). Then, the expectation of the acceptance length of\na candidate sequence is:"
    },
    {
      "page_no": 6,
      "bbox": [
        398.0419921875,
        414.6583557128906,
        412.4379577636719,
        424.6209411621094
      ],
      "text": "X"
    },
    {
      "page_no": 6,
      "bbox": [
        380.18798828125,
        431.57537841796875,
        429.7417297363281,
        439.2612609863281
      ],
      "text": "[i1,i2,··· ,ik]∈I"
    },
    {
      "page_no": 6,
      "bbox": [
        432.4749755859375,
        406.84173583984375,
        445.2071838378906,
        424.6209411621094
      ],
      "text": "k\nY"
    },
    {
      "page_no": 6,
      "bbox": [
        431.94598388671875,
        413.89874267578125,
        468.694580078125,
        438.0235595703125
      ],
      "text": "j=1\na(ij)\nj\n."
    },
    {
      "page_no": 6,
      "bbox": [
        307.1310119628906,
        451.30255126953125,
        543.1858520507812,
        556.8720092773438
      ],
      "text": "Thinking about building a tree by adding nodes one by one,\nthe contribution of a new node to the expectation is exactly\nthe accuracy associated with the node. Therefore, we can\ngreedily add nodes to the tree by choosing the node that is\nconnected to the current tree and has the highest accuracy.\nThis process can be repeated until the total number of nodes\nreaches the desired number. In this way, we can construct a\ntree that maximizes the expectation of the acceptance length.\nFurther details can be found in Appendix C."
    },
    {
      "page_no": 6,
      "bbox": [
        307.44000244140625,
        573.8541870117188,
        384.47930908203125,
        585.8093872070312
      ],
      "text": "3. Experiments"
    },
    {
      "page_no": 6,
      "bbox": [
        307.0820007324219,
        594.791015625,
        543.0911865234375,
        688.52001953125
      ],
      "text": "In this section, we present experiments to demonstrate the\neffectiveness of MEDUSA under different settings. First, we\nevaluate MEDUSA on the Vicuna-7B and 13B models (Chi-\nang et al., 2023) to show the performance of MEDUSA-1\nand MEDUSA-2. Then, we assess our method using the\nVicuna-33B and Zephyr-7B models to demonstrate self-\ndistillation’s viability in scenarios where direct access to\nthe fine-tuning recipe is unavailable, as with Vicuna-33B,"
    },
    {
      "page_no": 6,
      "bbox": [
        307.44000244140625,
        696.7556762695312,
        542.563720703125,
        717.20556640625
      ],
      "text": "2Here, the accuracy is defined for the single top i-th token, i.e.,\nthis accuracy is equal to top-i accuracy minus top-(i−1) accuracy."
    },
    {
      "page_no": 6,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "6"
    },
    {
      "page_no": 7,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 7,
      "bbox": [
        148.28018188476562,
        195.5640411376953,
        250.31842041015625,
        212.8180389404297
      ],
      "text": "7B\n13B\nModel Size"
    },
    {
      "page_no": 7,
      "bbox": [
        95.23382568359375,
        190.6232147216797,
        98.90876007080078,
        200.4230499267578
      ],
      "text": "0"
    },
    {
      "page_no": 7,
      "bbox": [
        91.55924987792969,
        176.4483184814453,
        98.90911865234375,
        186.24815368652344
      ],
      "text": "20"
    },
    {
      "page_no": 7,
      "bbox": [
        91.55924987792969,
        162.27342224121094,
        98.90911865234375,
        172.07325744628906
      ],
      "text": "40"
    },
    {
      "page_no": 7,
      "bbox": [
        91.55924987792969,
        148.0985107421875,
        98.90911865234375,
        157.89834594726562
      ],
      "text": "60"
    },
    {
      "page_no": 7,
      "bbox": [
        91.55924987792969,
        133.92361450195312,
        98.90911865234375,
        143.72344970703125
      ],
      "text": "80"
    },
    {
      "page_no": 7,
      "bbox": [
        87.88467407226562,
        119.74871826171875,
        98.90947723388672,
        129.5485382080078
      ],
      "text": "100"
    },
    {
      "page_no": 7,
      "bbox": [
        87.88467407226562,
        105.57382202148438,
        98.90947723388672,
        115.37364959716797
      ],
      "text": "120"
    },
    {
      "page_no": 7,
      "bbox": [
        77.56939697265625,
        117.80372619628906,
        88.34921264648438,
        177.19911193847656
      ],
      "text": "Tokens per Second"
    },
    {
      "page_no": 7,
      "bbox": [
        143.76864624023438,
        118.37432098388672,
        158.42332458496094,
        127.19416809082031
      ],
      "text": "2.18x"
    },
    {
      "page_no": 7,
      "bbox": [
        236.3304443359375,
        131.9391632080078,
        250.98512268066406,
        140.75900268554688
      ],
      "text": "2.33x"
    },
    {
      "page_no": 7,
      "bbox": [
        171.53720092773438,
        97.40019226074219,
        186.19187927246094,
        106.22003936767578
      ],
      "text": "2.83x"
    },
    {
      "page_no": 7,
      "bbox": [
        264.0989685058594,
        119.66480255126953,
        278.7536315917969,
        128.48464965820312
      ],
      "text": "2.83x"
    },
    {
      "page_no": 7,
      "bbox": [
        140.2798614501953,
        81.3994140625,
        256.480712890625,
        93.1592025756836
      ],
      "text": "Speedup on different model sizes"
    },
    {
      "page_no": 7,
      "bbox": [
        259.4096374511719,
        168.10867309570312,
        290.43511962890625,
        192.1956024169922
      ],
      "text": "w/o Medusa\nMedusa-1\nMedusa-2"
    },
    {
      "page_no": 7,
      "bbox": [
        182.7220001220703,
        219.42898559570312,
        192.67469787597656,
        228.3953857421875
      ],
      "text": "(a)"
    },
    {
      "page_no": 7,
      "bbox": [
        321.3880920410156,
        181.7928466796875,
        351.826904296875,
        212.23167419433594
      ],
      "text": "Humanities"
    },
    {
      "page_no": 7,
      "bbox": [
        345.7541198730469,
        181.79002380371094,
        374.1387023925781,
        210.1746063232422
      ],
      "text": "Reasoning"
    },
    {
      "page_no": 7,
      "bbox": [
        370.9828796386719,
        181.7942657470703,
        395.58062744140625,
        206.39202880859375
      ],
      "text": "Roleplay"
    },
    {
      "page_no": 7,
      "bbox": [
        395.7641906738281,
        181.79518127441406,
        417.4734191894531,
        203.50439453125
      ],
      "text": "Writing"
    },
    {
      "page_no": 7,
      "bbox": [
        421.07183837890625,
        181.7960662841797,
        438.83984375,
        199.56407165527344
      ],
      "text": "Stem"
    },
    {
      "page_no": 7,
      "bbox": [
        444.649169921875,
        181.78616333007812,
        461.9474182128906,
        199.08441162109375
      ],
      "text": "Math"
    },
    {
      "page_no": 7,
      "bbox": [
        465.94403076171875,
        181.7919464111328,
        487.3216552734375,
        203.16958618164062
      ],
      "text": "Coding"
    },
    {
      "page_no": 7,
      "bbox": [
        485.9576721191406,
        181.7889862060547,
        513.9859619140625,
        209.8172607421875
      ],
      "text": "Extraction"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        176.00123596191406,
        318.4201965332031,
        185.88450622558594
      ],
      "text": "1.0"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        158.697021484375,
        318.4201965332031,
        168.58029174804688
      ],
      "text": "1.5"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        141.39280700683594,
        318.4201965332031,
        151.2760772705078
      ],
      "text": "2.0"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        124.08859252929688,
        318.4201965332031,
        133.97186279296875
      ],
      "text": "2.5"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        106.78436279296875,
        318.4201965332031,
        116.66764068603516
      ],
      "text": "3.0"
    },
    {
      "page_no": 7,
      "bbox": [
        309.1546325683594,
        89.48014831542969,
        318.4201965332031,
        99.3634262084961
      ],
      "text": "3.5"
    },
    {
      "page_no": 7,
      "bbox": [
        298.7514953613281,
        118.39386749267578,
        309.62310791015625,
        146.62413024902344
      ],
      "text": "Speedup"
    },
    {
      "page_no": 7,
      "bbox": [
        330.0555419921875,
        112.15839385986328,
        438.1849060058594,
        127.82623291015625
      ],
      "text": "2.58x\n2.58x\n2.7x\n2.72x\n2.77x"
    },
    {
      "page_no": 7,
      "bbox": [
        446.7429504394531,
        103.98418426513672,
        461.52239990234375,
        112.8791275024414
      ],
      "text": "3.01x"
    },
    {
      "page_no": 7,
      "bbox": [
        470.0804443359375,
        94.0639419555664,
        484.8598937988281,
        102.9588851928711
      ],
      "text": "3.29x"
    },
    {
      "page_no": 7,
      "bbox": [
        493.41790771484375,
        82.80863952636719,
        508.1973571777344,
        91.70358276367188
      ],
      "text": "3.62x"
    },
    {
      "page_no": 7,
      "bbox": [
        337.7052307128906,
        65.84739685058594,
        499.7584228515625,
        77.70732116699219
      ],
      "text": "Speedup on different categories for Vicuna-7B"
    },
    {
      "page_no": 7,
      "bbox": [
        403.6600036621094,
        219.42898559570312,
        414.1148376464844,
        228.3953857421875
      ],
      "text": "(b)"
    },
    {
      "page_no": 7,
      "bbox": [
        54.938018798828125,
        243.0162811279297,
        543.0081787109375,
        274.223388671875
      ],
      "text": "Figure 3. Left: Speed comparison of baseline, MEDUSA-1 and MEDUSA-2 on Vicuna-7B/13B. MEDUSA-1 achieves more than 2×\nwall-time speedup compared to the baseline implementation while MEDUSA-2 further improves the speedup by a significant margin.\nRight: Detailed speedup performance of Vicuna-7B with MEDUSA-2 on 8 categories from MT-Bench."
    },
    {
      "page_no": 7,
      "bbox": [
        55.439998626708984,
        296.270263671875,
        290.6881408691406,
        354.11407470703125
      ],
      "text": "and in models like Zephyr-7B that employ Reinforcement\nLearning from Human Feedback (RLHF). The evaluation is\nconducted on MT-Bench (Zheng et al., 2023), a multi-turn,\nconversational-format benchmark. Detailed settings can be\nfound in Appendix B."
    },
    {
      "page_no": 7,
      "bbox": [
        55.439998626708984,
        369.1935119628906,
        286.80072021484375,
        391.111083984375
      ],
      "text": "3.1. Case Study: MEDUSA-1 v.s. MEDUSA-2 on Vicuna\n7B and 13B"
    },
    {
      "page_no": 7,
      "bbox": [
        54.69300079345703,
        399.8321533203125,
        291.09442138671875,
        529.4070434570312
      ],
      "text": "Experimental Setup. We use the Vicuna model class (Chi-\nang et al., 2023), which encompasses chat models of vary-\ning sizes (7B, 13B, 33B) that are fine-tuned from the Llama\nmodel (Touvron et al., 2023). Among them, the 7B and\n13B models are trained on the ShareGPT (ShareGPT, 2023)\ndataset, while the 33B model is an experimental model and\nis trained on a private dataset. In this section, we use the\nShareGPT dataset to train the MEDUSA heads on the 7B\nand 13B models for 2 epochs. We use the v1.5 version of\nVicuna models, which are fine-tuned from Llama-2 models\nwith sequence length 4096."
    },
    {
      "page_no": 7,
      "bbox": [
        54.69300079345703,
        537.2094116210938,
        291.187255859375,
        714.7288818359375
      ],
      "text": "Results. We collect the results and show them in Fig. 3.\nThe baseline is the default Huggingface implementation.\nIn Fig. 3a, we can see that for the 7B models, MEDUSA-\n1 and MEDUSA-2 configurations lead to a significant in-\ncrease in speed, measuring in tokens processed per second.\nMEDUSA-1 shows a 2.18× speedup, while MEDUSA-2 fur-\nther improves this to a 2.83×. When applied to the larger\n13B model, MEDUSA-1 results in a 2.33× speed increase,\nwhile MEDUSA-2 maintains a similar performance gain of\n2.83× over the baseline. We also plot the speedup per cate-\ngory for MEDUSA-2 Vicuna-7B model. We observe that the\ncoding category benefits from a 3.29× speedup, suggesting\nthat MEDUSA is particularly effective for tasks in this do-\nmain. This points to a significant potential for optimizing\ncoding LLMs, which are widely used in software develop-"
    },
    {
      "page_no": 7,
      "bbox": [
        307.44000244140625,
        296.4064025878906,
        543.215087890625,
        354.11407470703125
      ],
      "text": "ment and other programming-related tasks. The “Extraction”\ncategory shows the highest speedup at 3.62×, indicating\nthat this task is highly optimized by the MEDUSA. Overall,\nthe results suggest that the MEDUSA significantly enhances\ninference speed across different model sizes and tasks."
    },
    {
      "page_no": 7,
      "bbox": [
        307.44000244140625,
        369.1935119628906,
        519.3045654296875,
        391.111083984375
      ],
      "text": "3.2. Case Study: Training with Self-Distillation on\nVicuna-33B and Zephyr-7B"
    },
    {
      "page_no": 7,
      "bbox": [
        306.9419860839844,
        399.7254333496094,
        543.0985717773438,
        589.1820678710938
      ],
      "text": "Experimental Setup.\nIn this case study, we focus on\nthe cases where self-distillation is needed. We use the\nVicuna-33B model (Chiang et al., 2023) and the Zephyr-\n7B model (Tunstall et al., 2023) as examples.\nFollow-\ning the procedure described in Section 2.3.2, we first\ngenerate the datasets with some seed prompts. We use\nShareGPT (ShareGPT, 2023) and UltraChat (Ding et al.,\n2023) as the seed datasets and collect a dataset at about\n100k samples for both cases. Interestingly, we find that the\nZephyr model can continue to generate multiple rounds of\nconversation with a single prompt, which makes it easy to\ncollect a large dataset. For Vicuna-33B, we generate the\nmulti-turn conversations by iteratively feeding the prompts\nfrom each multi-turn seed conversation using random sam-\npling with temperature 0.3. Both models are trained with\nsequence length 2048 and batch size 128."
    },
    {
      "page_no": 7,
      "bbox": [
        307.0820007324219,
        597.1075439453125,
        543.097900390625,
        714.6878662109375
      ],
      "text": "Results. Table 1 complements these findings by comparing\nvarious MEDUSA-2 models in terms of their acceleration\nrate, overhead, and quality on MT-Bench with GPT-4 acting\nas the evaluator to assign performance scores ranging from\n0 to 10. We report the quality differences of MEDUSA com-\npared to the original model. Notably, while the MEDUSA-2\nVicuna-33B model shows a lower acceleration rate, it main-\ntains a comparable quality. We hypothesize that this is due\nto a mismatch between the hidden training dataset and the\ndataset we used for self-distillation. Hence, the model’s gen-"
    },
    {
      "page_no": 7,
      "bbox": [
        295.82501220703125,
        732.4114379882812,
        300.8063049316406,
        742.3740234375
      ],
      "text": "7"
    },
    {
      "page_no": 8,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 8,
      "bbox": [
        105.06930541992188,
        166.0911865234375,
        287.0538330078125,
        183.49209594726562
      ],
      "text": "0\n50\n100\n150\n200\n250\nNumber of Candidate Tokens"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        156.70278930664062,
        95.98518371582031,
        166.5860595703125
      ],
      "text": "1.0"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        138.74432373046875,
        95.98518371582031,
        148.62759399414062
      ],
      "text": "1.5"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        120.78582763671875,
        95.98518371582031,
        130.66909790039062
      ],
      "text": "2.0"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        102.82734680175781,
        95.98518371582031,
        112.71062469482422
      ],
      "text": "2.5"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        84.86886596679688,
        95.98518371582031,
        94.75213623046875
      ],
      "text": "3.0"
    },
    {
      "page_no": 8,
      "bbox": [
        86.71961212158203,
        66.91038513183594,
        95.98518371582031,
        76.79365539550781
      ],
      "text": "3.5"
    },
    {
      "page_no": 8,
      "bbox": [
        76.3165054321289,
        102.5628662109375,
        87.18810272216797,
        132.66448974609375
      ],
      "text": "Acc. Rate"
    },
    {
      "page_no": 8,
      "bbox": [
        114.6033935546875,
        155.20645141601562,
        145.89303588867188,
        164.1013946533203
      ],
      "text": "w/o Medusa"
    },
    {
      "page_no": 8,
      "bbox": [
        118.15753173828125,
        71.39440155029297,
        174.61753845214844,
        80.28934478759766
      ],
      "text": "Sparse Tree Attention"
    },
    {
      "page_no": 8,
      "bbox": [
        181.4759979248047,
        190.10696411132812,
        191.42869567871094,
        199.0733642578125
      ],
      "text": "(a)"
    },
    {
      "page_no": 8,
      "bbox": [
        327.8607177734375,
        166.2420196533203,
        508.3088073730469,
        183.4960174560547
      ],
      "text": "0\n50\n100\n150\n200\n250\nNumber of Candidate Tokens"
    },
    {
      "page_no": 8,
      "bbox": [
        311.50323486328125,
        138.1786346435547,
        318.85308837890625,
        147.9784698486328
      ],
      "text": "60"
    },
    {
      "page_no": 8,
      "bbox": [
        311.50323486328125,
        114.68811798095703,
        318.85308837890625,
        124.48794555664062
      ],
      "text": "80"
    },
    {
      "page_no": 8,
      "bbox": [
        307.82867431640625,
        91.19757843017578,
        318.8534851074219,
        100.99740600585938
      ],
      "text": "100"
    },
    {
      "page_no": 8,
      "bbox": [
        307.82867431640625,
        67.7070541381836,
        318.8534851074219,
        77.50688171386719
      ],
      "text": "120"
    },
    {
      "page_no": 8,
      "bbox": [
        297.5133972167969,
        93.11471557617188,
        308.293212890625,
        143.2477569580078
      ],
      "text": "Speed (token/s)"
    },
    {
      "page_no": 8,
      "bbox": [
        337.3143310546875,
        155.4491729736328,
        368.3398132324219,
        164.26901245117188
      ],
      "text": "w/o Medusa"
    },
    {
      "page_no": 8,
      "bbox": [
        454.3900146484375,
        72.34474182128906,
        510.3733215332031,
        81.16458892822266
      ],
      "text": "Sparse Tree Attention"
    },
    {
      "page_no": 8,
      "bbox": [
        54.93798828125,
        190.10696411132812,
        541.751953125,
        244.52239990234375
      ],
      "text": "(b)\nFigure 4. Effectiveness of numbers of candidate tokens for decoding introduced by trees (default number of candidate token for decoding\nis 1 when using KV cache). Left: The acceleration rate for randomly sampled dense tree settings (blue dots) and optimized sparse tree\nsettings (red stars). Right: The speed (tokens/s) for both settings. The trend lines indicate that while the acceleration rate remains relatively\nstable for sparse trees, there is a notable decrease in speed as the candidate tokens increases."
    },
    {
      "page_no": 8,
      "bbox": [
        62.104000091552734,
        268.0971374511719,
        281.7159423828125,
        275.0709228515625
      ],
      "text": "Model Name\nVicuna-7B\nZephyr-7B\nVicuna-13B\nVicuna-33B"
    },
    {
      "page_no": 8,
      "bbox": [
        62.104000091552734,
        281.06915283203125,
        282.77587890625,
        303.9829406738281
      ],
      "text": "Acc. rate\n3.47\n3.14\n3.51\n3.01\nOverhead\n1.22\n1.18\n1.23\n1.27\nQuality\n6.18 (+0.01)\n7.25 (-0.07)\n6.43 (-0.14)\n7.18 (+0.05)"
    },
    {
      "page_no": 8,
      "bbox": [
        62.104000091552734,
        309.82073974609375,
        260.25201416015625,
        325.4405212402344
      ],
      "text": "SSpecDecoding\n1.47\n-\n1.56\n1.60\nSMEDUSA\n2.83\n2.66\n2.83\n2.35"
    },
    {
      "page_no": 8,
      "bbox": [
        54.893001556396484,
        338.3503112792969,
        290.9248352050781,
        391.2203674316406
      ],
      "text": "Table 1. Comparison of various MEDUSA-2 models. The first\nsection reports the details of MEDUSA-2, including accelerate rate,\noverhead, and quality that denoted the average scores on the MT-\nBench compared to the original models. The second section lists\nthe speedup (S) of SpecDecoding and MEDUSA, respectively."
    },
    {
      "page_no": 8,
      "bbox": [
        55.082000732421875,
        417.98016357421875,
        291.09344482421875,
        487.6570739746094
      ],
      "text": "eration quality can be well aligned by self-distillation while\nMEDUSA heads learn distribution from the self-distillation\nthat potentially shifts from the training set. In our study,\nwe also applied speculative decoding (Chen et al., 2023;\nLeviathan et al., 2022) to the Vicuna lineup using open-\nsource draft models (details can be found in Appendix D)."
    },
    {
      "page_no": 8,
      "bbox": [
        55.13100051879883,
        495.55230712890625,
        291.0979919433594,
        577.321044921875
      ],
      "text": "These results underscore the complex interplay between\nspeed and performance when scaling up model sizes and\napplying self-distillation techniques. The findings also high-\nlight the potential of the MEDUSA-2 configuration to boost\nefficiency in processing while carefully preserving the qual-\nity of the model’s outputs, suggesting a promising direction\nfor co-optimizing LLMs with MEDUSA heads."
    },
    {
      "page_no": 8,
      "bbox": [
        55.44000244140625,
        592.4005126953125,
        137.37240600585938,
        602.3630981445312
      ],
      "text": "3.3. Ablation Study"
    },
    {
      "page_no": 8,
      "bbox": [
        55.689002990722656,
        611.1444702148438,
        246.74331665039062,
        621.1070556640625
      ],
      "text": "3.3.1. CONFIGURATION OF TREE ATTENTION"
    },
    {
      "page_no": 8,
      "bbox": [
        55.439998626708984,
        629.7212524414062,
        289.43701171875,
        675.6250610351562
      ],
      "text": "The study of tree attention is conducted on the writing\nand roleplay categories from the MT-Bench dataset using\nMEDUSA-2 Vicuna-7B. We target to depict tree attention’s\nmotivation and its performance."
    },
    {
      "page_no": 8,
      "bbox": [
        55.439998626708984,
        683.6666259765625,
        290.6883544921875,
        717.4764404296875
      ],
      "text": "Fig. 4a compares the acceleration rate of randomly sampled\ndense tree configurations (Section. 2.1.2, depicted by blue\ndots) against optimized sparse tree settings (Section. 2.3.3,"
    },
    {
      "page_no": 8,
      "bbox": [
        307.1310119628906,
        266.7054138183594,
        543.1859130859375,
        420.0540771484375
      ],
      "text": "shown with red stars). The sparse tree configuration with 64\nnodes shows a better acceleration rate than the dense tree\nsettings with 256 nodes. The decline in speed in Fig. 4b\nis attributed to the increased overhead introduced by the\ncompute-bound. While a more complex tree can improve\nacceleration, it does so at the cost of speed due to intensive\nmatrix multiplications for linear layers and self-attention.\nThe acceleration rate increase follows a logarithmic trend\nand slows down when the tree size grows as shown in Fig. 4a.\nHowever, the initial gains are substantial, allowing Medusa\nto achieve significant speedups. If the acceleration increase\nis less than the overhead, it will slow down overall perfor-\nmance. For detailed study, please refer to Appendix G."
    },
    {
      "page_no": 8,
      "bbox": [
        336.16815185546875,
        544.9923095703125,
        506.34869384765625,
        561.0231323242188
      ],
      "text": "0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nPosterior Thresholds"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        526.6700439453125,
        336.7455749511719,
        535.736572265625
      ],
      "text": "3.0"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        509.3194274902344,
        336.7455749511719,
        518.3859252929688
      ],
      "text": "3.1"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        491.96875,
        336.7455749511719,
        501.0352783203125
      ],
      "text": "3.2"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        474.61810302734375,
        336.7455749511719,
        483.68463134765625
      ],
      "text": "3.3"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        457.2674255371094,
        336.7455749511719,
        466.3339538574219
      ],
      "text": "3.4"
    },
    {
      "page_no": 8,
      "bbox": [
        328.2457275390625,
        439.9167785644531,
        336.7455749511719,
        448.9833068847656
      ],
      "text": "3.5"
    },
    {
      "page_no": 8,
      "bbox": [
        318.6394348144531,
        478.48797607421875,
        328.5301818847656,
        505.873779296875
      ],
      "text": "Acc. Rate"
    },
    {
      "page_no": 8,
      "bbox": [
        314.60955810546875,
        518.7449340820312,
        335.7266845703125,
        528.6356811523438
      ],
      "text": "Greedy"
    },
    {
      "page_no": 8,
      "bbox": [
        328.0263366699219,
        531.1982421875,
        335.7826232910156,
        541.0889892578125
      ],
      "text": "RS"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        538.3380126953125,
        520.6099853515625,
        547.404541015625
      ],
      "text": "7.0"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        521.7376098632812,
        520.6099853515625,
        530.8041381835938
      ],
      "text": "7.1"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        505.13720703125,
        520.6099853515625,
        514.2037353515625
      ],
      "text": "7.2"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        488.53680419921875,
        520.6099853515625,
        497.60333251953125
      ],
      "text": "7.3"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        471.9363708496094,
        520.6099853515625,
        481.0028991699219
      ],
      "text": "7.4"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        455.3359680175781,
        520.6099853515625,
        464.4024963378906
      ],
      "text": "7.5"
    },
    {
      "page_no": 8,
      "bbox": [
        512.110107421875,
        438.7355651855469,
        520.6099853515625,
        447.8020935058594
      ],
      "text": "7.6"
    },
    {
      "page_no": 8,
      "bbox": [
        519.0386962890625,
        482.4924011230469,
        528.929443359375,
        501.8658142089844
      ],
      "text": "Scores"
    },
    {
      "page_no": 8,
      "bbox": [
        512.4415283203125,
        460.8928527832031,
        533.5587158203125,
        476.59375
      ],
      "text": "Greedy\nRS"
    },
    {
      "page_no": 8,
      "bbox": [
        306.93798828125,
        579.933349609375,
        543.01171875,
        654.7203979492188
      ],
      "text": "Figure 5. Performance comparison of MEDUSA using proposed\ntypical sampling. The model is fully fine-tuned from Vicuna-7B.\nThe plot illustrates the acceleration rate and average scores on the\nwriting and roleplay (MT-Bench) with a fixed temperature of 0.7\nfor 3 different settings: greedy sampling and random sampling\n(RS) plotted as the star and the dot, and typical sampling curves\nunder different thresholds."
    },
    {
      "page_no": 8,
      "bbox": [
        307.6889953613281,
        676.8974609375,
        507.508544921875,
        686.8600463867188
      ],
      "text": "3.3.2. THRESHOLDS OF TYPICAL ACCEPTANCE"
    },
    {
      "page_no": 8,
      "bbox": [
        307.1310119628906,
        695.4752807617188,
        541.4432373046875,
        717.4920043945312
      ],
      "text": "The thresholds of typical acceptance are studied on\nthe writing and roleplay categories from the MT-Bench"
    },
    {
      "page_no": 8,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "8"
    },
    {
      "page_no": 9,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 9,
      "bbox": [
        55.082000732421875,
        69.39230346679688,
        291.1878967285156,
        222.89205932617188
      ],
      "text": "dataset (Zheng et al., 2023) using MEDUSA-2 Vicuna 7B.\nUtilizing the Vicuna 7B model, we aligned our methodology\nwith the approach delineated by (Hewitt et al., 2022) set-\nting the α = √ϵ. Fig. 5 presents a comparative analysis of\nour model’s performance across various sampling settings.\nThese settings range from a threshold ϵ starting at 0.01 and\nincrementally increasing to 0.25 in steps of 0.01. Our obser-\nvations indicate a discernible trade-off: as ϵ increases, there\nis an elevation in quality at the expense of a reduced accel-\neration rate. Furthermore, for tasks demanding creativity, it\nis noted that the default random sampling surpasses greedy\nsampling in performance, and the proposed typical sampling\nis comparable with random sampling when ϵ increases."
    },
    {
      "page_no": 9,
      "bbox": [
        98.96600341796875,
        238.31112670898438,
        275.9101867675781,
        245.28492736816406
      ],
      "text": "Baseline\nDirect Fine-tuning\nMEDUSA-1\nMEDUSA-2"
    },
    {
      "page_no": 9,
      "bbox": [
        62.99300003051758,
        251.28317260742188,
        265.6795349121094,
        266.22796630859375
      ],
      "text": "Quality\n6.17\n5.925\n6.23\n6.18\nSpeedup\nN/A\nN/A\n2.18\n2.83"
    },
    {
      "page_no": 9,
      "bbox": [
        54.893001556396484,
        279.6792907714844,
        289.7561950683594,
        310.6043701171875
      ],
      "text": "Table 2. Comparison of Different Settings of Vicuna-7B. Quality\nis obtained by evaluating models on MT-Bench using GPT-4 as\nthe judge (higher the better)."
    },
    {
      "page_no": 9,
      "bbox": [
        55.68899917602539,
        330.17645263671875,
        280.1632080078125,
        340.1390686035156
      ],
      "text": "3.3.3. EFFECTIVENESS OF TWO-STAGE FINE-TUNING"
    },
    {
      "page_no": 9,
      "bbox": [
        54.69300079345703,
        348.88916015625,
        291.09368896484375,
        490.2980651855469
      ],
      "text": "Table 2 shows the performance differences between various\nfine-tuning strategies for the Vicuna-7B model. MEDUSA-\n1, which fine-tunes only the MEDUSA heads, achieves\na 2.18x speedup without compromising generation qual-\nity.\nMEDUSA-2, which employs two-stage fine-tuning\n(Section 2.2.2), maintains generation quality and provides\ngreater speedup (2.83x) compared to MEDUSA-1. In con-\ntrast, direct fine-tuning the model with the MEDUSA heads\nresults in degraded generation quality.\nThe findings in-\ndicate that implementing our MEDUSA-2 for fine-tuning\nmaintains the model’s quality and concurrently improves\nthe speedup versus MEDUSA-1."
    },
    {
      "page_no": 9,
      "bbox": [
        71.69400024414062,
        511.1189880371094,
        270.6969299316406,
        535.217041015625
      ],
      "text": "Table 3. Impact of Techniques on Speedup\nTechnique\nSpeedup"
    },
    {
      "page_no": 9,
      "bbox": [
        71.69400024414062,
        541.8517456054688,
        266.13153076171875,
        588.0400390625
      ],
      "text": "Medusa-1 heads without tree attention\n∼1.5x\nAdding tree attention\n∼1.9x\nUsing optimized tree configuration\n∼2.2x\nTraining heads with Medusa-2\n∼2.8x"
    },
    {
      "page_no": 9,
      "bbox": [
        55.439998626708984,
        614.7811889648438,
        121.20556640625,
        626.7363891601562
      ],
      "text": "4. Discussion"
    },
    {
      "page_no": 9,
      "bbox": [
        55.439998626708984,
        635.8199462890625,
        291.09613037109375,
        717.4920043945312
      ],
      "text": "In conclusion, MEDUSA enhances LLM inference speed by\n2.3-2.8 times by equipping models with additional predic-\ntive decoding heads, allowing for generating multiple tokens\nsimultaneously and bypassing the sequential decoding limi-\ntation. Key advantages of MEDUSA include its simplicity,\nparameter efficiency, and ease of integration into existing\nsystems. MEDUSA avoids the need for specialized draft"
    },
    {
      "page_no": 9,
      "bbox": [
        307.44000244140625,
        69.47882080078125,
        543.09619140625,
        139.20608520507812
      ],
      "text": "models. The typical acceptance scheme removes complica-\ntions from rejection sampling while providing reasonable\noutputs. Our approach including two efficient training pro-\ncedures, ensures high-quality output across various models\nand prompt types. We summarize the development of each\ntechnique and their impact on the speedup in Table 3."
    },
    {
      "page_no": 9,
      "bbox": [
        307.44000244140625,
        147.10128784179688,
        543.0933227539062,
        204.96005249023438
      ],
      "text": "In the paper, we focus on the setting with batch size 1 for\nsimplicity. Yet, we want to emphasize that the ideas pre-\nsented in our paper can be generalized to larger batch-size\nsettings, which are now supported by libraries like TensorRT\nand Huggingface TGI following our paper."
    },
    {
      "page_no": 9,
      "bbox": [
        307.44000244140625,
        221.942138671875,
        406.2736511230469,
        233.8973388671875
      ],
      "text": "Acknowledgements"
    },
    {
      "page_no": 9,
      "bbox": [
        306.97198486328125,
        242.86032104492188,
        541.4364624023438,
        264.8530578613281
      ],
      "text": "We extend our heartfelt gratitude to several individuals\nwhose contributions were invaluable to this project:"
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        281.02825927734375,
        543.0961303710938,
        314.9620666503906
      ],
      "text": "• Zhuohan Li, for his invaluable insights on LLM serv-\ning. If you haven’t already, do check out Zhuohan’s\nvLLM project—it’s nothing short of impressive."
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        325.4402770996094,
        541.4356689453125,
        347.4330749511719
      ],
      "text": "• Shaojie Bai, for engaging in crucial discussions that\nhelped shape the early phases of this work."
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        357.9385070800781,
        541.4400024414062,
        391.8600769042969
      ],
      "text": "• Denny Zhou, for introducing the truncation sampling\nscheme to Tianle and encouraging Tianle to explore\nthe area of LLM serving."
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        402.33929443359375,
        543.09326171875,
        436.2870788574219
      ],
      "text": "• Yanping Huang,\nfor pointing out the memory-\nbandwidth-bound challenges associated with LLM\nserving to Tianle."
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        446.7652893066406,
        541.4356689453125,
        468.7580871582031
      ],
      "text": "• Lianmin Zheng, for clarifying the different training\nrecipes used in different sizes of Vicuna models."
    },
    {
      "page_no": 9,
      "bbox": [
        307.0820007324219,
        484.9192810058594,
        541.715576171875,
        530.8220825195312
      ],
      "text": "Jason D. Lee acknowledges the support of the NSF CCF\n2002272, NSF IIS 2107304, and NSF CAREER Award\n2144994. Deming Chen acknowledges the support from the\nAMD Center of Excellence at UIUC."
    },
    {
      "page_no": 9,
      "bbox": [
        307.44000244140625,
        547.8041381835938,
        398.7418518066406,
        559.7593383789062
      ],
      "text": "Impact Statement"
    },
    {
      "page_no": 9,
      "bbox": [
        307.11199951171875,
        568.7222900390625,
        543.0907592773438,
        626.5810546875
      ],
      "text": "The introduction of MEDUSA, an innovative method to\nimprove the inference speed of Large Language Models\n(LLMs), presents a range of broader implications for so-\nciety, technology, and ethics. This section explores these\nimplications in detail."
    },
    {
      "page_no": 9,
      "bbox": [
        307.44000244140625,
        641.6605224609375,
        475.62860107421875,
        651.6231079101562
      ],
      "text": "Societal and Technological Implications"
    },
    {
      "page_no": 9,
      "bbox": [
        318.8970031738281,
        659.5667724609375,
        543.09423828125,
        717.4920043945312
      ],
      "text": "• Accessibility and Democratization of AI: By signif-\nicantly enhancing the efficiency of LLMs, MEDUSA\nmakes advanced AI technologies more accessible to\na wider range of users and organizations. Democrati-\nzation can spur innovation across various sectors, in-"
    },
    {
      "page_no": 9,
      "bbox": [
        295.95001220703125,
        732.4114379882812,
        300.9313049316406,
        742.3740234375
      ],
      "text": "9"
    },
    {
      "page_no": 10,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 10,
      "bbox": [
        75.36599731445312,
        69.39604187011719,
        291.0937194824219,
        103.34109497070312
      ],
      "text": "cluding education, healthcare, and entertainment, po-\ntentially leading to breakthroughs that benefit society\nat large."
    },
    {
      "page_no": 10,
      "bbox": [
        66.8969955444336,
        112.79744720458984,
        291.1834411621094,
        182.70309448242188
      ],
      "text": "• Environmental Impact: The acceleration for LLM\ninference due to MEDUSA could lead to decreased\nenergy consumption and a smaller carbon footprint.\nThis aligns with the growing need for sustainable AI\npractices, contributing to environmental conservation\nefforts."
    },
    {
      "page_no": 10,
      "bbox": [
        66.8969955444336,
        192.16041564941406,
        291.17645263671875,
        262.05181884765625
      ],
      "text": "• Economic Implications: The increased efficiency\nbrought about by MEDUSA may lower the cost barrier\nto deploying state-of-the-art AI models, enabling small\nand medium-sized enterprises to leverage advanced AI\ncapabilities. This could stimulate economic growth,\nfoster competition, and drive technological innovation."
    },
    {
      "page_no": 10,
      "bbox": [
        55.439998626708984,
        278.1485290527344,
        152.58529663085938,
        288.1111145019531
      ],
      "text": "Ethical Considerations"
    },
    {
      "page_no": 10,
      "bbox": [
        66.8969955444336,
        296.05859375,
        289.4418640136719,
        377.8660888671875
      ],
      "text": "• Bias and Fairness: While MEDUSA aims to improve\nLLM efficiency, it inherits the ethical considerations\nof its backbone models, including issues related to\nbias and fairness. The method’s ability to maintain\ngeneration quality necessitates investigation to ensure\nthat the models do not perpetuate or amplify existing\nbiases."
    },
    {
      "page_no": 10,
      "bbox": [
        66.8969955444336,
        387.32720947265625,
        291.0945129394531,
        469.18408203125
      ],
      "text": "• Transparency and Accountability: The complexity\nof MEDUSA, particularly with its tree-based attention\nmechanism and multiple decoding heads, may pose\nchallenges in terms of model interpretability. Ensuring\ntransparency in how decisions are made and maintain-\ning accountability for those decisions are crucial for\nbuilding trust in AI systems."
    },
    {
      "page_no": 10,
      "bbox": [
        66.8969955444336,
        478.7586975097656,
        289.44134521484375,
        548.5460815429688
      ],
      "text": "• Security and Privacy: The accelerated capabilities of\nLLMs augmented by MEDUSA could potentially be\nexploited for malicious purposes, such as generating\ndisinformation at scale or automating cyber-attacks. It\nis imperative to develop and enforce ethical guidelines\nand security measures to prevent misuse."
    },
    {
      "page_no": 10,
      "bbox": [
        55.43999481201172,
        565.0932006835938,
        110.98384094238281,
        577.0484008789062
      ],
      "text": "References"
    },
    {
      "page_no": 10,
      "bbox": [
        55.439998626708984,
        584.5702514648438,
        291.09637451171875,
        630.4740600585938
      ],
      "text": "Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr´on, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023."
    },
    {
      "page_no": 10,
      "bbox": [
        55.439998626708984,
        639.5874633789062,
        290.6356201171875,
        662.0160522460938
      ],
      "text": "Axolotl.\nAxolotl.\nhttps://github.com/\nOpenAccess-AI-Collective/axolotl, 2023."
    },
    {
      "page_no": 10,
      "bbox": [
        55.439998626708984,
        671.7153930664062,
        291.1834716796875,
        717.44384765625
      ],
      "text": "Basu, S., Ramachandran, G. S., Keskar, N. S., and Varshney,\nL. R. {MIROSTAT}: A {neural} {text} {decoding}\n{algorithm} {that} {directly} {controls} {perplexity}.\nIn International Conference on Learning Representations,"
    },
    {
      "page_no": 10,
      "bbox": [
        317.4029846191406,
        68.95743560791016,
        542.037353515625,
        91.38607788085938
      ],
      "text": "2021. URL https://openreview.net/forum?\nid=W1G1JZEIy5_."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        103.63262176513672,
        543.1849975585938,
        161.34408569335938
      ],
      "text": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        173.44430541992188,
        542.827880859375,
        219.34707641601562
      ],
      "text": "Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,\nL., and Jumper, J. Accelerating large language model\ndecoding with speculative sampling. February 2023. doi:\n10.48550/ARXIV.2302.01318."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        231.44729614257812,
        543.093505859375,
        265.3960876464844
      ],
      "text": "Chen, L.\nDissecting batching effects in gpt infer-\nence.\nhttps://le.qun.ch/en/blog/2023/\n05/13/transformer-batching/, 2023. Blog."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        277.4963073730469,
        542.6869506835938,
        347.3090515136719
      ],
      "text": "Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        359.4882507324219,
        542.6813354492188,
        405.3130798339844
      ],
      "text": "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        417.552978515625,
        543.0947265625,
        451.361083984375
      ],
      "text": "Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-\nbit optimizers via block-wise quantization. International\nConference on Learning Representations, 2021."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        463.4613037109375,
        543.1849365234375,
        497.4090881347656
      ],
      "text": "Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm. int8 (): 8-bit matrix multiplication for transformers\nat scale. arXiv preprint arXiv:2208.07339, 2022."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        509.54681396484375,
        542.6848754882812,
        543.4580688476562
      ],
      "text": "Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\nL. Qlora: Efficient finetuning of quantized llms. arXiv\npreprint arXiv:2305.14314, 2023."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        555.6098022460938,
        543.09814453125,
        601.4610595703125
      ],
      "text": "Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu,\nZ., Sun, M., and Zhou, B. Enhancing chat language mod-\nels by scaling high-quality instructional conversations,\n2023."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        613.561279296875,
        543.0892944335938,
        659.4640502929688
      ],
      "text": "Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba,\nJ., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback, 2023."
    },
    {
      "page_no": 10,
      "bbox": [
        307.44000244140625,
        671.5642700195312,
        542.8242797851562,
        717.468017578125
      ],
      "text": "Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\nlinear units for neural network function approximation\nin reinforcement learning. Neural Networks, 2017. doi:\n10.1016/j.neunet.2017.12.012."
    },
    {
      "page_no": 10,
      "bbox": [
        293.08502197265625,
        732.4114379882812,
        303.047607421875,
        742.3740234375
      ],
      "text": "10"
    },
    {
      "page_no": 11,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        69.39230346679688,
        289.44439697265625,
        127.25106811523438
      ],
      "text": "Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural\nstory generation.\nIn Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). Association for Computational\nLinguistics, 2018. doi: 10.18653/v1/p18-1082."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        137.32838439941406,
        291.0944519042969,
        183.08108520507812
      ],
      "text": "Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers. arXiv preprint arXiv:2210.17323,\n2022."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        193.00729370117188,
        289.9247741699219,
        226.95510864257812
      ],
      "text": "Google.\nPalm 2 technical report,\n2023.\nURL\nhttps://ai.google/static/documents/\npalm2techreport.pdf."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        236.88229370117188,
        291.1791076660156,
        270.8300476074219
      ],
      "text": "Hewitt, J., Manning, C. D., and Liang, P. Truncation sam-\npling as language model desmoothing. October 2022.\ndoi: 10.48550/ARXIV.2210.15191."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        280.7712707519531,
        290.6858215332031,
        338.6150817871094
      ],
      "text": "Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        348.5412902832031,
        290.6868896484375,
        406.4000549316406
      ],
      "text": "Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi,\nY.\nThe curious case of neural text degeneration.\nIn\nInternational Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=rygGQyrFvH."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        416.35626220703125,
        290.6868591308594,
        450.2740783691406
      ],
      "text": "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., and Chen, W. Lora: Low-rank adaptation of large\nlanguage models. ICLR, 2021."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        460.2002868652344,
        291.1790771484375,
        506.10406494140625
      ],
      "text": "Joao Gante.\nAssisted generation:\na new direc-\ntion\ntoward\nlow-latency\ntext\ngeneration,\n2023.\nURL\nhttps://huggingface.co/blog/\nassisted-generation."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        516.0302734375,
        291.1859436035156,
        561.9340209960938
      ],
      "text": "Kim, S., Hooper, C., Gholami, A., Dong, Z., Li,\nX., Shen, S., Mahoney, M. W., and Keutzer, K.\nSqueezellm:\nDense-and-sparse quantization.\narXiv\npreprint arXiv:2306.07629, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        571.9278564453125,
        291.09747314453125,
        593.85302734375
      ],
      "text": "Kim, Y. and Rush, A. M. Sequence-level knowledge distil-\nlation. EMNLP, 2016."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        603.8318481445312,
        291.0904235839844,
        649.6830444335938
      ],
      "text": "Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang,\nP. Fine-tuning can distort pretrained features and under-\nperform out-of-distribution. International Conference on\nLearning Representations, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        55.439998626708984,
        659.6092529296875,
        291.1497497558594,
        717.4668579101562
      ],
      "text": "Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        69.39230346679688,
        541.6132202148438,
        103.34109497070312
      ],
      "text": "Leviathan, Y., Kalman, M., and Matias, Y. Fast inference\nfrom transformers via speculative decoding. November\n2022. doi: 10.48550/ARXIV.2211.17192."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        113.59628295898438,
        543.0977172851562,
        171.45407104492188
      ],
      "text": "Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        181.70932006835938,
        541.4442749023438,
        227.61306762695312
      ],
      "text": "Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and\nHan, S.\nAwq: Activation-aware weight quantization\nfor llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        237.86831665039062,
        543.1791381835938,
        271.8160705566406
      ],
      "text": "Meister, C., Wiher, G., Pimentel, T., and Cotterell, R. On\nthe probability-quality paradox in language generation.\nMarch 2022. doi: 10.48550/ARXIV.2203.17217."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        282.0712890625,
        543.0934448242188,
        316.02008056640625
      ],
      "text": "Meister, C., Pimentel, T., Wiher, G., and Cotterell, R. Lo-\ncally typical sampling. Transactions of the Association\nfor Computational Linguistics, 11:102–121, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        326.4264221191406,
        542.6879272460938,
        384.1340637207031
      ],
      "text": "Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong,\nR. Y. Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,\nZ. Specinfer: Accelerating generative llm serving with\nspeculative inference and token tree verification. arXiv\npreprint arXiv:2305.09781, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        394.4644470214844,
        464.30108642578125,
        404.42706298828125
      ],
      "text": "NVIDIA. Nvidia a100 tensor core gpu."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        414.7574462890625,
        460.9835205078125,
        424.7200622558594
      ],
      "text": "OpenAI. Gpt-4 technical report, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        434.97528076171875,
        543.097900390625,
        492.83306884765625
      ],
      "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., et al.\nTraining language models to fol-\nlow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        502.6534118652344,
        542.6349487304688,
        525.08203125
      ],
      "text": "Pan, J. Tiny vicuna 1b. https://huggingface.co/\nJiayi-Pan/Tiny-Vicuna-1B, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        535.3372802734375,
        544.4292602539062,
        617.1060180664062
      ],
      "text": "Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J.,\nWelleck, S., Choi, Y., and Harchaoui, Z. MAUVE: Mea-\nsuring the gap between neural text and human text using\ndivergence frontiers. In Beygelzimer, A., Dauphin, Y.,\nLiang, P., and Vaughan, J. W. (eds.), Advances in Neural\nInformation Processing Systems, 2021. URL https:\n//openreview.net/forum?id=Tqx7nJp7PR."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        627.3612670898438,
        543.1791381835938,
        673.2640380859375
      ],
      "text": "Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Brad-\nbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S.,\nand Dean, J. Efficiently scaling transformer inference.\nNovember 2022. doi: 10.48550/ARXIV.2211.05102."
    },
    {
      "page_no": 11,
      "bbox": [
        307.44000244140625,
        683.08447265625,
        545.6242065429688,
        717.4680786132812
      ],
      "text": "ShareGPT.\nShareGPT.\nhttps://huggingface.\nco/datasets/Aeala/ShareGPT_Vicuna_\nunfiltered, 2023."
    },
    {
      "page_no": 11,
      "bbox": [
        293.08502197265625,
        732.4114379882812,
        303.047607421875,
        742.3740234375
      ],
      "text": "11"
    },
    {
      "page_no": 12,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        69.40353393554688,
        289.4377136230469,
        91.38607788085938
      ],
      "text": "Shazeer, N. Fast transformer decoding: One write-head is\nall you need. arXiv preprint arXiv:1911.02150, 2019."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        102.39932250976562,
        289.4440002441406,
        136.34707641601562
      ],
      "text": "Spector, B. and Re, C.\nAccelerating llm inference\nwith staged speculative decoding.\narXiv preprint\narXiv:2308.04623, 2023."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        147.36032104492188,
        289.4431457519531,
        181.30807495117188
      ],
      "text": "Stern, M., Shazeer, N. M., and Uszkoreit, J. Blockwise\nparallel decoding for deep autoregressive models. Neural\nInformation Processing Systems, 2018."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        192.32131958007812,
        291.09783935546875,
        250.18008422851562
      ],
      "text": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        261.197021484375,
        290.68975830078125,
        319.05206298828125
      ],
      "text": "Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul,\nK., Belkada, Y., Huang, S., von Werra, L., Fourrier, C.,\nHabib, N., Sarrazin, N., Sanseviero, O., Rush, A. M.,\nand Wolf, T. Zephyr: Direct distillation of lm alignment,\n2023."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        330.2164306640625,
        290.6837463378906,
        375.9680480957031
      ],
      "text": "Xia, H., Ge, T., Chen, S.-Q., Wei, F., and Sui, Z. Speculative\ndecoding: Lossless speedup of autoregressive translation,\n2023. URL https://openreview.net/forum?\nid=H-VlwsYvVi."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        387.1324157714844,
        291.1796569824219,
        444.840087890625
      ],
      "text": "Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\nS. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International\nConference on Machine Learning, pp. 38087–38099.\nPMLR, 2023a."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        456.0044250488281,
        291.0929870605469,
        501.757080078125
      ],
      "text": "Xiao, Y., Wu, L., Guo, J., Li, J., Zhang, M., Qin, T., and Liu,\nT.-y. A survey on non-autoregressive generation for neu-\nral machine translation and beyond. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2023b."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        512.92138671875,
        290.6878967285156,
        558.6730346679688
      ],
      "text": "Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y.,\nand Liu, T.-Y. Do transformers really perform badly for\ngraph representation? Advances in Neural Information\nProcessing Systems, 34:28877–28888, 2021."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        569.686279296875,
        289.43707275390625,
        591.6790161132812
      ],
      "text": "Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama: An\nopen-source small language model, 2024."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        602.6922607421875,
        291.180419921875,
        648.5960083007812
      ],
      "text": "Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022."
    },
    {
      "page_no": 12,
      "bbox": [
        55.439998626708984,
        659.6092529296875,
        290.8249206542969,
        717.468017578125
      ],
      "text": "Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y., R´e, C., Barrett, C., et al. H 2 o:\nHeavy-hitter oracle for efficient generative inference of\nlarge language models. arXiv preprint arXiv:2306.14048,\n2023."
    },
    {
      "page_no": 12,
      "bbox": [
        307.44000244140625,
        69.39230346679688,
        542.6869506835938,
        115.29605102539062
      ],
      "text": "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\nwith mt-bench and chatbot arena, 2023."
    },
    {
      "page_no": 12,
      "bbox": [
        293.08502197265625,
        732.4114379882812,
        303.047607421875,
        742.3740234375
      ],
      "text": "12"
    },
    {
      "page_no": 13,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        67.84716033935547,
        141.20660400390625,
        79.80236053466797
      ],
      "text": "A. Related Work"
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        88.74950408935547,
        196.6399383544922,
        98.71210479736328
      ],
      "text": "A.1. LLM Inference Acceleration"
    },
    {
      "page_no": 13,
      "bbox": [
        55.11199951171875,
        107.53895568847656,
        541.7864379882812,
        201.14205932617188
      ],
      "text": "The inefficiency of Large Language Model (LLM) inference is primarily attributed to the memory-bandwidth-bound nature\nof the auto-regressive decoding process. Several methods have been proposed to alleviate this issue, improving inference\nlatency and throughput. Traditionally, batch inference has been employed as a straightforward method to enhance arithmetic\nintensity and escape memory-bandwidth-bound limitations. However, with LLMs, both model parameters and the Key-Value\n(KV) cache consume substantial accelerator memory, hindering the utilization of large batch sizes. Existing methods to\ntackle this problem can be conceptually divided into two main categories: (1) Reducing memory consumption, thereby\nminimizing memory transfer overhead and enabling larger batch sizes, and (2) Minimizing the number of decoding steps to\ndecrease latency directly."
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        215.17552185058594,
        542.6846313476562,
        284.93206787109375
      ],
      "text": "Reducing KV Cache.\nMethods such as Multi-query attention (Shazeer, 2019) and Grouped-query attention (Ainslie et al.,\n2023) adopt a direct approach to diminish the KV cache. By utilizing fewer key and value heads in the attention modules\nrelative to query heads, these strategies substantially cut the KV’s memory consumption, thereby facilitating larger batch\nsizes and enhanced accelerator utilization (Pope et al., 2022). Additionally, Zhang et al. (2023) proposes to selectively retain\nthe most critical KV tokens, further reducing the KV cache. From a system perspective, Kwon et al. (2023) introduces a\npaged memory management scheme for reducing fragmentation of the KV cache."
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        298.8848571777344,
        542.1083374023438,
        368.7220764160156
      ],
      "text": "Quantization.\nQuantization techniques are extensively used to shrink LLMs’ memory consumption. Xiao et al. (2023a)\napply rescaling between activations and parameters to eliminate outliers and simplify the quantization process. Dettmers\net al. (2022) breaks down matrix multiplications into predominantly 8-bit and a minority of 16-bit operations. Frantar\net al. (2022) iteratively round weight columns into 3/4 bits, while Lin et al. (2023) present an activation-aware quantization\nscheme to protect salient weights and compress LLMs to 3/4 bits. Kim et al. (2023) introduce a sparse plus low-precision\npattern to handle a minor portion of vital weights, among other techniques."
    },
    {
      "page_no": 13,
      "bbox": [
        55.082000732421875,
        382.747802734375,
        543.182373046875,
        584.0180053710938
      ],
      "text": "Speculative Decoding.\nAs an approach orthogonal to the aforementioned methods, speculative decoding (Leviathan et al.,\n2022; Chen et al., 2023) aims to execute several decoding steps in parallel, thus reducing the total number of steps required.\nThis parallelization is realized by employing a smaller draft model to conjecture several subsequent words, which the LLMs\nthen collectively evaluate and accept as appropriate. While resonating with non-autoregressive generation literature (Xiao\net al., 2023b), this method is specifically tailored for LLMs to address the aforementioned inefficiency. Unlike previous\nworks, we propose leveraging the original model to make predictions rather than introducing an additional draft model. This\napproach is more straightforward and seamlessly integrates into existing systems without the complexities of managing two\nmodels. Independently, Miao et al. (2023); Spector & Re (2023) propose the use of tree-structured attention to generate\nmultiple candidates in parallel, where Miao et al. (2023) suggest employing an ensemble of models to propose candidates,\nand Spector & Re (2023) advocate adding another hierarchy for the draft model. However, draft models require specialized\npretraining and alignment with the target models. While employing multiple draft models can be cumbersome and involves\nthe complexity of managing parallelism, our approach, which relies solely on decoding heads, offers a simpler alternative.\nMiao et al. (2023) employ multiple draft models to generate tokens and merge them using tree attention, while Spector &\nRe (2023) utilize a small draft model to process each level of the tree in batches. In contrast, our method directly uses the\ntop predicted tokens from each of MEDUSA heads to create a static sparse tree without autoregression or adjusting the tree\nstructure. This approach simplifies the process and improves efficiency. Additionally, we demonstrate through a detailed\nablation study how the nodes of the tree can affect decoding speed."
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        599.0985107421875,
        150.64259338378906,
        609.0610961914062
      ],
      "text": "A.2. Sampling Scheme"
    },
    {
      "page_no": 13,
      "bbox": [
        55.13100051879883,
        617.7887573242188,
        543.1785888671875,
        687.5800170898438
      ],
      "text": "The manner in which text is sampled from Large Language Models (LLMs) can significantly influence the quality of the\ngenerated output. Recent studies have revealed that direct sampling from a language model may lead to incoherent or\nnonsensical results (Pillutla et al., 2021; Holtzman et al., 2020). In response to this challenge, truncation sampling schemes\nhave been introduced (Fan et al., 2018; Basu et al., 2021; Meister et al., 2022; Hewitt et al., 2022; Meister et al., 2023).\nThese approaches aim to produce high-quality and diverse samples by performing sampling on a truncated distribution over\na specific allowed set at each decoding step."
    },
    {
      "page_no": 13,
      "bbox": [
        55.439998626708984,
        695.3199462890625,
        541.4391479492188,
        717.4511108398438
      ],
      "text": "Different strategies define this allowed set in various ways. For example, top-k sampling (Fan et al., 2018) retains the k\nmost likely words, whereas top-p sampling (Holtzman et al., 2020) incorporates the minimal set of words that account for p"
    },
    {
      "page_no": 13,
      "bbox": [
        293.0849609375,
        732.4114379882812,
        303.04754638671875,
        742.3740234375
      ],
      "text": "13"
    },
    {
      "page_no": 14,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        69.41477966308594,
        541.4411010742188,
        103.34109497070312
      ],
      "text": "percent of the probability. Another method, known as typical decoding (Meister et al., 2023), employs the entropy of the\npredicted distribution to establish the threshold for inclusion. Hewitt et al. (2022) offers a unified framework to understand\ntruncation sampling techniques comprehensively."
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        111.32183837890625,
        541.4417724609375,
        157.13906860351562
      ],
      "text": "Drawing inspiration from these methods, our typical acceptance scheme aligns with the concept of defining an allowed set\nto exclude improbable candidates from the sampling process. However, we diverge because we do not insist on an exact\ncorrespondence between the output and language model distribution. This deviation allows us to facilitate more diverse yet\nhigh-quality outputs, achieving greater efficiency without compromising the integrity of the generated text."
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        174.12115478515625,
        173.33023071289062,
        186.07635498046875
      ],
      "text": "B. Experiment Settings"
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        195.02349853515625,
        143.07102966308594,
        204.98609924316406
      ],
      "text": "B.1. Common Terms"
    },
    {
      "page_no": 14,
      "bbox": [
        54.97200012207031,
        213.69131469726562,
        541.6137084960938,
        271.5500793457031
      ],
      "text": "We clarify three commonly used terms: a) Acceleration rate: This refers to the average number of tokens decoded per\ndecoding step. In a standard auto-regressive model, this rate is 1.0. b) Overhead: This is used to characterize the per\ndecoding step overhead compared to classic decoding, and is calculated by dividing the average per step latency of the\nMEDUSA models by that of the vanilla model. c) Speedup: This refers to the wall-time acceleration rate. Following these\ndefinitions, we have the relation: Speedup = Acceleration rate / Overhead."
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        286.6295166015625,
        141.05856323242188,
        296.59210205078125
      ],
      "text": "B.2. Shared Settings"
    },
    {
      "page_no": 14,
      "bbox": [
        55.082000732421875,
        305.4255676269531,
        543.1832885742188,
        375.111083984375
      ],
      "text": "For all the experiments, we use the Axolotl (Axolotl, 2023) framework for training. We use a cosine learning rate scheduler\nwith warmup and use 8-bit AdamW (Dettmers et al., 2021) optimizer. We train 5 MEDUSA heads with 1 layer and set λk in\nEq. (1) to be 0.8k. For MEDUSA-2, we use either LoRA (Hu et al., 2021) or QLoRA (Dettmers et al., 2023) for fine-tuning\nand set the learning rate of MEDUSA heads to be 4 times larger than the backbone model. LoRA is applied to all the linear\nlayers of the backbone model, including the language model head. The rank of LoRA adapter is set to 32, and α is set to 16.\nA dropout of 0.05 is added to the LoRA adapter."
    },
    {
      "page_no": 14,
      "bbox": [
        55.43999481201172,
        390.1905212402344,
        285.9443359375,
        400.1531066894531
      ],
      "text": "B.3. MEDUSA-1 v.s. MEDUSA-2 on Vicuna 7B and 13B"
    },
    {
      "page_no": 14,
      "bbox": [
        54.97200012207031,
        405.2432861328125,
        542.1876220703125,
        454.7620544433594
      ],
      "text": "We use a global batch size of 64 and a peak learning rate of 5e−4 for the backbone and 2e−3 for MEDUSA heads and\nwarmup for 40 steps. We use 4-bit quantized backbone models for both models. We first train the models with MEDUSA-1\nand use these trained models as initialization to train MEDUSA-2. We employ QLoRA for MEDUSA-2 and the λ0 in Eq. (2)\nis set to be 0.2."
    },
    {
      "page_no": 14,
      "bbox": [
        55.44003677368164,
        469.8424987792969,
        336.6842346191406,
        479.8050842285156
      ],
      "text": "B.4. Training with Self-Distillation on Vicuna-33B and Zephyr-7B"
    },
    {
      "page_no": 14,
      "bbox": [
        54.97200012207031,
        488.35491943359375,
        541.44091796875,
        535.9080200195312
      ],
      "text": "We use MEDUSA-2 for both models instead of using a two-stage training procedure. We use a sine schedule for the θ0 to\ngradually increase the value to its peak at the end of the training. We find this approach is equally effective. We set the\npeak learning rate of the backbone LoRA adapter to be 1e−4 and the warmup steps to be 20 since the self-distillation loss is\nrelatively small. We set the λ0 in Eq. (2) to be 0.01."
    },
    {
      "page_no": 14,
      "bbox": [
        55.44001770019531,
        551.3952026367188,
        274.6027526855469,
        563.3504028320312
      ],
      "text": "C. Visualization of optimized tree attention"
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        572.46435546875,
        541.7936401367188,
        630.1720581054688
      ],
      "text": "Fig. 6 illustrates the structure of a sparsely constructed tree for the MEDUSA-2 Vicuna-7B model. This tree structure extends\nfour levels deep, indicating the engagement of four MEDUSA heads in the computation. The tree is initially formed through a\nCartesian product approach and subsequently refined by pruning based on the statistical expectations of the top-k predictions\nfrom each MEDUSA head measured on the Alpaca-eval dataset (Dubois et al., 2023). The tree’s lean towards the left visually\nrepresents the algorithm’s preference for nodes with higher probabilities on each head."
    },
    {
      "page_no": 14,
      "bbox": [
        55.439998626708984,
        647.1541748046875,
        231.95852661132812,
        659.109375
      ],
      "text": "D. Results of Speculative Decoding"
    },
    {
      "page_no": 14,
      "bbox": [
        54.69300079345703,
        668.1323852539062,
        542.6892700195312,
        713.9810791015625
      ],
      "text": "In this study, speculative decoding was applied to Vicuna models (Chiang et al., 2023) with varying sizes, specifically 7B,\n13B, and 33B. The preliminary framework utilized open-source models such as Llama-68M and 160M (Miao et al., 2023),\nalongside Tiny-Llama (Zhang et al., 2024) and Tiny-Vicuna (Pan, 2023), fine-tuned from Tiny-Llama with the Vicuna-style\ninstructional tuning strategy. Due to the proprietary nature of speculative decoding methods (Chen et al., 2023; Leviathan"
    },
    {
      "page_no": 14,
      "bbox": [
        293.0849914550781,
        732.4114379882812,
        303.0475769042969,
        742.3740234375
      ],
      "text": "14"
    },
    {
      "page_no": 15,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 15,
      "bbox": [
        54.9379997253418,
        226.91331481933594,
        541.4977416992188,
        257.8653564453125
      ],
      "text": "Figure 6. Visualization of a sparse tree setting for MEDUSA-2 Vicuna-7B. The tree has 64 nodes representing candidate tokens and a\ndepth of 4 which indicates 4 MEDUSA heads involved in calculation. Each node indicates a token from a top-k prediction of a MEDUSA\nhead, and the edges show the connections between them. The red lines highlight the path that correctly predicts the future tokens."
    },
    {
      "page_no": 15,
      "bbox": [
        55.439998626708984,
        286.706298828125,
        541.43994140625,
        312.3140869140625
      ],
      "text": "et al., 2022), open-source alternatives3 were deployed for evaluation. Additionally, we utilize torch.compile() to\naccelerate the inference speed of draft models."
    },
    {
      "page_no": 15,
      "bbox": [
        55.439998626708984,
        320.36041259765625,
        542.6898193359375,
        378.0680847167969
      ],
      "text": "Our results shown in Fig. 7, reveal that the optimal settings of the draft model vary with the Vicuna model sizes. Specifically,\nthe Llama-68M, with a setting of the draft token number γ = 4, yielded the best performance for Vicuna-7B, while the same\ndraft model with γ = 3 was most effective for Vicuna-13B. For the larger Vicuna-33B, the Tiny-Vicuna (Vicuna-1B), with\nγ = 3, provided the greatest acceleration. These results suggest that the choice and setting of the drafting model should be\ntailored to the size of the LLMs, presenting an area for further exploration in the field."
    },
    {
      "page_no": 15,
      "bbox": [
        77.43876647949219,
        512.684814453125,
        205.8521728515625,
        525.6988525390625
      ],
      "text": "0\n2\n4\n6\n8\n10\n12\n14\nGamma"
    },
    {
      "page_no": 15,
      "bbox": [
        69.96045684814453,
        502.1698303222656,
        75.4806900024414,
        509.5301513671875
      ],
      "text": "20"
    },
    {
      "page_no": 15,
      "bbox": [
        69.96045684814453,
        481.2329406738281,
        75.4806900024414,
        488.59326171875
      ],
      "text": "30"
    },
    {
      "page_no": 15,
      "bbox": [
        69.96045684814453,
        460.2960510253906,
        75.4806900024414,
        467.6563720703125
      ],
      "text": "40"
    },
    {
      "page_no": 15,
      "bbox": [
        69.96045684814453,
        439.3591613769531,
        75.4806900024414,
        446.719482421875
      ],
      "text": "50"
    },
    {
      "page_no": 15,
      "bbox": [
        69.96045684814453,
        418.4222717285156,
        75.4806900024414,
        425.7825927734375
      ],
      "text": "60"
    },
    {
      "page_no": 15,
      "bbox": [
        62.16193771362305,
        434.60247802734375,
        70.19137573242188,
        478.844482421875
      ],
      "text": "Tokens per Second"
    },
    {
      "page_no": 15,
      "bbox": [
        185.70372009277344,
        403.898681640625,
        212.46731567382812,
        430.6563720703125
      ],
      "text": "Llama-68M\nLlama-160M\nLlama-1B\nVicuna-1B"
    },
    {
      "page_no": 15,
      "bbox": [
        114.77799987792969,
        532.407958984375,
        165.7788848876953,
        541.3743896484375
      ],
      "text": "(a) Vicuna-7B"
    },
    {
      "page_no": 15,
      "bbox": [
        235.45376586914062,
        512.684814453125,
        363.8671875,
        525.6988525390625
      ],
      "text": "0\n2\n4\n6\n8\n10\n12\n14\nGamma"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        498.92828369140625,
        233.49569702148438,
        506.2886047363281
      ],
      "text": "20"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        484.8664855957031,
        233.49569702148438,
        492.226806640625
      ],
      "text": "25"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        470.8046569824219,
        233.49569702148438,
        478.16497802734375
      ],
      "text": "30"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        456.74285888671875,
        233.49569702148438,
        464.1031799316406
      ],
      "text": "35"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        442.6810607910156,
        233.49569702148438,
        450.0413818359375
      ],
      "text": "40"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        428.6192626953125,
        233.49569702148438,
        435.9795837402344
      ],
      "text": "45"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        414.55743408203125,
        233.49569702148438,
        421.9177551269531
      ],
      "text": "50"
    },
    {
      "page_no": 15,
      "bbox": [
        227.9754638671875,
        400.4956359863281,
        233.49569702148438,
        407.85595703125
      ],
      "text": "55"
    },
    {
      "page_no": 15,
      "bbox": [
        220.17694091796875,
        434.60247802734375,
        228.2063751220703,
        478.844482421875
      ],
      "text": "Tokens per Second"
    },
    {
      "page_no": 15,
      "bbox": [
        343.7187194824219,
        403.898681640625,
        370.4822998046875,
        430.6563720703125
      ],
      "text": "Llama-68M\nLlama-160M\nLlama-1B\nVicuna-1B"
    },
    {
      "page_no": 15,
      "bbox": [
        270.29998779296875,
        532.407958984375,
        326.28619384765625,
        541.3743896484375
      ],
      "text": "(b) Vicuna-13B"
    },
    {
      "page_no": 15,
      "bbox": [
        393.4677734375,
        512.684814453125,
        521.8812255859375,
        525.6988525390625
      ],
      "text": "0\n2\n4\n6\n8\n10\n12\n14\nGamma"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        498.3253479003906,
        391.50970458984375,
        505.6856689453125
      ],
      "text": "16"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        482.9287414550781,
        391.50970458984375,
        490.2890625
      ],
      "text": "18"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        467.5321044921875,
        391.50970458984375,
        474.8924255371094
      ],
      "text": "20"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        452.135498046875,
        391.50970458984375,
        459.4958190917969
      ],
      "text": "22"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        436.7388610839844,
        391.50970458984375,
        444.09918212890625
      ],
      "text": "24"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        421.3422546386719,
        391.50970458984375,
        428.70257568359375
      ],
      "text": "26"
    },
    {
      "page_no": 15,
      "bbox": [
        385.98944091796875,
        405.9456481933594,
        391.50970458984375,
        413.30596923828125
      ],
      "text": "28"
    },
    {
      "page_no": 15,
      "bbox": [
        378.1909484863281,
        434.60247802734375,
        386.2203674316406,
        478.844482421875
      ],
      "text": "Tokens per Second"
    },
    {
      "page_no": 15,
      "bbox": [
        501.73272705078125,
        403.898681640625,
        528.496337890625,
        430.6563720703125
      ],
      "text": "Llama-68M\nLlama-160M\nLlama-1B\nVicuna-1B"
    },
    {
      "page_no": 15,
      "bbox": [
        428.56500244140625,
        532.407958984375,
        484.049072265625,
        541.3743896484375
      ],
      "text": "(c) Vicuna-33B"
    },
    {
      "page_no": 15,
      "bbox": [
        54.93798828125,
        556.2513427734375,
        541.7586059570312,
        576.243408203125
      ],
      "text": "Figure 7. Inference speed of various models using speculative decoding on MT-Bench. Baseline model speeds are presented by grey\ndotted lines for comparison. γ denotes the draft token number."
    },
    {
      "page_no": 15,
      "bbox": [
        55.43999481201172,
        609.6521606445312,
        240.4586944580078,
        621.6073608398438
      ],
      "text": "E. Additional Results for All Models"
    },
    {
      "page_no": 15,
      "bbox": [
        54.97199630737305,
        630.6454467773438,
        240.18663024902344,
        640.6080322265625
      ],
      "text": "We show speedup on various models in Fig. 8."
    },
    {
      "page_no": 15,
      "bbox": [
        55.43999481201172,
        657.5901489257812,
        284.48968505859375,
        669.5453491210938
      ],
      "text": "F. Additional Results on AlpacalEval Dataset"
    },
    {
      "page_no": 15,
      "bbox": [
        54.97200012207031,
        678.6517333984375,
        541.6085815429688,
        700.5010375976562
      ],
      "text": "We conduct further experiments on the AlpacaEval (Li et al., 2023) dataset. MEDUSA-2 achieves consistent speedup similar\nto the results on MT-Bench."
    },
    {
      "page_no": 15,
      "bbox": [
        68.09300231933594,
        706.7186889648438,
        270.85821533203125,
        717.2273559570312
      ],
      "text": "3https://github.com/feifeibear/LLMSpeculativeSampling"
    },
    {
      "page_no": 15,
      "bbox": [
        293.08502197265625,
        732.4114379882812,
        303.047607421875,
        742.3740234375
      ],
      "text": "15"
    },
    {
      "page_no": 16,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 16,
      "bbox": [
        223.996337890625,
        190.45323181152344,
        399.78875732421875,
        210.6914825439453
      ],
      "text": "Vicuna-7B\nZephyr-7B\nVicuna-13B\nVicuna-33B\nModel Size"
    },
    {
      "page_no": 16,
      "bbox": [
        208.7238006591797,
        184.65785217285156,
        213.0343475341797,
        196.1526641845703
      ],
      "text": "0"
    },
    {
      "page_no": 16,
      "bbox": [
        204.41368103027344,
        168.03125,
        213.03477478027344,
        179.52606201171875
      ],
      "text": "20"
    },
    {
      "page_no": 16,
      "bbox": [
        204.41368103027344,
        151.40464782714844,
        213.03477478027344,
        162.8994598388672
      ],
      "text": "40"
    },
    {
      "page_no": 16,
      "bbox": [
        204.41368103027344,
        134.77804565429688,
        213.03477478027344,
        146.27285766601562
      ],
      "text": "60"
    },
    {
      "page_no": 16,
      "bbox": [
        204.41368103027344,
        118.15145111083984,
        213.03477478027344,
        129.64625549316406
      ],
      "text": "80"
    },
    {
      "page_no": 16,
      "bbox": [
        200.10354614257812,
        101.52485656738281,
        213.0352020263672,
        113.01966857910156
      ],
      "text": "100"
    },
    {
      "page_no": 16,
      "bbox": [
        200.10354614257812,
        84.89825439453125,
        213.0352020263672,
        96.39306640625
      ],
      "text": "120"
    },
    {
      "page_no": 16,
      "bbox": [
        188.00413513183594,
        99.24345397949219,
        200.64842224121094,
        168.91189575195312
      ],
      "text": "Tokens per Second"
    },
    {
      "page_no": 16,
      "bbox": [
        243.04129028320312,
        76.40469360351562,
        254.50086975097656,
        83.30158233642578
      ],
      "text": "2.83x"
    },
    {
      "page_no": 16,
      "bbox": [
        289.5019836425781,
        93.29521179199219,
        300.9615478515625,
        100.19210052490234
      ],
      "text": "2.66x"
    },
    {
      "page_no": 16,
      "bbox": [
        335.96270751953125,
        102.52019500732422,
        347.4222717285156,
        109.41708374023438
      ],
      "text": "2.83x"
    },
    {
      "page_no": 16,
      "bbox": [
        382.42340087890625,
        149.24234008789062,
        393.8829650878906,
        156.13922119140625
      ],
      "text": "2.35x"
    },
    {
      "page_no": 16,
      "bbox": [
        242.65151977539062,
        65.65214538574219,
        378.9505615234375,
        79.44591522216797
      ],
      "text": "Speedup on different model sizes"
    },
    {
      "page_no": 16,
      "bbox": [
        363.4765319824219,
        80.31543731689453,
        399.8681640625,
        99.61461639404297
      ],
      "text": "w/o Medusa\nMedusa-2"
    },
    {
      "page_no": 16,
      "bbox": [
        54.9379997253418,
        225.2853240966797,
        541.7508544921875,
        256.23736572265625
      ],
      "text": "Figure 8. Speedup of various models with MEDUSA-2. MEDUSA-2 shows significant speed improvement over all the models, while\nmodels trained with self-distillation (Zephyr-7B, Vicuna-13/33B) have weaker speedup due to the trade-off between preserving quality\nand boosting speed."
    },
    {
      "page_no": 16,
      "bbox": [
        126.12799835205078,
        275.9254455566406,
        482.70849609375,
        285.8880615234375
      ],
      "text": "Model\nBase speed (tokens/s)\nMEDUSA speed (tokens/s)\nAcc. rate\nSpeedup"
    },
    {
      "page_no": 16,
      "bbox": [
        126.12799835205078,
        292.8824462890625,
        482.7093200683594,
        338.7110290527344
      ],
      "text": "Vicuna-7b\n37.07\n106.76\n3.23\n2.88\nVicuna-13b\n29.01\n91.54\n3.28\n3.16\nVicuna-33b\n17.87\n40.43\n2.85\n2.26\nZephyr-7b\n34.21\n99.50\n3.08\n2.91"
    },
    {
      "page_no": 16,
      "bbox": [
        182.94400024414062,
        352.6769714355469,
        413.9393615722656,
        361.64337158203125
      ],
      "text": "Table 4. Speedup results on AlpacaEval (Li et al., 2023) dataset."
    },
    {
      "page_no": 16,
      "bbox": [
        55.44000244140625,
        385.6901550292969,
        408.5430603027344,
        397.6453552246094
      ],
      "text": "G. Exploration and Modeling of Hardware Constraints and MEDUSA"
    },
    {
      "page_no": 16,
      "bbox": [
        54.97200012207031,
        424.57501220703125,
        543.0973510742188,
        518.2650756835938
      ],
      "text": "We explore the hardware constraints, specifically memory-bandwidth bound, and their impact on MEDUSA-style parallel\ndecoding by incorporating a simplified Llama-series model. First, we identify that the operators involving matrix multi-\nplications, such as linear layers and attention matrix multiplications, are the primary sources of overhead. We profile the\nperformance of FLOP/s vs. Operational Intensity which is the ratio of FLOP/s to bandwidth (bytes/s), across various GPUs,\nincluding the A100-80GB-PCIe, A40, and A6000. Next, we examine the changes in FLOP/s vs. Operational Intensity when\nusing MEDUSA for different operators. Finally, we apply a straightforward analytical model to calculate acceleration rates\nand combine it with hardware benchmarks. This provides insights into the effects under different model sizes, sequence\nlengths, and batch sizes."
    },
    {
      "page_no": 16,
      "bbox": [
        55.439998626708984,
        533.344482421875,
        197.67604064941406,
        543.3070678710938
      ],
      "text": "G.1. Roofline Model of Operators"
    },
    {
      "page_no": 16,
      "bbox": [
        54.97200012207031,
        552.0808715820312,
        542.6831665039062,
        609.8710327148438
      ],
      "text": "We present an analysis of the roofline model for various operators in large language models (LLMs), specifically focusing\non Llama-7B, Llama-13B, and Llama-33B (Touvron et al., 2023). These models were benchmarked on different GPUs,\nincluding the A100-80GB-PCIe, A40, and A6000. We looked into the three categories of matrix multiplication operators\nsince they represent the primary sources of computational overhead in these models. Our study follows the report (Chen,\n2023) which investigates the effectiveness of batch size but ours focuses more on decoding and parallel decoding."
    },
    {
      "page_no": 16,
      "bbox": [
        55.13100051879883,
        617.9097290039062,
        541.4425048828125,
        687.5800170898438
      ],
      "text": "Table 5 details the computation and space complexity for each operator during the prefill, decoding, and MEDUSA decoding\nphases. The operators include the linear layers for query, key, and value matrices (XWQ, XWK, XWV ), the attention\nmatrix multiplications (QKT , PV ), and the up/gate/down linear layers (XWu, XWg, XWd). b stands for the batch size, s\nstands for the sequence length, h stands for the hidden dimension, i stands for the intermediate dimension, n stands for the\nnumber of attention heads, d stands for the head dimension and q stands for the candidate length for MEDUSA. For more\ndetails of these operators please refer to the articles (Touvron et al., 2023; Chen, 2023)."
    },
    {
      "page_no": 16,
      "bbox": [
        55.439998626708984,
        695.614990234375,
        541.441162109375,
        717.4583740234375
      ],
      "text": "Figures 9-17 show the benchmark of three categories of operators on different models (7/13/33B) under various settings. To\nevaluate each operator’s performance and throughput, we chose the combination of settings including batch sizes from 1 to"
    },
    {
      "page_no": 16,
      "bbox": [
        293.0849914550781,
        732.4114379882812,
        303.0475769042969,
        742.3740234375
      ],
      "text": "16"
    },
    {
      "page_no": 17,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 17,
      "bbox": [
        54.893001556396484,
        76.33798217773438,
        542.5601196289062,
        85.30438232421875
      ],
      "text": "Table 5. Computational and space complexity of the main operators in different phases. The table is based on Table 2 in the report (Chen,"
    },
    {
      "page_no": 17,
      "bbox": [
        55.439998626708984,
        87.29696655273438,
        78.60021209716797,
        96.26336669921875
      ],
      "text": "2023)."
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        100.49647521972656,
        467.6906433105469,
        107.47026824951172
      ],
      "text": "Operator\nInput Shape\nOutput Shape\nComp. Complexity\nSpace Complexity"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        113.46846008300781,
        135.0890655517578,
        120.44225311279297
      ],
      "text": "Prefill"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        125.5320053100586,
        464.20526123046875,
        134.52130126953125
      ],
      "text": "XWQ, XWK, XWV\n(b, s, h)\n(b, s, h)\nO(bsh2)\nO(2bsh + h2)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.0059814453125,
        139.33493041992188,
        470.1712951660156,
        155.728515625
      ],
      "text": "QKT\n(b, n, s, d), (b, n, s, d)\n(b, n, s, s)\nO(bs2nd)\nO(2bsnd + bs2n)\nP V\n(b, n, s, s), (b, n, s, d)\n(b, n, s, d)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.0059814453125,
        161.7267608642578,
        471.0793151855469,
        177.292236328125
      ],
      "text": "XWu, XWg\n(b, s, h)\n(b, s, i)\nO(bshi)\nO(bs(h + i) + hi)\nXWd\n(b, s, i)\n(b, s, h)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        182.76649475097656,
        144.90118408203125,
        189.74029541015625
      ],
      "text": "Decoding"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        194.82894897460938,
        462.32427978515625,
        203.81927490234375
      ],
      "text": "XWQ, XWK, XWV\n(b, 1, h)\n(b, 1, h)\nO(bh2)\nO(2bh + h2)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00595092773438,
        208.63296508789062,
        477.4012756347656,
        225.0255126953125
      ],
      "text": "QKT\n(b, n, 1, d), (b, n, s, d)\n(b, n, s, 1)\nO(bsnd)\nO(bsn + bsnd + bnd)\nP V\n(b, n, s, 1), (b, n, 1, d)\n(b, n, 1, d)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.0059814453125,
        231.0247344970703,
        469.1993103027344,
        246.5902099609375
      ],
      "text": "XWu, XWg\n(b, 1, h)\n(b, 1, i)\nO(bhi)\nO(b(h + i) + hi)\nXWd\n(b, 1, i)\n(b, 1, h)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        252.06446838378906,
        168.6609344482422,
        259.03826904296875
      ],
      "text": "Parallel decoding"
    },
    {
      "page_no": 17,
      "bbox": [
        117.00599670410156,
        264.1269226074219,
        464.2752685546875,
        273.1162109375
      ],
      "text": "XWQ, XWK, XWV\n(b, q, h)\n(b, q, h)\nO(bqh2)\nO(2bqh + h2)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.0059814453125,
        277.9309387207031,
        478.1282958984375,
        294.32354736328125
      ],
      "text": "QKT\n(b, n, q, d), (b, n, s, d)\n(b, n, s, q)\nO(bsqnd)\nO(bsqn + b(s + q)nd)\nP V\n(b, n, s, q), (b, n, q, d)\n(b, n, q, d)"
    },
    {
      "page_no": 17,
      "bbox": [
        117.0059814453125,
        300.3217468261719,
        471.1502990722656,
        315.8882751464844
      ],
      "text": "XWu, XWg\n(b, q, h)\n(b, q, i)\nO(bqhi)\nO(bq(h + i) + hi)\nXWd\n(b, q, i)\n(b, q, h)"
    },
    {
      "page_no": 17,
      "bbox": [
        55.082000732421875,
        341.0544128417969,
        542.6895751953125,
        374.8520812988281
      ],
      "text": "64 in powers of 2 and sequence lengths from 128 to 8192 in powers of 2 (49 settings for each operator). From all the figures,\nwe observe that the datapoints of each operator in the prefill and decoding stages cluster at very similar positions across all\nGPUs and for various model sizes."
    },
    {
      "page_no": 17,
      "bbox": [
        55.439998626708984,
        382.3114318847656,
        543.178955078125,
        452.5600891113281
      ],
      "text": "During the prefill phase, increasing the batch size changes the FLOP/s of the attention matrix multiplications (see ‘qk/pv\ninit‘) but does not affect the Operational Intensity (refer to the vertical dashed arrow in Fig. 9). In contrast, increasing\nthe sequence length impacts both FLOP/s and Operational Intensity in the prefill phase (refer to the diagonal dashed arrow\nin Fig. 9). During the decoding phase, the attention matrix multiplications are significantly limited by memory bandwidth.\nDespite an increase in FLOP/s with changes in batch size and sequence length, the Operational Intensity remains nearly\nunchanged (see ‘qk/pv ar‘). This indicates suboptimal resource utilization in the self-attention mechanism."
    },
    {
      "page_no": 17,
      "bbox": [
        55.13100051879883,
        460.0204162597656,
        543.18408203125,
        530.26904296875
      ],
      "text": "The linear layers in the prefill phase are mostly compute-bound (see ‘qkv mlp init‘ and ‘up/gate/down init‘).\nDuring the decoding phase, the datapoints of the linear layer form a line with the same slope as the GPU’s memory\nbandwidth (see ‘qkv mlp ar‘ and ‘up/gate/down ar‘). This indicates the linear layers in the decoding stage are\nalso bounded by memory bandwidth. Increasing the batch size improves the achieved FLOP/s and Operational Intensity\nunder memory bandwidth constraints through better parallelism. Note that linear layers only process the new token and are\nindependent of sequence length (See ‘Decoding‘ section in Table 5)."
    },
    {
      "page_no": 17,
      "bbox": [
        293.0849914550781,
        732.4114379882812,
        303.0475769042969,
        742.3740234375
      ],
      "text": "17"
    },
    {
      "page_no": 18,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 18,
      "bbox": [
        250.10552978515625,
        202.52064514160156,
        287.36962890625,
        220.62342834472656
      ],
      "text": "qk/pv init\nIncrease bs"
    },
    {
      "page_no": 18,
      "bbox": [
        301.0239562988281,
        183.00076293945312,
        354.3753662109375,
        217.38645935058594
      ],
      "text": "qk/pv init\nIncrease seq_len"
    },
    {
      "page_no": 18,
      "bbox": [
        54.9379997253418,
        335.2923278808594,
        542.9290161132812,
        388.161376953125
      ],
      "text": "Figure 9. The figure shows the relationship between FLOP/s and Operational Intensity for all benchmarked datapoints of Llama-7B\noperators on A100-80GB-PCIe. The dashed lines represent the HBM bandwidth limit (1,935GB/s) and the peak performance limit (312\nTFLOP/s) (NVIDIA). ‘qkv mlp’ stands for the linear layers projecting hidden features to query/key/value features. ‘up/gate/down’\nstands for the linear layers following the attention block. ‘qk/pv’ stands for the two steps of attention matrix multiplications. ‘ar’ stands\nfor the decoding (autoregressive) and ‘init’ stands for the prefill phase."
    },
    {
      "page_no": 18,
      "bbox": [
        166.07015991210938,
        645.7382202148438,
        460.96124267578125,
        668.0357055664062
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 18,
      "bbox": [
        133.38616943359375,
        628.884521484375,
        148.86178588867188,
        641.70654296875
      ],
      "text": "10G"
    },
    {
      "page_no": 18,
      "bbox": [
        128.5767059326172,
        594.9988403320312,
        148.86056518554688,
        607.8208618164062
      ],
      "text": "100G"
    },
    {
      "page_no": 18,
      "bbox": [
        139.43597412109375,
        561.1131591796875,
        148.86346435546875,
        573.9351806640625
      ],
      "text": "1T"
    },
    {
      "page_no": 18,
      "bbox": [
        134.62649536132812,
        527.2275390625,
        148.86224365234375,
        540.049560546875
      ],
      "text": "10T"
    },
    {
      "page_no": 18,
      "bbox": [
        129.81704711914062,
        493.34185791015625,
        148.8610382080078,
        506.1638488769531
      ],
      "text": "100T"
    },
    {
      "page_no": 18,
      "bbox": [
        115.52560424804688,
        514.9677124023438,
        128.3476104736328,
        596.4647216796875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 18,
      "bbox": [
        201.220703125,
        451.478759765625,
        405.9737548828125,
        466.8651428222656
      ],
      "text": "Roofline Model (Llama 13B, A100 80GB PCIe)"
    },
    {
      "page_no": 18,
      "bbox": [
        380.6651916503906,
        550.169921875,
        447.1717224121094,
        640.6605834960938
      ],
      "text": "1,935GB/s\n312 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 18,
      "bbox": [
        199.7729949951172,
        685.3169555664062,
        397.10931396484375,
        694.2833862304688
      ],
      "text": "Figure 10. Llama-13B operators on A100-80GB-PCIe."
    },
    {
      "page_no": 18,
      "bbox": [
        293.0849914550781,
        732.4114379882812,
        303.0475769042969,
        742.3740234375
      ],
      "text": "18"
    },
    {
      "page_no": 19,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 19,
      "bbox": [
        166.07015991210938,
        306.8211975097656,
        460.96124267578125,
        329.1186828613281
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 19,
      "bbox": [
        133.38616943359375,
        289.9674987792969,
        148.86178588867188,
        302.78948974609375
      ],
      "text": "10G"
    },
    {
      "page_no": 19,
      "bbox": [
        128.5767059326172,
        256.08184814453125,
        148.86056518554688,
        268.9038391113281
      ],
      "text": "100G"
    },
    {
      "page_no": 19,
      "bbox": [
        139.43597412109375,
        222.19618225097656,
        148.86346435546875,
        235.0181884765625
      ],
      "text": "1T"
    },
    {
      "page_no": 19,
      "bbox": [
        134.62649536132812,
        188.31051635742188,
        148.86224365234375,
        201.1325225830078
      ],
      "text": "10T"
    },
    {
      "page_no": 19,
      "bbox": [
        129.81704711914062,
        154.4248504638672,
        148.8610382080078,
        167.24685668945312
      ],
      "text": "100T"
    },
    {
      "page_no": 19,
      "bbox": [
        115.52560424804688,
        176.05068969726562,
        128.3476104736328,
        257.5477294921875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 19,
      "bbox": [
        201.220703125,
        112.56177520751953,
        405.9737548828125,
        127.94817352294922
      ],
      "text": "Roofline Model (Llama 33B, A100 80GB PCIe)"
    },
    {
      "page_no": 19,
      "bbox": [
        380.6651916503906,
        211.2529296875,
        447.1717224121094,
        301.7435607910156
      ],
      "text": "1,935GB/s\n312 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 19,
      "bbox": [
        199.7729949951172,
        346.3999938964844,
        397.10931396484375,
        355.36639404296875
      ],
      "text": "Figure 11. Llama-33B operators on A100-80GB-PCIe."
    },
    {
      "page_no": 19,
      "bbox": [
        166.07015991210938,
        634.8062133789062,
        460.96124267578125,
        657.1036987304688
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 19,
      "bbox": [
        133.38616943359375,
        617.9525146484375,
        148.86178588867188,
        630.7745361328125
      ],
      "text": "10G"
    },
    {
      "page_no": 19,
      "bbox": [
        128.5767059326172,
        584.0668334960938,
        148.86056518554688,
        596.8888549804688
      ],
      "text": "100G"
    },
    {
      "page_no": 19,
      "bbox": [
        139.43597412109375,
        550.18115234375,
        148.86346435546875,
        563.003173828125
      ],
      "text": "1T"
    },
    {
      "page_no": 19,
      "bbox": [
        134.62649536132812,
        516.2955322265625,
        148.86224365234375,
        529.1175537109375
      ],
      "text": "10T"
    },
    {
      "page_no": 19,
      "bbox": [
        129.81704711914062,
        482.40985107421875,
        148.8610382080078,
        495.2318420410156
      ],
      "text": "100T"
    },
    {
      "page_no": 19,
      "bbox": [
        115.52560424804688,
        504.0356750488281,
        128.3476104736328,
        585.53271484375
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 19,
      "bbox": [
        232.22064208984375,
        440.5467529296875,
        374.8111572265625,
        455.9331359863281
      ],
      "text": "Roofline Model (Llama 7B, A40)"
    },
    {
      "page_no": 19,
      "bbox": [
        380.6651916503906,
        539.2379150390625,
        447.1717224121094,
        629.7285766601562
      ],
      "text": "696GB/s\n149.7 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 19,
      "bbox": [
        226.91900634765625,
        674.3849487304688,
        369.9627380371094,
        683.3513793945312
      ],
      "text": "Figure 12. Llama-7B operators on A40."
    },
    {
      "page_no": 19,
      "bbox": [
        293.0849914550781,
        732.4114379882812,
        303.0475769042969,
        742.3740234375
      ],
      "text": "19"
    },
    {
      "page_no": 20,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 20,
      "bbox": [
        166.07015991210938,
        306.8211975097656,
        460.96124267578125,
        329.1186828613281
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 20,
      "bbox": [
        133.38616943359375,
        289.9674987792969,
        148.86178588867188,
        302.78948974609375
      ],
      "text": "10G"
    },
    {
      "page_no": 20,
      "bbox": [
        128.5767059326172,
        256.08184814453125,
        148.86056518554688,
        268.9038391113281
      ],
      "text": "100G"
    },
    {
      "page_no": 20,
      "bbox": [
        139.43597412109375,
        222.19618225097656,
        148.86346435546875,
        235.0181884765625
      ],
      "text": "1T"
    },
    {
      "page_no": 20,
      "bbox": [
        134.62649536132812,
        188.31051635742188,
        148.86224365234375,
        201.1325225830078
      ],
      "text": "10T"
    },
    {
      "page_no": 20,
      "bbox": [
        129.81704711914062,
        154.4248504638672,
        148.8610382080078,
        167.24685668945312
      ],
      "text": "100T"
    },
    {
      "page_no": 20,
      "bbox": [
        115.52560424804688,
        176.05068969726562,
        128.3476104736328,
        257.5477294921875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 20,
      "bbox": [
        229.34341430664062,
        112.56177520751953,
        377.703857421875,
        127.94817352294922
      ],
      "text": "Roofline Model (Llama 13B, A40)"
    },
    {
      "page_no": 20,
      "bbox": [
        380.6651916503906,
        211.2529296875,
        447.1717224121094,
        301.7435607910156
      ],
      "text": "696GB/s\n149.7 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 20,
      "bbox": [
        224.677001953125,
        346.3999938964844,
        372.2049255371094,
        355.36639404296875
      ],
      "text": "Figure 13. Llama-13B operators on A40."
    },
    {
      "page_no": 20,
      "bbox": [
        166.07015991210938,
        634.8062133789062,
        460.96124267578125,
        657.1036987304688
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 20,
      "bbox": [
        133.38616943359375,
        617.9525146484375,
        148.86178588867188,
        630.7745361328125
      ],
      "text": "10G"
    },
    {
      "page_no": 20,
      "bbox": [
        128.5767059326172,
        584.0668334960938,
        148.86056518554688,
        596.8888549804688
      ],
      "text": "100G"
    },
    {
      "page_no": 20,
      "bbox": [
        139.43597412109375,
        550.18115234375,
        148.86346435546875,
        563.003173828125
      ],
      "text": "1T"
    },
    {
      "page_no": 20,
      "bbox": [
        134.62649536132812,
        516.2955322265625,
        148.86224365234375,
        529.1175537109375
      ],
      "text": "10T"
    },
    {
      "page_no": 20,
      "bbox": [
        129.81704711914062,
        482.40985107421875,
        148.8610382080078,
        495.2318420410156
      ],
      "text": "100T"
    },
    {
      "page_no": 20,
      "bbox": [
        115.52560424804688,
        504.0356750488281,
        128.3476104736328,
        585.53271484375
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 20,
      "bbox": [
        229.34341430664062,
        440.5467529296875,
        377.703857421875,
        455.9331359863281
      ],
      "text": "Roofline Model (Llama 33B, A40)"
    },
    {
      "page_no": 20,
      "bbox": [
        380.6651916503906,
        539.2379150390625,
        447.1717224121094,
        629.7285766601562
      ],
      "text": "696GB/s\n149.7 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 20,
      "bbox": [
        224.677001953125,
        674.3849487304688,
        372.2049255371094,
        683.3513793945312
      ],
      "text": "Figure 14. Llama-33B operators on A40."
    },
    {
      "page_no": 20,
      "bbox": [
        293.4590148925781,
        732.4114379882812,
        303.4216003417969,
        742.3740234375
      ],
      "text": "20"
    },
    {
      "page_no": 21,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 21,
      "bbox": [
        166.07015991210938,
        306.8211975097656,
        460.96124267578125,
        329.1186828613281
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 21,
      "bbox": [
        133.38616943359375,
        289.9674987792969,
        148.86178588867188,
        302.78948974609375
      ],
      "text": "10G"
    },
    {
      "page_no": 21,
      "bbox": [
        128.5767059326172,
        256.08184814453125,
        148.86056518554688,
        268.9038391113281
      ],
      "text": "100G"
    },
    {
      "page_no": 21,
      "bbox": [
        139.43597412109375,
        222.19618225097656,
        148.86346435546875,
        235.0181884765625
      ],
      "text": "1T"
    },
    {
      "page_no": 21,
      "bbox": [
        134.62649536132812,
        188.31051635742188,
        148.86224365234375,
        201.1325225830078
      ],
      "text": "10T"
    },
    {
      "page_no": 21,
      "bbox": [
        129.81704711914062,
        154.4248504638672,
        148.8610382080078,
        167.24685668945312
      ],
      "text": "100T"
    },
    {
      "page_no": 21,
      "bbox": [
        115.52560424804688,
        176.05068969726562,
        128.3476104736328,
        257.5477294921875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 21,
      "bbox": [
        226.46617126464844,
        112.56177520751953,
        380.59649658203125,
        127.94817352294922
      ],
      "text": "Roofline Model (Llama 7B, A6000)"
    },
    {
      "page_no": 21,
      "bbox": [
        380.6651916503906,
        211.2529296875,
        447.1717224121094,
        301.7435607910156
      ],
      "text": "768GB/s\n181 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 21,
      "bbox": [
        222.43499755859375,
        346.3999938964844,
        374.4461364746094,
        355.36639404296875
      ],
      "text": "Figure 15. Llama-7B operators on A6000."
    },
    {
      "page_no": 21,
      "bbox": [
        166.07015991210938,
        634.8062133789062,
        460.96124267578125,
        657.1036987304688
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 21,
      "bbox": [
        133.38616943359375,
        617.9525146484375,
        148.86178588867188,
        630.7745361328125
      ],
      "text": "10G"
    },
    {
      "page_no": 21,
      "bbox": [
        128.5767059326172,
        584.0668334960938,
        148.86056518554688,
        596.8888549804688
      ],
      "text": "100G"
    },
    {
      "page_no": 21,
      "bbox": [
        139.43597412109375,
        550.18115234375,
        148.86346435546875,
        563.003173828125
      ],
      "text": "1T"
    },
    {
      "page_no": 21,
      "bbox": [
        134.62649536132812,
        516.2955322265625,
        148.86224365234375,
        529.1175537109375
      ],
      "text": "10T"
    },
    {
      "page_no": 21,
      "bbox": [
        129.81704711914062,
        482.40985107421875,
        148.8610382080078,
        495.2318420410156
      ],
      "text": "100T"
    },
    {
      "page_no": 21,
      "bbox": [
        115.52560424804688,
        504.0356750488281,
        128.3476104736328,
        585.53271484375
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 21,
      "bbox": [
        223.58892822265625,
        440.5467529296875,
        383.4891662597656,
        455.9331359863281
      ],
      "text": "Roofline Model (Llama 13B, A6000)"
    },
    {
      "page_no": 21,
      "bbox": [
        380.6651916503906,
        539.2379150390625,
        447.1717224121094,
        629.7285766601562
      ],
      "text": "768GB/s\n181 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 21,
      "bbox": [
        220.19400024414062,
        674.3849487304688,
        376.6883850097656,
        683.3513793945312
      ],
      "text": "Figure 16. Llama-13B operators on A6000."
    },
    {
      "page_no": 21,
      "bbox": [
        293.4590148925781,
        732.4114379882812,
        303.4216003417969,
        742.3740234375
      ],
      "text": "21"
    },
    {
      "page_no": 22,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 22,
      "bbox": [
        166.07015991210938,
        470.8141784667969,
        460.96124267578125,
        493.1116638183594
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 22,
      "bbox": [
        133.38616943359375,
        453.9604797363281,
        148.86178588867188,
        466.782470703125
      ],
      "text": "10G"
    },
    {
      "page_no": 22,
      "bbox": [
        128.5767059326172,
        420.0748291015625,
        148.86056518554688,
        432.8968200683594
      ],
      "text": "100G"
    },
    {
      "page_no": 22,
      "bbox": [
        139.43597412109375,
        386.18914794921875,
        148.86346435546875,
        399.0111389160156
      ],
      "text": "1T"
    },
    {
      "page_no": 22,
      "bbox": [
        134.62649536132812,
        352.3034973144531,
        148.86224365234375,
        365.12548828125
      ],
      "text": "10T"
    },
    {
      "page_no": 22,
      "bbox": [
        129.81704711914062,
        318.4178466796875,
        148.8610382080078,
        331.2398376464844
      ],
      "text": "100T"
    },
    {
      "page_no": 22,
      "bbox": [
        115.52560424804688,
        340.0436706542969,
        128.3476104736328,
        421.54071044921875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 22,
      "bbox": [
        223.58892822265625,
        276.55474853515625,
        383.4891662597656,
        291.9411315917969
      ],
      "text": "Roofline Model (Llama 33B, A6000)"
    },
    {
      "page_no": 22,
      "bbox": [
        380.6651916503906,
        375.24591064453125,
        447.1717224121094,
        465.7365417480469
      ],
      "text": "768GB/s\n181 TFLOP/s\nqkv mlp init\nqkv mlp ar\nup/gate/down init\nup/gate/down ar\nqk/pv init\nqk/pv ar"
    },
    {
      "page_no": 22,
      "bbox": [
        220.19400024414062,
        510.3919982910156,
        376.6883850097656,
        519.3583984375
      ],
      "text": "Figure 17. Llama-33B operators on A6000."
    },
    {
      "page_no": 22,
      "bbox": [
        293.4590148925781,
        732.4114379882812,
        303.4216003417969,
        742.3740234375
      ],
      "text": "22"
    },
    {
      "page_no": 23,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 23,
      "bbox": [
        55.439998626708984,
        69.37651824951172,
        316.2173156738281,
        79.33911895751953
      ],
      "text": "G.2. FLOP/s vs. Operational Intensity Variations in MEDUSA"
    },
    {
      "page_no": 23,
      "bbox": [
        54.97200012207031,
        88.04531860351562,
        543.0968017578125,
        110.03805541992188
      ],
      "text": "We investigate how Medusa can change Operational Intensity and elevate the FLOP/s. We choose Llama 33B on A100-\n80GB-PCIe as the setting."
    },
    {
      "page_no": 23,
      "bbox": [
        55.43999481201172,
        118.08443450927734,
        541.7929077148438,
        199.70205688476562
      ],
      "text": "First, we examine the attention matrix multiplication. Fig. 18 and Table 6 illustrate the effects of MEDUSA while keeping the\nbatch size fixed at 16. We observe increased FLOP/s and Operational Intensity as more candidate tokens are added (original\ndecoding results are plotted as grey dots). This indicates that MEDUSA can leverage additional candidate tokens to improve\ncomputational throughput. Compared to regular decoding, MEDUSA achieves 44× FLOP/s and 41× Operational Intensity\nunder the setting of batch size 16 and sequence length 1024 with 64 candidate tokens. Fig. 19 and Table 7 illustrate the\neffects of MEDUSA decoding while keeping the sequence length fixed at 1024. Increasing the batch size does not improve\nOperational Intensity in this scenario."
    },
    {
      "page_no": 23,
      "bbox": [
        55.13100051879883,
        207.59628295898438,
        542.6866455078125,
        277.4100646972656
      ],
      "text": "Next, we examine the linear layer, focusing on the up/gate/down linear layers. The results are shown in Fig. 20 and\nTable 8. Since the linear layers in the decoding phase only process the future tokens while the past tokens are cached,\nthey are independent of the sequence length. We vary the batch size to observe the effects. As MEDUSA increases the\nnumber of candidate tokens with the increasing batch size, we observe a shift from a memory-bandwidth-bound region to a\ncomputation-bound region. This shift demonstrates how MEDUSA can transition the performance characteristics of the\nlinear layers from being limited by memory bandwidth to being limited by computational capacity."
    },
    {
      "page_no": 23,
      "bbox": [
        166.07015991210938,
        496.9411926269531,
        460.96124267578125,
        519.2387084960938
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 23,
      "bbox": [
        133.38616943359375,
        480.0874938964844,
        148.86178588867188,
        492.90948486328125
      ],
      "text": "10G"
    },
    {
      "page_no": 23,
      "bbox": [
        128.5767059326172,
        446.20184326171875,
        148.86056518554688,
        459.0238342285156
      ],
      "text": "100G"
    },
    {
      "page_no": 23,
      "bbox": [
        139.43597412109375,
        412.316162109375,
        148.86346435546875,
        425.1381530761719
      ],
      "text": "1T"
    },
    {
      "page_no": 23,
      "bbox": [
        134.62649536132812,
        378.4305114746094,
        148.86224365234375,
        391.25250244140625
      ],
      "text": "10T"
    },
    {
      "page_no": 23,
      "bbox": [
        129.81704711914062,
        344.54486083984375,
        148.8610382080078,
        357.3668518066406
      ],
      "text": "100T"
    },
    {
      "page_no": 23,
      "bbox": [
        115.52560424804688,
        366.1706848144531,
        128.3476104736328,
        447.667724609375
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 23,
      "bbox": [
        239.43905639648438,
        302.6817626953125,
        367.52899169921875,
        318.0681457519531
      ],
      "text": "Llama 33B, A100 80GB PCIe"
    },
    {
      "page_no": 23,
      "bbox": [
        337.1943664550781,
        379.181884765625,
        447.1490478515625,
        491.8635559082031
      ],
      "text": "1,935GB/s\n312 TFLOP/s\nqk/pv ar\nqk/pv Medusa (# cand.: 16)\nqk/pv Medusa (# cand.: 32)\nqk/pv Medusa (# cand.: 48)\nqk/pv Medusa (# cand.: 64)\nqk/pv Medusa (# cand.: 80)\nqk/pv Medusa (# cand.: 96)\nqk/pv Medusa (# cand.: 112)"
    },
    {
      "page_no": 23,
      "bbox": [
        123.40799713134766,
        536.5199584960938,
        473.4776306152344,
        545.4863891601562
      ],
      "text": "Figure 18. FLOP/s vs. Operational Intensity of attention matrix multiplication with batch size 16."
    },
    {
      "page_no": 23,
      "bbox": [
        293.458984375,
        732.4114990234375,
        303.42156982421875,
        742.3740844726562
      ],
      "text": "23"
    },
    {
      "page_no": 24,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 24,
      "bbox": [
        166.07015991210938,
        306.8211975097656,
        460.96124267578125,
        329.1186828613281
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 24,
      "bbox": [
        133.38616943359375,
        289.9674987792969,
        148.86178588867188,
        302.78948974609375
      ],
      "text": "10G"
    },
    {
      "page_no": 24,
      "bbox": [
        128.5767059326172,
        256.08184814453125,
        148.86056518554688,
        268.9038391113281
      ],
      "text": "100G"
    },
    {
      "page_no": 24,
      "bbox": [
        139.43597412109375,
        222.19618225097656,
        148.86346435546875,
        235.0181884765625
      ],
      "text": "1T"
    },
    {
      "page_no": 24,
      "bbox": [
        134.62649536132812,
        188.31051635742188,
        148.86224365234375,
        201.1325225830078
      ],
      "text": "10T"
    },
    {
      "page_no": 24,
      "bbox": [
        129.81704711914062,
        154.4248504638672,
        148.8610382080078,
        167.24685668945312
      ],
      "text": "100T"
    },
    {
      "page_no": 24,
      "bbox": [
        115.52560424804688,
        176.05068969726562,
        128.3476104736328,
        257.5477294921875
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 24,
      "bbox": [
        239.43905639648438,
        112.56177520751953,
        367.52899169921875,
        127.94817352294922
      ],
      "text": "Llama 33B, A100 80GB PCIe"
    },
    {
      "page_no": 24,
      "bbox": [
        337.1943664550781,
        189.06190490722656,
        447.1490478515625,
        301.7435607910156
      ],
      "text": "1,935GB/s\n312 TFLOP/s\nqk/pv ar\nqk/pv Medusa (# cand.: 16)\nqk/pv Medusa (# cand.: 32)\nqk/pv Medusa (# cand.: 48)\nqk/pv Medusa (# cand.: 64)\nqk/pv Medusa (# cand.: 80)\nqk/pv Medusa (# cand.: 96)\nqk/pv Medusa (# cand.: 112)"
    },
    {
      "page_no": 24,
      "bbox": [
        107.96800231933594,
        346.3999938964844,
        488.9180603027344,
        355.36639404296875
      ],
      "text": "Figure 19. FLOP/s vs. Operational Intensity of attention matrix multiplication with sequence length 1024."
    },
    {
      "page_no": 24,
      "bbox": [
        166.07015991210938,
        634.8062133789062,
        460.96124267578125,
        657.1036987304688
      ],
      "text": "1\n10\n100\n1k\n10k\nOperational Intensity (FLOP/Byte)"
    },
    {
      "page_no": 24,
      "bbox": [
        133.38616943359375,
        617.9525146484375,
        148.86178588867188,
        630.7745361328125
      ],
      "text": "10G"
    },
    {
      "page_no": 24,
      "bbox": [
        128.5767059326172,
        584.0668334960938,
        148.86056518554688,
        596.8888549804688
      ],
      "text": "100G"
    },
    {
      "page_no": 24,
      "bbox": [
        139.43597412109375,
        550.18115234375,
        148.86346435546875,
        563.003173828125
      ],
      "text": "1T"
    },
    {
      "page_no": 24,
      "bbox": [
        134.62649536132812,
        516.2955322265625,
        148.86224365234375,
        529.1175537109375
      ],
      "text": "10T"
    },
    {
      "page_no": 24,
      "bbox": [
        129.81704711914062,
        482.40985107421875,
        148.8610382080078,
        495.2318420410156
      ],
      "text": "100T"
    },
    {
      "page_no": 24,
      "bbox": [
        115.52560424804688,
        504.0356750488281,
        128.3476104736328,
        585.53271484375
      ],
      "text": "Performance (FLOP/s)"
    },
    {
      "page_no": 24,
      "bbox": [
        239.43905639648438,
        440.5467529296875,
        367.52899169921875,
        455.9331359863281
      ],
      "text": "Llama 33B, A100 80GB PCIe"
    },
    {
      "page_no": 24,
      "bbox": [
        355.689697265625,
        517.046875,
        447.15985107421875,
        629.7285766601562
      ],
      "text": "1,935GB/s\n312 TFLOP/s\nup/gate/down ar\nup/gate/down spec: 16\nup/gate/down spec: 32\nup/gate/down spec: 48\nup/gate/down spec: 64\nup/gate/down spec: 80\nup/gate/down spec: 96\nup/gate/down spec: 112"
    },
    {
      "page_no": 24,
      "bbox": [
        188.2969970703125,
        674.3849487304688,
        408.5860595703125,
        683.3513793945312
      ],
      "text": "Figure 20. FLOP/s vs. Operational Intensity of Linear layers."
    },
    {
      "page_no": 24,
      "bbox": [
        293.458984375,
        732.4114379882812,
        303.42156982421875,
        742.3740234375
      ],
      "text": "24"
    },
    {
      "page_no": 25,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 25,
      "bbox": [
        75.86599731445312,
        70.93611907958984,
        362.5171813964844,
        77.90991973876953
      ],
      "text": "Seq. Length\nNumber of Candidate Tokens"
    },
    {
      "page_no": 25,
      "bbox": [
        136.86099243164062,
        83.90816497802734,
        506.0959167480469,
        90.88196563720703
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112"
    },
    {
      "page_no": 25,
      "bbox": [
        75.86599731445312,
        96.8801498413086,
        521.0106811523438,
        151.67481994628906
      ],
      "text": "128\n0.54 & 0.98\n7.87 & 12.8\n14.73 & 21.33\n19.78 & 27.43\n25.25 & 32.0\n28.63 & 35.56\n32.58 & 38.4\n36.57 & 40.73\n256\n0.75 & 0.99\n11.2 & 13.47\n21.29 & 23.27\n28.69 & 30.72\n36.59 & 36.57\n41.2 & 41.29\n45.99 & 45.18\n52.33 & 48.43\n512\n1.02 & 0.99\n14.69 & 13.84\n27.47 & 24.38\n37.35 & 32.68\n47.09 & 39.38\n52.24 & 44.91\n59.55 & 49.55\n66.35 & 53.49\n1024\n1.24 & 0.99\n17.42 & 14.03\n32.15 & 24.98\n43.89 & 33.76\n54.8 & 40.96\n60.19 & 46.97\n68.28 & 52.07\n75.45 & 56.44\n2048\n1.39 & 0.99\n19.03 & 14.12\n35.05 & 25.28\n48.03 & 34.32\n59.66 & 41.8\n63.91 & 48.08\n72.83 & 53.43\n80.05 & 58.04\n4096\n1.48 & 0.99\n19.8 & 14.17\n36.59 & 25.44\n50.4 & 34.61\n62.29 & 42.23\n65.84 & 48.65\n74.86 & 54.13\n82.06 & 58.87\n8192\n1.53 & 0.99\n20.08 & 14.2\n36.89 & 25.52\n50.44 & 34.76\n62.11 & 42.45\n67.5 & 48.94\n76.97 & 54.49\n84.5 & 59.3"
    },
    {
      "page_no": 25,
      "bbox": [
        54.893001556396484,
        165.16799926757812,
        540.5667724609375,
        174.1343994140625
      ],
      "text": "Table 6. TFLOP/s & Operational Intensity of attention matrix multiplication with batch size 16 for Llama 33B on an A100 80GB PCIe."
    },
    {
      "page_no": 25,
      "bbox": [
        77.91600036621094,
        196.73617553710938,
        360.4665832519531,
        203.70997619628906
      ],
      "text": "Batch Size\nNumber of Candidate Tokens"
    },
    {
      "page_no": 25,
      "bbox": [
        134.80999755859375,
        209.70816040039062,
        504.0448913574219,
        216.6819610595703
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112"
    },
    {
      "page_no": 25,
      "bbox": [
        77.91600036621094,
        222.68014526367188,
        518.967041015625,
        277.4737548828125
      ],
      "text": "1\n0.37 & 0.99\n5.22 & 14.03\n10.15 & 24.98\n15.02 & 33.76\n19.79 & 40.96\n21.52 & 46.97\n25.65 & 52.07\n29.4 & 56.44\n2\n0.54 & 0.99\n8.25 & 14.03\n16.0 & 24.98\n21.62 & 33.76\n28.24 & 40.96\n31.84 & 46.97\n37.49 & 52.07\n43.04 & 56.44\n4\n0.75 & 0.99\n11.41 & 14.03\n21.97 & 24.98\n30.02 & 33.76\n38.71 & 40.96\n43.41 & 46.97\n50.06 & 52.07\n56.77 & 56.44\n8\n1.02 & 0.99\n14.78 & 14.03\n27.78 & 24.98\n38.09 & 33.76\n47.99 & 40.96\n53.32 & 46.97\n61.0 & 52.07\n68.11 & 56.44\n16\n1.24 & 0.99\n17.42 & 14.03\n32.15 & 24.98\n43.89 & 33.76\n54.8 & 40.96\n60.19 & 46.97\n68.28 & 52.07\n75.45 & 56.44\n32\n1.39 & 0.99\n18.89 & 14.03\n34.67 & 24.98\n47.57 & 33.76\n58.89 & 40.96\n63.61 & 46.97\n72.17 & 52.07\n79.21 & 56.44\n64\n1.48 & 0.99\n19.58 & 14.03\n35.87 & 24.98\n49.45 & 33.76\n61.13 & 40.96\n64.84 & 46.97\n73.73 & 52.07\n81.02 & 56.44"
    },
    {
      "page_no": 25,
      "bbox": [
        54.893001556396484,
        290.96697998046875,
        541.4410400390625,
        310.89239501953125
      ],
      "text": "Table 7. TFLOP/s & Operational Intensity of attention matrix multiplication with sequence length 1024 for Llama 33B on an A100 80GB\nPCIe."
    },
    {
      "page_no": 25,
      "bbox": [
        71.20600128173828,
        331.73663330078125,
        352.4580993652344,
        337.7142333984375
      ],
      "text": "Batch Size\nNumber of Candidate Tokens"
    },
    {
      "page_no": 25,
      "bbox": [
        124.66899871826172,
        343.712646484375,
        508.4070129394531,
        349.69024658203125
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112"
    },
    {
      "page_no": 25,
      "bbox": [
        71.20600128173828,
        355.6886291503906,
        525.677001953125,
        403.50921630859375
      ],
      "text": "1\n1.26 & 1.0\n19.95 & 15.95\n39.69 & 31.79\n58.4 & 47.53\n76.57 & 63.17\n94.4 & 78.7\n111.91 & 94.14\n128.64 & 109.47\n2\n2.51 & 2.0\n39.66 & 31.79\n76.53 & 63.17\n112.05 & 94.14\n145.73 & 124.71\n130.67 & 154.89\n129.1 & 184.69\n148.56 & 214.12\n4\n5.03 & 4.0\n76.44 & 63.17\n145.8 & 124.71\n128.85 & 184.69\n167.85 & 243.17\n201.19 & 300.21\n236.93 & 355.85\n195.91 & 410.14\n8\n10.06 & 7.99\n145.72 & 124.71\n168.26 & 243.17\n236.83 & 355.85\n221.11 & 463.14\n207.79 & 565.44\n236.95 & 663.07\n227.8 & 756.36\n16\n19.96 & 15.95\n168.35 & 243.17\n221.41 & 463.14\n237.5 & 663.07\n224.71 & 845.59\n232.49 & 1012.87\n241.12 & 1166.74\n229.25 & 1308.76\n32\n39.69 & 31.79\n221.74 & 463.14\n224.88 & 845.59\n241.33 & 1166.74\n239.02 & 1440.25\n245.83 & 1675.97\n243.55 & 1881.24\n240.33 & 2061.59\n64\n76.57 & 63.17\n225.19 & 845.59\n239.2 & 1440.25\n243.26 & 1881.24\n246.16 & 2221.31\n246.91 & 2491.55\n244.52 & 2711.46\n246.14 & 2893.91"
    },
    {
      "page_no": 25,
      "bbox": [
        91.79199981689453,
        416.9439697265625,
        502.31365966796875,
        425.9103698730469
      ],
      "text": "Table 8. TFLOP/s & Operational Intensity of linear layers (up/gate/down) for Llama 33B on an A100 80GB PCIe."
    },
    {
      "page_no": 25,
      "bbox": [
        55.43999481201172,
        449.65850830078125,
        218.96600341796875,
        459.62109375
      ],
      "text": "G.3. Predicting MEDUSA Performance"
    },
    {
      "page_no": 25,
      "bbox": [
        54.97200012207031,
        468.3262939453125,
        542.6806640625,
        609.8710327148438
      ],
      "text": "We further employ a straightforward analytical model for the acceleration rate. The ablation study results in Sec. 3.3.1\nindicate that the acceleration rate can be approximated by a simple logarithmic function. Using the results from Fig. 4a,\nwe model the curve as acc rate = 0.477 log(num candidate). We simulate the latency of one simplified block of\nthe Llama-7B model (sequentially processing XWQ, XWK, XWV , QKT , PV , XWu, XWg, XWd) by first fixing the\nbatch size at 1 and the sequence length at 1024. The candidate tokens are processed parallelly by constructing the tree\nattention described in Section 2.1.2. We omit the latency of the post-processing steps including verification and acceptance\nfor MEDUSA since they introduce marginal overhead.\nFig. 21 illustrates the simulated acceleration rate and speedup\nfor different numbers of candidate tokens under these settings. As the number of candidate tokens increases, both the\nacceleration rate and speedup initially show improvements. However, beyond 64, the speedup starts to decline, indicating\ndiminishing returns with further increases in candidate length. This aligns with the experimental results in Fig. 4b and\nsuggests that there is an optimal range for the numbers of candidate tokens where MEDUSA provides the most significant\nperformance gains."
    },
    {
      "page_no": 25,
      "bbox": [
        54.97200012207031,
        617.7662353515625,
        541.440185546875,
        651.7140502929688
      ],
      "text": "We plot the simulated speedup under different batch size settings with a fixed sequence length of 1024 in Fig. 22. The\nresults indicate that when the batch size exceeds 32, the speedup decreases and may even have a negative effect. This occurs\nbecause the linear layers shift from being memory-bandwidth-bound to computationally bound."
    },
    {
      "page_no": 25,
      "bbox": [
        54.97200012207031,
        659.6092529296875,
        541.6845092773438,
        717.468017578125
      ],
      "text": "We conduct another experiment using a batch size of 4 and different sequence lengths. As shown in Fig. 23, the optimal\nnumber of candidate tokens remains relatively consistent across different sequence lengths. However, as the sequence length\nincreases, the overall performance decreases. This performance drop is primarily due to the overhead from attention matrix\nmultiplication, while the linear layer computation remains constant since the computation of linear layers is independent of\nthe sequence length."
    },
    {
      "page_no": 25,
      "bbox": [
        293.458984375,
        732.4114379882812,
        303.42156982421875,
        742.3740234375
      ],
      "text": "25"
    },
    {
      "page_no": 26,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 26,
      "bbox": [
        55.439998626708984,
        69.39230346679688,
        541.6138305664062,
        163.11709594726562
      ],
      "text": "Our simulations show that the optimal number of candidate tokens is key for model scaling with MEDUSA, as benefits\ndecrease beyond a certain range. Initially, increasing batch size improves performance through parallelism, but too large a\nbatch size shifts linear layers from memory-bandwidth-bound to compute-bound, reducing speedup. Longer sequences\nincrease attention matrix multiplication overhead, lowering performance, and emphasizing the need to optimize attention\nmechanisms. Effective model scaling requires balancing the number of candidate tokens, adjusting batch sizes to avoid\ncompute-bound transitions, and enhancing attention mechanisms for longer sequences. These strategies ensure better\nresource utilization and higher performance, demonstrating the value of simulations in predicting performance and guiding\nacceleration strategy design."
    },
    {
      "page_no": 26,
      "bbox": [
        170.45498657226562,
        382.648193359375,
        440.95819091796875,
        404.9456787109375
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112\nNumber of Candidate Tokens"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        375.9951171875,
        148.8577880859375,
        388.8171081542969
      ],
      "text": "0.0"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        350.1874694824219,
        148.8577880859375,
        363.00946044921875
      ],
      "text": "0.5"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        324.37982177734375,
        148.8577880859375,
        337.2018127441406
      ],
      "text": "1.0"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        298.57220458984375,
        148.8577880859375,
        311.3941955566406
      ],
      "text": "1.5"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        272.7645568847656,
        148.8577880859375,
        285.5865478515625
      ],
      "text": "2.0"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        246.95693969726562,
        148.8577880859375,
        259.7789306640625
      ],
      "text": "2.5"
    },
    {
      "page_no": 26,
      "bbox": [
        136.83717346191406,
        221.14930725097656,
        148.8577880859375,
        233.9713134765625
      ],
      "text": "3.0"
    },
    {
      "page_no": 26,
      "bbox": [
        123.78607177734375,
        215.6865997314453,
        136.6080780029297,
        369.56805419921875
      ],
      "text": "Normalized Latency/ Acc. Rate/ Speedup"
    },
    {
      "page_no": 26,
      "bbox": [
        193.01507568359375,
        188.38876342773438,
        414.2331848144531,
        203.77516174316406
      ],
      "text": "Llama 7B, Batch Size: 1, Sequence Length: 1024"
    },
    {
      "page_no": 26,
      "bbox": [
        371.02093505859375,
        263.20489501953125,
        447.1443786621094,
        320.4089660644531
      ],
      "text": "Simulated Acc. Rate\nSimulated Speedup\nqk/pv ar\nqkv linear ar\nup/gate/down ar"
    },
    {
      "page_no": 26,
      "bbox": [
        54.9379997253418,
        422.2157897949219,
        541.4384765625,
        442.1513671875
      ],
      "text": "Figure 21. Simulated acceleration rate, speedup, and normalized latency ablation using different numbers of candidate tokens under the\nsetting of batch size 1 and sequence length 1024 for Llama-7B on an A100 80GB PCIe."
    },
    {
      "page_no": 26,
      "bbox": [
        293.458984375,
        732.4114379882812,
        303.42156982421875,
        742.3740234375
      ],
      "text": "26"
    },
    {
      "page_no": 27,
      "bbox": [
        126.02799987792969,
        47.22712326049805,
        471.0809020996094,
        56.19352340698242
      ],
      "text": "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"
    },
    {
      "page_no": 27,
      "bbox": [
        144.03488159179688,
        303.6027526855469,
        329.76214599609375,
        325.90020751953125
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112\nNumber of Candidate Tokens"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        296.691162109375,
        133.8509063720703,
        309.5131530761719
      ],
      "text": "0.0"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        266.4857482910156,
        133.8509063720703,
        279.3077392578125
      ],
      "text": "0.5"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        236.28036499023438,
        133.8509063720703,
        249.1023712158203
      ],
      "text": "1.0"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        206.07496643066406,
        133.8509063720703,
        218.89697265625
      ],
      "text": "1.5"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        175.86956787109375,
        133.8509063720703,
        188.6915740966797
      ],
      "text": "2.0"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        145.66416931152344,
        133.8509063720703,
        158.48617553710938
      ],
      "text": "2.5"
    },
    {
      "page_no": 27,
      "bbox": [
        121.83029174804688,
        115.45878601074219,
        133.8509063720703,
        128.28079223632812
      ],
      "text": "3.0"
    },
    {
      "page_no": 27,
      "bbox": [
        108.77919006347656,
        186.01300048828125,
        121.60118865966797,
        234.79103088378906
      ],
      "text": "Speedup (%)"
    },
    {
      "page_no": 27,
      "bbox": [
        156.6455841064453,
        102.97119903564453,
        312.7890625,
        118.35759735107422
      ],
      "text": "Llama 7B, Sequence Length: 1024"
    },
    {
      "page_no": 27,
      "bbox": [
        369.0158996582031,
        120.62576293945312,
        476.0523681640625,
        200.0208740234375
      ],
      "text": "Simulated Speedup @ bs 1\nSimulated Speedup @ bs 2\nSimulated Speedup @ bs 4\nSimulated Speedup @ bs 8\nSimulated Speedup @ bs 16\nSimulated Speedup @ bs 32\nSimulated Speedup @ bs 64"
    },
    {
      "page_no": 27,
      "bbox": [
        168.64199829101562,
        346.3999938964844,
        428.2409973144531,
        355.36639404296875
      ],
      "text": "Figure 22. Simulated speedup with sequence length 1024 for Llama-7B."
    },
    {
      "page_no": 27,
      "bbox": [
        142.6387176513672,
        631.5877075195312,
        301.41455078125,
        653.88525390625
      ],
      "text": "1\n16\n32\n48\n64\n80\n96\n112\nNumber of Candidate Tokens"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        614.1804809570312,
        133.80230712890625,
        627.0025024414062
      ],
      "text": "1.0"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        591.5576171875,
        133.80230712890625,
        604.379638671875
      ],
      "text": "1.2"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        568.9347534179688,
        133.80230712890625,
        581.7567749023438
      ],
      "text": "1.4"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        546.3118896484375,
        133.80230712890625,
        559.1339111328125
      ],
      "text": "1.6"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        523.68896484375,
        133.80230712890625,
        536.510986328125
      ],
      "text": "1.8"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        501.06610107421875,
        133.80230712890625,
        513.8881225585938
      ],
      "text": "2.0"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        478.4432067871094,
        133.80230712890625,
        491.26519775390625
      ],
      "text": "2.2"
    },
    {
      "page_no": 27,
      "bbox": [
        121.78169250488281,
        455.8203430175781,
        133.80230712890625,
        468.642333984375
      ],
      "text": "2.4"
    },
    {
      "page_no": 27,
      "bbox": [
        108.7305908203125,
        513.9979858398438,
        121.5525894165039,
        562.7760009765625
      ],
      "text": "Speedup (%)"
    },
    {
      "page_no": 27,
      "bbox": [
        166.04461669921875,
        430.9561767578125,
        273.53167724609375,
        446.3425598144531
      ],
      "text": "Llama 7B, Batch Size: 4"
    },
    {
      "page_no": 27,
      "bbox": [
        337.8384094238281,
        448.6107482910156,
        474.4653015136719,
        529.271484375
      ],
      "text": "Simulated Speedup @ seq_len 128\nSimulated Speedup @ seq_len 256\nSimulated Speedup @ seq_len 512\nSimulated Speedup @ seq_len 1024\nSimulated Speedup @ seq_len 2048\nSimulated Speedup @ seq_len 4096\nSimulated Speedup @ seq_len 8192"
    },
    {
      "page_no": 27,
      "bbox": [
        186.32400512695312,
        674.3849487304688,
        410.5594177246094,
        683.3513793945312
      ],
      "text": "Figure 23. Simulated speedup with batch size 4 for Llama-7B."
    },
    {
      "page_no": 27,
      "bbox": [
        293.4590148925781,
        732.4114379882812,
        303.4216003417969,
        742.3740234375
      ],
      "text": "27"
    }
  ],
  "pictures": [
    {
      "page_no": 2,
      "bbox": [
        315.0,
        307.0,
        386.0,
        419.0
      ],
      "xref": 7,
      "image_path": "../data/parsed_documents/2401.10774/images/2401.10774_p2_blk1_crop.png"
    },
    {
      "page_no": 2,
      "bbox": [
        407.6658630371094,
        286.91455078125,
        443.2181701660156,
        324.44171142578125
      ],
      "xref": 24,
      "image_path": "../data/parsed_documents/2401.10774/images/2401.10774_p2_blk2_crop.png"
    },
    {
      "page_no": 4,
      "bbox": [
        63.090999603271484,
        67.03536987304688,
        281.73291015625,
        211.260009765625
      ],
      "xref": 1,
      "image_path": "../data/parsed_documents/2401.10774/images/2401.10774_p4_blk1_crop.png"
    }
  ],
  "tables": [
    {
      "page_no": 1,
      "index": 1,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p1_table1_stream.csv"
    },
    {
      "page_no": 1,
      "index": 2,
      "flavor": "stream",
      "nrows": 73,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p1_table2_stream.csv"
    },
    {
      "page_no": 3,
      "index": 1,
      "flavor": "stream",
      "nrows": 75,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p3_table1_stream.csv"
    },
    {
      "page_no": 4,
      "index": 1,
      "flavor": "lattice",
      "nrows": 8,
      "ncols": 8,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p4_table1_lattice.csv"
    },
    {
      "page_no": 5,
      "index": 1,
      "flavor": "stream",
      "nrows": 37,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p5_table1_stream.csv"
    },
    {
      "page_no": 5,
      "index": 2,
      "flavor": "stream",
      "nrows": 60,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p5_table2_stream.csv"
    },
    {
      "page_no": 6,
      "index": 1,
      "flavor": "stream",
      "nrows": 61,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p6_table1_stream.csv"
    },
    {
      "page_no": 7,
      "index": 1,
      "flavor": "stream",
      "nrows": 25,
      "ncols": 10,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p7_table1_stream.csv"
    },
    {
      "page_no": 7,
      "index": 2,
      "flavor": "stream",
      "nrows": 46,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p7_table2_stream.csv"
    },
    {
      "page_no": 8,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 7,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p8_table1_lattice.csv"
    },
    {
      "page_no": 8,
      "index": 2,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p8_table2_lattice.csv"
    },
    {
      "page_no": 8,
      "index": 3,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p8_table3_lattice.csv"
    },
    {
      "page_no": 9,
      "index": 1,
      "flavor": "stream",
      "nrows": 25,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p9_table1_stream.csv"
    },
    {
      "page_no": 9,
      "index": 2,
      "flavor": "stream",
      "nrows": 32,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p9_table2_stream.csv"
    },
    {
      "page_no": 9,
      "index": 3,
      "flavor": "stream",
      "nrows": 14,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p9_table3_stream.csv"
    },
    {
      "page_no": 10,
      "index": 1,
      "flavor": "stream",
      "nrows": 44,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p10_table1_stream.csv"
    },
    {
      "page_no": 11,
      "index": 1,
      "flavor": "stream",
      "nrows": 63,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p11_table1_stream.csv"
    },
    {
      "page_no": 12,
      "index": 1,
      "flavor": "stream",
      "nrows": 49,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p12_table1_stream.csv"
    },
    {
      "page_no": 13,
      "index": 1,
      "flavor": "stream",
      "nrows": 49,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p13_table1_stream.csv"
    },
    {
      "page_no": 14,
      "index": 1,
      "flavor": "stream",
      "nrows": 43,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p14_table1_stream.csv"
    },
    {
      "page_no": 15,
      "index": 1,
      "flavor": "lattice",
      "nrows": 7,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p15_table1_lattice.csv"
    },
    {
      "page_no": 15,
      "index": 2,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p15_table2_lattice.csv"
    },
    {
      "page_no": 15,
      "index": 3,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 3,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p15_table3_lattice.csv"
    },
    {
      "page_no": 16,
      "index": 1,
      "flavor": "lattice",
      "nrows": 5,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p16_table1_lattice.csv"
    },
    {
      "page_no": 17,
      "index": 1,
      "flavor": "stream",
      "nrows": 20,
      "ncols": 5,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p17_table1_stream.csv"
    },
    {
      "page_no": 18,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p18_table1_lattice.csv"
    },
    {
      "page_no": 19,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p19_table1_lattice.csv"
    },
    {
      "page_no": 19,
      "index": 2,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p19_table2_lattice.csv"
    },
    {
      "page_no": 20,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p20_table1_lattice.csv"
    },
    {
      "page_no": 20,
      "index": 2,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p20_table2_lattice.csv"
    },
    {
      "page_no": 21,
      "index": 1,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p21_table1_lattice.csv"
    },
    {
      "page_no": 21,
      "index": 2,
      "flavor": "lattice",
      "nrows": 4,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p21_table2_lattice.csv"
    },
    {
      "page_no": 22,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 1,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p22_table1_lattice.csv"
    },
    {
      "page_no": 23,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p23_table1_lattice.csv"
    },
    {
      "page_no": 24,
      "index": 1,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p24_table1_lattice.csv"
    },
    {
      "page_no": 24,
      "index": 2,
      "flavor": "lattice",
      "nrows": 3,
      "ncols": 2,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p24_table2_lattice.csv"
    },
    {
      "page_no": 25,
      "index": 1,
      "flavor": "stream",
      "nrows": 10,
      "ncols": 9,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p25_table1_stream.csv"
    },
    {
      "page_no": 25,
      "index": 2,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 9,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p25_table2_stream.csv"
    },
    {
      "page_no": 25,
      "index": 3,
      "flavor": "stream",
      "nrows": 9,
      "ncols": 9,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p25_table3_stream.csv"
    },
    {
      "page_no": 26,
      "index": 1,
      "flavor": "lattice",
      "nrows": 2,
      "ncols": 12,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p26_table1_lattice.csv"
    },
    {
      "page_no": 27,
      "index": 1,
      "flavor": "stream",
      "nrows": 15,
      "ncols": 10,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p27_table1_stream.csv"
    },
    {
      "page_no": 27,
      "index": 2,
      "flavor": "stream",
      "nrows": 12,
      "ncols": 4,
      "csv_path": "../data/parsed_documents/2401.10774/2401.10774_p27_table2_stream.csv"
    }
  ]
}