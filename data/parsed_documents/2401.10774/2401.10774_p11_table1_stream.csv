"MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",""
"Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural","Leviathan, Y., Kalman, M., and Matias, Y. Fast inference"
"the 56th Annual
story generation.
In Proceedings of","from transformers via speculative decoding. November"
"Meeting of the Association for Computational Linguistics","2022. doi: 10.48550/ARXIV.2211.17192."
"(Volume 1: Long Papers). Association for Computational",""
"","Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani,
I.,"
"Linguistics, 2018. doi: 10.18653/v1/p18-1082.",""
"","Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-"
"Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:","val: An automatic evaluator of
instruction-following"
"Accurate post-training quantization for generative pre-","https://github.com/tatsu-lab/
models."
"trained transformers. arXiv preprint arXiv:2210.17323,","alpaca_eval, 2023."
"2022.",""
"","Lin,
J., Tang,
J., Tang, H., Yang, S., Dang, X.,
and"
"Google.
Palm 2
technical
report,
2023.
URL","Han, S.
Awq: Activation-aware weight quantization"
"https://ai.google/static/documents/","arXiv preprint
for
llm compression and acceleration."
"palm2techreport.pdf.","arXiv:2306.00978, 2023."
"Hewitt, J., Manning, C. D., and Liang, P. Truncation sam-","Meister, C., Wiher, G., Pimentel, T., and Cotterell, R. On"
"pling as language model desmoothing. October 2022.","the probability-quality paradox in language generation."
"doi: 10.48550/ARXIV.2210.15191.","March 2022. doi: 10.48550/ARXIV.2203.17217."
"Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,","Meister, C., Pimentel, T., Wiher, G., and Cotterell, R. Lo-"
"Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,","cally typical sampling. Transactions of the Association"
"Welbl, J., Clark, A., et al.
Training compute-optimal","for Computational Linguistics, 11:102â€“121, 2023."
"large language models. arXiv preprint arXiv:2203.15556,",""
"","Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong,"
"2022.",""
"","R. Y. Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,"
"Holtzman, A., Buys,
J., Du, L., Forbes, M.,
and Choi,","Z. Specinfer: Accelerating generative llm serving with"
"Y
.
The curious case of neural
text degeneration.
In","speculative inference and token tree verification. arXiv"
"International Conference on Learning Representations,","preprint arXiv:2305.09781, 2023."
"2020. URL https://openreview.net/forum?",""
"id=rygGQyrFvH.","NVIDIA. Nvidia a100 tensor core gpu."
"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,","OpenAI. Gpt-4 technical report, 2023."
"S., and Chen, W. Lora: Low-rank adaptation of large",""
"","Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,"
"language models.
ICLR, 2021.",""
"","C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,"
"Joao
Gante.
Assisted
generation:
a
new
direc-","K., Ray, A., et al.
Training language models
to fol-"
"tion
toward
low-latency
text
generation,
2023.","arXiv preprint
low instructions with human feedback."
"https://huggingface.co/blog/
URL","arXiv:2203.02155, 2022."
"assisted-generation.",""
"","Pan, J. Tiny vicuna 1b. https://huggingface.co/"
"Kim,
S.,
Hooper,
C.,
Gholami,
A.,
Dong,
Z.,
Li,","Jiayi-Pan/Tiny-Vicuna-1B, 2023."
"X.,
Shen,
S., Mahoney, M. W.,
and Keutzer, K.",""
"","Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J.,"
"arXiv
Squeezellm:
Dense-and-sparse
quantization.",""
"","Welleck, S., Choi, Y., and Harchaoui, Z. MAUVE: Mea-"
"preprint arXiv:2306.07629, 2023.",""
"","suring the gap between neural text and human text using"
"Kim, Y. and Rush, A. M. Sequence-level knowledge distil-","divergence frontiers.
In Beygelzimer, A., Dauphin, Y.,"
"lation. EMNLP, 2016.","Liang, P., and Vaughan, J. W. (eds.), Advances in Neural"
"","Information Processing Systems, 2021. URL https:"
"Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang,",""
"","//openreview.net/forum?id=Tqx7nJp7PR."
"P. Fine-tuning can distort pretrained features and under-",""
"International Conference on
perform out-of-distribution.","Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Brad-"
"Learning Representations, 2022.","bury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S.,"
"","and Dean, J. Efficiently scaling transformer inference."
"Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,",""
"","November 2022. doi: 10.48550/ARXIV.2211.05102."
"C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient",""
"memory management for large language model serving","https://huggingface.
ShareGPT.
ShareGPT."
"with pagedattention.
In Proceedings of the ACM SIGOPS","co/datasets/Aeala/ShareGPT_Vicuna_"
"29th Symposium on Operating Systems Principles, 2023.","unfiltered, 2023."
