"","MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",""
"","percent of the probability. Another method, known as typical decoding (Meister et al., 2023), employs the entropy of the",""
"","predicted distribution to establish the threshold for inclusion. Hewitt et al. (2022) offers a unified framework to understand",""
"truncation sampling techniques comprehensively.","",""
"","Drawing inspiration from these methods, our typical acceptance scheme aligns with the concept of defining an allowed set",""
"","to exclude improbable candidates from the sampling process. However, we diverge because we do not insist on an exact",""
"","correspondence between the output and language model distribution. This deviation allows us to facilitate more diverse yet",""
"","high-quality outputs, achieving greater efficiency without compromising the integrity of the generated text.",""
"B. Experiment Settings","",""
"B.1. Common Terms","",""
"","We clarify three commonly used terms: a) Acceleration rate: This refers to the average number of tokens decoded per",""
"decoding step.","In a standard auto-regressive model,","this rate is 1.0. b) Overhead: This is used to characterize the per"
"","decoding step overhead compared to classic decoding, and is calculated by dividing the average per step latency of the",""
"","MEDUSA models by that of the vanilla model. c) Speedup: This refers to the wall-time acceleration rate. Following these",""
"","definitions, we have the relation: Speedup = Acceleration rate / Overhead.",""
"B.2. Shared Settings","",""
"","For all the experiments, we use the Axolotl (Axolotl, 2023) framework for training. We use a cosine learning rate scheduler",""
"","with warmup and use 8-bit AdamW (Dettmers et al., 2021) optimizer. We train 5 MEDUSA heads with 1 layer and set λk in",""
"","Eq. (1) to be 0.8k. For MEDUSA-2, we use either LoRA (Hu et al., 2021) or QLoRA (Dettmers et al., 2023) for fine-tuning",""
"","and set the learning rate of MEDUSA heads to be 4 times larger than the backbone model. LoRA is applied to all the linear",""
"","layers of the backbone model, including the language model head. The rank of LoRA adapter is set to 32, and α is set to 16.",""
"A dropout of 0.05 is added to the LoRA adapter.","",""
"","B.3. MEDUSA-1 v.s. MEDUSA-2 on Vicuna 7B and 13B",""
"","We use a global batch size of 64 and a peak learning rate of 5e−4 for the backbone and 2e−3 for MEDUSA heads and",""
"","warmup for 40 steps. We use 4-bit quantized backbone models for both models. We first train the models with MEDUSA-1",""
"","and use these trained models as initialization to train MEDUSA-2. We employ QLoRA for MEDUSA-2 and the λ0 in Eq. (2)",""
"is set to be 0.2.","",""
"","B.4. Training with Self-Distillation on Vicuna-33B and Zephyr-7B",""
"","We use MEDUSA-2 for both models instead of using a two-stage training procedure. We use a sine schedule for the θ0 to",""
"","gradually increase the value to its peak at the end of the training. We find this approach is equally effective. We set the",""
"","peak learning rate of the backbone LoRA adapter to be 1e−4 and the warmup steps to be 20 since the self-distillation loss is",""
"relatively small. We set the λ0 in Eq. (2) to be 0.01.","",""
"","C. Visualization of optimized tree attention",""
"","Fig. 6 illustrates the structure of a sparsely constructed tree for the MEDUSA-2 Vicuna-7B model. This tree structure extends",""
"","four levels deep, indicating the engagement of four MEDUSA heads in the computation. The tree is initially formed through a",""
"","Cartesian product approach and subsequently refined by pruning based on the statistical expectations of the top-k predictions",""
"","from each MEDUSA head measured on the Alpaca-eval dataset (Dubois et al., 2023). The tree’s lean towards the left visually",""
"","represents the algorithm’s preference for nodes with higher probabilities on each head.",""
"D. Results of Speculative Decoding","",""
"","In this study, speculative decoding was applied to Vicuna models (Chiang et al., 2023) with varying sizes, specifically 7B,",""
"13B, and 33B. The preliminary framework utilized open-source models such as Llama-68M and 160M (Miao et al., 2023),","",""
"","alongside Tiny-Llama (Zhang et al., 2024) and Tiny-Vicuna (Pan, 2023), fine-tuned from Tiny-Llama with the Vicuna-style",""
"","instructional tuning strategy. Due to the proprietary nature of speculative decoding methods (Chen et al., 2023; Leviathan",""
