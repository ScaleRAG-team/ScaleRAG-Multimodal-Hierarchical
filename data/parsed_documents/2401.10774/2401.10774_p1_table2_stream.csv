"Tianle Cai * 1 2 Yuhong Li * 3 Zhengyang Geng 4 Hongwu Peng 5
Jason D. Lee 1 Deming Chen 3 Tri Dao 1 2"
"MEDUSA on models of various sizes and train-
Abstract"
"ing procedures.
Our experiments demonstrate"
"Large Language Models (LLMs) employ auto-"
"that MEDUSA-1 can achieve over 2.2× speedup"
"regressive decoding that requires sequential com-"
"without compromising generation quality, while"
"putation, with each step reliant on the previous"
"MEDUSA-2 further improves the speedup to 2.3-"
"one’s output. This creates a bottleneck as each"
"2.8×."
"step necessitates moving the full model param-"
"eters from High-Bandwidth Memory (HBM) to"
"the accelerator’s cache. While methods such as"
"1. Introduction"
"speculative decoding have been suggested to ad-"
"dress this issue, their implementation is impeded"
"The
recent
advancements
in Large Language Models"
"by the challenges associated with acquiring and"
"(LLMs) have demonstrated that
the quality of
language"
"maintaining a separate draft model.
In this pa-"
"generation significantly improves with an increase in model"
"per, we present MEDUSA, an efficient method"
"size, reaching billions of parameters (Brown et al., 2020;"
"that augments LLM inference by adding extra"
"Chowdhery et al., 2022; Zhang et al., 2022; Hoffmann et al.,"
"decoding heads to predict multiple subsequent"
"2022; OpenAI, 2023; Google, 2023; Touvron et al., 2023)."
"tokens in parallel. Using a tree-based attention"
"However,
this growth has led to an increase in inference"
"mechanism, MEDUSA constructs multiple can-"
"latency, which poses a significant challenge in practical ap-"
"didate continuations and verifies them simulta-"
"plications. From a system perspective, LLM inference is"
"neously in each decoding step.
By leveraging"
"predominantly memory-bandwidth-bound (Shazeer, 2019;"
"parallel processing, MEDUSA substantially re-"
"Kim et al., 2023), with the main latency bottleneck stem-"
"duces the number of decoding steps required. We"
"ming from accelerators’ memory bandwidth rather
than"
"present
two levels of fine-tuning procedures for"
"arithmetic computations.
This bottleneck is inherent
to"
"MEDUSA to meet the needs of different use cases:"
"the sequential nature of auto-regressive decoding, where"
"MEDUSA-1: MEDUSA is directly fine-tuned on"
"each forward pass requires transferring the complete model"
"top of a frozen backbone LLM, enabling lossless"
"parameters from High-Bandwidth Memory (HBM) to the"
"inference acceleration. MEDUSA-2: MEDUSA"
"accelerator’s cache. This process, which generates only a"
"is fine-tuned together with the backbone LLM,"
"single token, underutilizes the arithmetic computation po-"
"enabling better prediction accuracy of MEDUSA"
"tential of modern accelerators, leading to inefficiency."
"heads and higher speedup but needing a special"
"To address this, one approach to speed up LLM inference
training recipe that preserves the model’s capabil-"
"involves increasing the arithmetic intensity (the ratio of total
ities. Moreover, we propose several extensions"
"floating-point operations (FLOPs) to total data movement)
that
improve or expand the utility of MEDUSA,"
"of the decoding process and reducing the number of decod-
including a self-distillation to handle situations"
"ing steps.
In line with this idea, speculative decoding has
where no training data is available and a typical"
"been proposed (Leviathan et al., 2022; Chen et al., 2023;
acceptance scheme to boost the acceptance rate"
"Xia et al., 2023; Miao et al., 2023). This method uses a
while maintaining generation quality. We evaluate"
"smaller draft model to generate a token sequence, which is"
"*Equal
contribution
1Princeton University
2Together AI"
"then refined by the original, larger model for acceptable con-"
"3University of Illinois Urbana-Champaign 4Carnegie Mellon Uni-"
"tinuation. However, obtaining an appropriate draft model"
"versity 5University of Connecticut. Correspondence to: Tianle Cai"
"remains challenging, and it’s even harder to integrate the
<tianle.cai@princeton.edu>, Yuhong Li <leeyh@illinois.edu>."
"draft model into a distributed system (Chen et al., 2023)."
"Proceedings of
International Conference on Machine
the 41 st"
"Instead of using a separate draft model to sequentially gen-
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by"
"the author(s).
erate candidate outputs,
in this paper, we revisit and re-"
