"","Specifically, given x1, x2, · · ·
, xn as context, when eval-"
"warmup strategy by gradually increasing the weight λ0",""
"","uating the candidate sequence (xn+1, xn+2, · · ·
, xn+K+1)"
"of the backbone model’s loss. We find both strategies",""
"","(composed by top predictions of the original language model"
"work well in practice.",""
"","head and MEDUSA heads), we consider the condition"
"Putting these strategies together, we can train MEDUSA",""
"heads together with the backbone model without hurting",", xn+k−1) >
poriginal(xn+k|x1, x2, · · ·"
"the backbone model’s capability. Moreover, this recipe can",""
"",", xn+k−1)))) ,
min (ϵ, δ exp (−H(poriginal(·|x1, x2, · · ·"
"be applied together with Supervised Fine-Tuning (SFT),",""
"enabling us to get a model with native MEDUSA support.",""
"","where H(·) denotes the entropy function, and ϵ, δ are the"
"","hard threshold and the entropy-dependent threshold respec-"
"2.2.3. HOW TO SELECT THE NUMBER OF HEADS",""
"","tively. This criterion is adapted from Hewitt et al. (2022)"
"Empirically, we found that five heads are sufficient at most.",""
"","and rests on two observations:
(1) tokens with relatively"
"Therefore, we recommend training with five heads and refer-",""
"","high probability are meaningful, and (2) when the distribu-"
"ring to the strategy described in Section 2.3.3 to determine",""
"","tion’s entropy is high, various continuations may be deemed"
"the optimal configuration of the tree attention. With opti-",""
"","reasonable. During decoding, every candidate is evaluated"
"mized tree attention, sometimes three or four heads may",""
"","using this criterion, and a prefix of the candidate is accepted"
"be enough for inference.
In this case, we can ignore the",""
"","if it satisfies the condition. To guarantee the generation of"
"redundant heads without overhead.",""
"","at least one token at each step, we apply greedy decoding"
"","for the first token and unconditionally accept it while em-"
"2.3. Extensions","ploying typical acceptance for subsequent tokens. The final"
"","prediction for the current step is determined by the longest"
"2.3.1. TYPICAL ACCEPTANCE",""
"","accepted prefix among all candidates."
"In speculative decoding papers (Leviathan et al., 2022; Chen",""
"","Examining this scheme leads to several
insights.
Firstly,"
"et al., 2023), authors employ rejection sampling to yield di-",""
"","when the temperature is set to 0, it reverts to greedy decod-"
"verse outputs that align with the distribution of the original",""
"","ing, as only the most probable token possesses non-zero"
"model. However, subsequent implementations (Joao Gante,",""
"","probability. As the temperature surpasses 0,
the outcome"
"2023; Spector & Re, 2023) reveal that this sampling strategy",""
"","of greedy decoding will consistently be accepted with ap-"
"results in diminished efficiency as the sampling tempera-",""
"","propriate ϵ, δ, since those tokens have the maximum prob-"
"ture increases. Intuitively, this can be comprehended in the",""
"","ability, yielding maximal speedup. Likewise,
in general"
"extreme instance where the draft model is the same as the",""
"","scenarios, an increased temperature will correspondingly"
"original one: Using greedy decoding, all output of the draft",""
"","result in longer accepted sequences, as corroborated by our"
"model will be accepted, therefore maximizing the efficiency.",""
"","experimental findings."
"Conversely, rejection sampling introduces extra overhead,",""
"as the draft model and the original model are sampled in-","Empirically, we verify that typical acceptance can achieve"
"dependently. Even if their distributions align perfectly, the","a better speedup while maintaining a similar generation"
"output of the draft model may still be rejected.","quality as shown in Figure 5."
