"append-only characteristic of KV cache is exploited to enable","addresses the heterogeneous sensitivity to packing/spreading"
"migration capability of requests in the inference engine. Such","of different jobs with introspective job migration. This chal-"
"a mechanism opens great policy design space to offer prior-","lenge becomes more complex for LLM serving due to the"
"ity and performance isolation, improve memory efficiency,","unpredictable autoregressive execution. Llumnix exploits re-"
"and enable instance auto-scaling. We also plan to explore","quest migration at runtime to react to the workload dynamics"
"the interplay between the global scheduling across instances","to better reconcile these two goals."
"with local scheduling techniques inside each instance (e.g.,","Migration.
Gandiva [64] enables introspective migration"
"preemptive [63] and fair [55] scheduling) as future works.","for deep learning training jobs during scheduling. It utilizes"
"Request scheduling.
To support deep learning model de-","the inherent iterative behavior of deep learning, and conducts"
"ployment, numerous systems (e.g., Clipper [19], Nexus [54],","checkpoint-resume approach on the minimal working set (i.e.,"
"DVABatch [20], and TritonServer [47]) have been proposed","mini-batch boundary) to migrate model weights. Even though"
"to optimize request scheduling for DNN inference serving. To","LLM inference is iterative as well, directly migrating the"
"meet the SLOs of DNN inference requests, Clockwork [29]","entire states of a request is unacceptable, because the latency"
"utilizes the execution predictability of traditional DNNs, while","SLO of an inference request is crucial. Moreover, the working"
"Reef [33] and Shepherd [68] perform preemptions to serve","set per request
is linear to the sequence length, which can"
"high-priority requests. AlpaServe [35] uses a simple load-","be considerable given the trend of longer contexts [49, 50]."
"balancing dispatching policy based on queue lengths. These","The migration approach in Llumnix is inspired by virtual"
"works mostly focus on traditional DNN model serving, where","machine live migration [17]. By carrying out the majority of"
"a request requires only one-time inference on the model. How-","migration while LLM requests continue decoding tokens on"
"ever, LLM inference service requires autoregressive com-","GPUs, Llumnix minimizes the downtime of request migration,"
"putation on models for unpredictable numbers of iterations","making the cost negligible regardless of the sequence lengths."
"and introduces intermediate states (i.e., KV cache), showing",""
"brand new characteristics. DeepSpeed-MII [4], albeit target-",""
"","8
Conclusion"
"ing multi-instance LLM serving, uses a simple round-robin",""
"dispatching policy that ignores LLM characteristics. Llumnix",""
"","Llumnix, as implied by the name, represents our vision of"
"steps further to incorporate request migration and ensures",""
"","serving LLMs as Unix. This vision originates in the observa-"
"high throughput and low latency, provides SLO for prioritized",""
"","tion that LLMs and modern operating systems have common"
"requests, and auto-scales instances for
resource efficiency",""
"","natures such as the universality, multi-tenancy, and dynamism,"
"with a unified load-aware dynamic scheduling policy.",""
"","and hence share similar requirements and challenges. This"
"Beyond multiple model instances, INFaaS [53] further sup-",""
"","paper takes an important step towards this vision by draw-"
"ports scheduling across multiple model types/variants, con-",""
"","ing lessons from conventional OS wisdom including: defini-"
"sidering the performance and accuracy requirements in differ-",""
"","tion of classic abstractions like isolation and priorities in the"
"ence applications. This is also a typical scenario for LLMs:",""
"","new context of LLM serving;
implementation of the “con-"
"for example, fine-tuned models for a specific task (e.g., cod-",""
"","text switching” as the key approach with inference request"
"ing [3, 13, 30]); variants with different sizes or precisions",""
"","migration; and continuous, dynamic request rescheduling ex-"
"( [26, 37, 39]) of the base LLM. We plan to extend Llumnix to",""
"","ploiting the migration. All these combined, Llumnix delivers"
"support multiple model types in future work, considering the",""
"","better latency, cost efficiency, and support for differentiated"
"larger tradeoff space of latency/throughput and accuracy.",""
"","SLOs, pointing to a new way of LLM serving."
"Isolation vs.
fragmentation.
The tradeoff between iso-",""
"lation and fragmentation, or
that between workload pack-",""
"ing and spreading, have been a classic scheduling challenge.","Acknowledgement"
"That is, workload packing improves resource utilization, at",""
"the
expense of potential
interference between co-located","We thank the anonymous OSDI reviewers and our shepherd"
"workloads; spreading workloads, on the contrary, provides","for their valuable feedback, which helped improve the pre-"
"better
isolation but also increases
resource fragmentation.","sentation of
this paper. We thank Shiru Ren for early dis-"
"Many research efforts have been devoted to better balanc-","cussion on VM live migration techniques. This work was"
"ing isolation and fragmentation in datacenters for big-data","supported by Alibaba Group through the Alibaba Research"
"jobs and virtual machines, by identifying the interference-","Intern Program. This work was also supported by National"
"sensitivity of workloads and optimized scheduling policies","Key Research and Development Program of China (Grant"
"( [16,18,23,24,27,31,32,40,60,66]). This challenge was also","No. 2023YFB3001801), National Natural Science Founda-"
"identified in GPU clusters for deep learning workloads. Ama-","tion of China (Grant No. 62322201, 62072018, U23B2020"
"ral et al proposed a topology-aware placement algorithm to ad-","and U22A2028), Fundamental Research Funds for the Central"
"dress the tradeoff between packing and spreading deep learn-","Universities (YWF-23-L-1121), and State Key Laboratory of"
"ing training jobs on multi-GPU servers [12]. Gandiva [64]","Software Development Environment (SKLSDE-2023ZX-05)."
