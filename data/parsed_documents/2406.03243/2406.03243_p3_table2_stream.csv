"Figure 2: Request queuing and preemption using continuous",""
"","Figure 3: Request preemptions in LLaMA-7B serving."
"batching and dynamic memory allocation.",""
"","value tensors used in the attention operation [59]) for each"
"2
Background",""
"","token are involved in the generation of all following tokens."
"","Therefore, the inference engine typically stores these states"
"Application diversity of LLMs.
Recent LLMs are becom-",""
"","in GPU memory for reuse, known as the KV cache [52]."
"ing task-agnostic. That is, the same model can work for vari-",""
"","Batching and memory management.
State-of-the-art infer-"
"ous tasks with context-specific inputs (a.k.a. the “prompts”)",""
"","ence engines apply the continuous batching technique [34,67]"
"provided. This is achieved by both increasingly larger model",""
"","to handle the varying sequence lengths and dynamic arrivals"
"and dataset sizes and advanced pre-training approaches such",""
"","of requests. That is, a new/completed request can join/leave"
"as few-shot
learning [14]. Task-agnostic models enable di-",""
"","the running batch immediately, instead of waiting for all the"
"verse applications, from chatbots, search engines, summariza-",""
"","running requests to complete. Batching also raises concern"
"tion, coding, AI assistants, to AI agents, to name a few.",""
"","about memory management of KV cache. Since the memory"
"The diverse applications lead to requests with different",""
"","demand of KV cache is not known a priori, it would clearly"
"requirements for the serving. An important aspect is the se-",""
"","limit
the batch size and batching benefits if the memory is"
"quence lengths. LLMs are racing to support longer sequence",""
"","reserved to the maximum length. For example, a LLaMA-2-"
"lengths — for example, from March to November 2023, the",""
"","13B [58] model supports sequence lengths up to 4k, which"
"maximum sequence lengths of the GPT family have scaled",""
"","translates to 3.2 GB KV cache for a single request; while the"
"from 32k 1 (GPT-4 [49]) to 128k (GPT-4 Turbo [50]). We",""
"","memory of current GPUs remain tens of GBs, let alone the"
"expect this trend to continue as longer sequences are neces-",""
"","space for model weights (26 GB for LLaMA-2-13B). There-"
"sary for broader applications of LLMs. Consider an intuitive",""
"","fore, recent work (vLLM [34]) proposed dynamic memory"
"example of the tasks for summarizing and writing an article:",""
"","allocation for KV cache to increase batch size and throughput,"
"they require sufficiently long input and output lengths, respec-",""
"","enabled by a technique named PagedAttention: the KV cache"
"tively. Another aspect is expected latencies. A real product",""
"","tensors are stored in dynamically allocated blocks as the KV"
"example is that OpenAI introduces a subscription plan called",""
"","cache grows. Figure 2 presents an example of using continu-"
"ChatGPT Plus [2] to offer faster responses of common Chat-",""
"","ous batching with dynamic memory allocation. The running"
"GPT services. In general, different applications and situations",""
"","requests are chosen based on the free memory blocks, hence"
"also naturally have different levels of urgency. For example,",""
"","there is a queuing request (the gray one) at iteration N as the"
"more interactive applications like personal assistants expect",""
"","memory is insufficient. At the next iteration, the system runs"
"shorter latencies than tasks like summarizing an article.",""
"","out of memory for the new blocks of the running requests."
"Autoregressive generation.
The inference for state-of-the-",""
"","Therefore, the system preempts certain running requests (the"
"art LLMs is autoregressive: the model iteratively accepts the",""
"","blue one), which then goes back to the queue."
