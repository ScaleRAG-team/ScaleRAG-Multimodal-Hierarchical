"Figure 13: Performance of high-priority and normal requests, as annotated on the Y-axis labels.",""
"2×, through migration to reduce preemptions. Although IN-","giving lower instance loads and interference to high execution"
"FaaS++ already implements load balancing in dispatching to","priorities, shown by the similar gains in the average decode"
"reduce preemptions, migration complements it by reacting","computation time (the rightmost column). We also notice"
"to the real sequence lengths, which are unknown at request","that Llumnix preserves similar performance of the normal"
"arrivals. As shown in Figure 11, Llumnix significantly re-","requests (the second row in Figure 13): Llumnix increases the"
"duces the preemption loss, in many cases down to near zero.","mean request, prefill, and decode latencies of normal requests"
"The reduction is 70.4% on average across all experiments,","by up to 4.5%, 13%, and 2%, respectively."
"which translates to an average reduction of 1.3 seconds in the",""
"end-to-end request latency.",""
"","6.5
Auto-scaling"
"","We evaluate the auto-scaling capability of Llumnix using"
"6.4
Support for Priorities",""
"","larger ranges of request rates and Gamma CVs to show the"
"We evaluate the support for priorities of Llumnix by randomly","adaptivity to load variation. By default, Llumnix uses a scaling"
"picking 10% of the requests and assigning high scheduling","threshold range of [10, 60], i.e., Llumnix scales instances up"
"and execution priorities. We use traces with the Short-Short","or down when the average freeness is under 10 or above"
"length distribution and Gamma arrival distribution. We vary","60; recall
that
this metric represents the most decode steps"
"the CV parameter to show the interference to high-priority","an instance can still run for given the current batch. We let"
"requests due to bursty workloads and load spikes. We em-","INFaaS++ use the same scaling strategy, thus both Llumnix"
"pirically choose a target memory load of 1,600 tokens for","and INFaaS++ have the same degree of aggressiveness of"
"high-priority requests, as we observe that such load preserves","scaling up instances. We use a maximum instance number of"
"near-ideal decode speed (refer to Figure 4). Llumnix translates","16 and the Long-Long sequence length distribution."
"this target load to the corresponding memory headroom for","We first vary the request rates using Poisson distribution."
"high-priority requests. We compare Llumnix with Llumnix-","As shown in the first row of Figure 14, Llumnix consistently"
"base, which simply treats all requests as the same priority.","achieves latency improvements across all request rates, e.g.,"
"As shown in the first row in Figure 13, Llumnix improves","up to 12.2× for P99 prefill
latency. We also measure the"
"mean request latencies for the high-priority by 1.2× to 1.5×","resource cost in terms of average instances used, shown in the"
"with increasing CVs. Higher CVs leads to more high-load","rightmost column. Llumnix saves costs by up to 16%, because"
"periods, where high-priority requests can suffer more inter-","Llumnix increases the auto-scaling efficiency by saturating"
"ference if not protected. Even with higher CVs, Llumnix still","or draining out instances more quickly. We also test different"
"delivers similar latencies of high-priority requests, showing","workload burstiness with varying CVs of Gamma distribution"
"the isolation Llumnix provides to such requests. This is be-","(request
rate = 2). As shown in the second row, Llumnix"
"cause Llumnix can handle changing high-priority loads by","shows similar improvements in latencies and costs, e.g., up to"
"dynamically creating space for them, which is difficult in ap-","11× for P99 prefill latency and 18% for the cost."
"proaches like static resource reservation. For prefill latencies,","Finally, we examine the cost efficiency of Llumnix in terms"
"Llumnix shows 2.9× to 8.6× gains for the mean, and 3.6× to","of how aggressively Llumnix needs to scale out instances to"
"10× for the P99, respectively. This is achieved by reducing","preserve a certain latency objective, e.g., a given P99 prefill"
"the queuing delays with high scheduling priorities. Llumnix","latency. We vary the scaling up threshold t, and the scaling"
"also improves decode latencies by 1.2× to 1.5× for the mean","threshold range is determined as [t, t+50]. Higher values of t"
"and 1.3× to 2.2× for the P99, respectively. This improvement","means that Llumnix tends to use more instances. Figure 15"
"comes from the acceleration of the decode computation by","shows the P99 prefill latencies and costs with different scaling"
