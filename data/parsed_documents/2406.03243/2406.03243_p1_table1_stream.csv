"Abstract","Inference
serving of LLMs plays
a key role
in LLM-"
"","powered services, becoming a critical workload in datacenters."
"Inference serving for large language models (LLMs) is the key",""
"","Such services are typically backed by multiple instances of"
"to unleashing their potential in people’s daily lives. However,",""
"","the LLM deployed on a GPU cluster. The system involves"
"efficient LLM serving remains challenging today because the",""
"","a scheduler and an inference engine, where a request is first"
"requests are inherently heterogeneous and unpredictable in",""
"","dispatched by the scheduler to a model serving instance, then"
"terms of
resource and latency requirements, as a result of",""
"","gets executed by the inference engine inside. The requests are"
"the diverse applications and the dynamic execution nature of",""
"","typically batched for execution on each instance to increase"
"LLMs. Existing systems are fundamentally limited in han-",""
"","throughput and cost efficiency."
"dling these characteristics and cause problems such as severe",""
"","We observe unique characteristics of LLMs that call for"
"queuing delays, poor tail latencies, and SLO violations.",""
"","new design philosophy of the serving infrastructure. The first"
"We introduce Llumnix, an LLM serving system that re-",""
"","is workload heterogeneity. LLMs are designed to be universal,"
"acts to such heterogeneous and unpredictable requests by",""
"","by learning as much knowledge as possible from whatever"
"runtime rescheduling across multiple model instances. Sim-",""
"","domains. People can query the same LLM in totally different"
"ilar to context switching across CPU cores in modern op-",""
"","situations or even build custom applications atop LLMs for"
"erating systems, Llumnix reschedules requests to improve",""
"","various scenarios; for all of these, a context-specific input (i.e.,"
"load balancing and isolation, mitigate resource fragmenta-",""
"","prompt) is all you need [15]. Such universality and application"
"tion, and differentiate request priorities and SLOs. Llumnix",""
"","diversity lead to heterogeneity of the inference requests, in"
"implements the rescheduling with an efficient and scalable",""
"","terms of input lengths, output lengths, expected latencies, etc."
"live migration mechanism for requests and their in-memory",""
"","For instance, the task of summarizing long text can introduce"
"states, and exploits it
in a dynamic scheduling policy that",""
"","significant input lengths, where the latency of returning the"
"unifies the multiple rescheduling scenarios elegantly. Our",""
"","first token (word) is often important to user experience [38]."
"evaluations
show that Llumnix improves
tail
latencies by",""
"","The second characteristic is execution unpredictability."
"an order of magnitude, accelerates high-priority requests",""
"","Serving an LLM request needs to run the model for multiple"
"by up to 1.5×, and delivers up to 36% cost savings while",""
"","iterations, each producing a single output token; however, it"
"achieving similar tail latencies, compared against state-of-the-",""
"","is not known a priori how many tokens will be generated"
"art LLM serving systems. Llumnix is publicly available at",""
"","eventually. Moreover, the iterative generation also brings con-"
"https://github.com/AlibabaPAI/llumnix.",""
"","siderable GPU memory consumption that dynamically grows"
"","with the tokens. As such, the execution time and the resource"
"1
Introduction","demand of a request are both unpredictable."
"","These characteristics make an LLM inherently a multi-"
"Large language models (LLMs) such as the GPT series [15,",""
"","tenant and dynamic environment, serving heterogeneous and"
"49] are bringing generative AI
to an unprecedented level.",""
"","unpredictable workloads on multiple instances. This behav-"
"Their human-level generation capabilities are being quickly",""
"","ior is fundamentally different from traditional DNN models,"
"adopted in a wide range of domains, inspiring many imagina-",""
"","where the requests are homogeneous and the execution is"
"tions for future applications, and expected to have profound",""
"","one-shot, stateless, and deterministic. Instead, we find LLMs"
"influences on how people live and work.",""
"","more similar to modern operating systems hosting processes"
"∗Equal contribution.","with dynamic working sets and different priorities on multiple"
"†Work done during internship at Alibaba Group.","cores. Managing such systems has complex goals, which goes"
