"","Downtime"
"4
Llumnix Design","Source instance"
"","Legend"
"","Stage-N
Stage-0
Stage-1
Decoding"
"","computation"
"","…
…
Compute"
"",""
"4.1
Overview","Generated 
KV cache"
"","…
Mem
Copy KV cache"
"",""
"Llumnix builds upon the key idea of rescheduling LLM in-","Time"
"","Migration initiated"
"ference requests at runtime across model instances. Llumnix",""
"","…
…"
"inherits continuous batching [67] and dynamic memory allo-",""
"","Compute"
"cation [34] from state-of-the-art systems for high throughput.",""
"","…
Mem"
"Beyond that, Llumnix exploits request rescheduling to react",""
"","Time"
"to the unpredictable workload dynamics in various situations","Migration completed
Destination instance"
"with different scheduling goals, as illustrated in Figure 1.",""
"A first goal
is load balancing (Figure 1-a) to reduce re-","Figure 6: Llumnix adopts multi-stage migration to overlap the"
"quest preemptions and interference on high-load instances.","computation and KV cache copying for minimal downtime."
"Although the dispatching can also consider load balancing of",""
"memory usage, it could be sub-optimal as the final memory",""
"usages of requests are unknown at the arrivals, due to the un-","4.2
Live Migration of LLM Requests"
"predictability of output lengths. Rescheduling complements it",""
"by reacting to the real usage growths of requests. Meanwhile,","The significant KV cache states of requests can potentially"
"as shown before, load balancing can also lead to higher mem-","introduce great cost and serving stalls during rescheduling."
"ory fragmentation and longer queuing delays of long inputs","Llumnix addresses this challenge by exploiting a key char-"
"probably. Therefore, Llumnix also reschedules requests for","acteristic of LLM inference:
the KV cache is append-only."
"de-fragmentation (1-b), i.e., creating contiguous space on an","LLM inference iteratively concatenates the output token of"
"instance by moving requests onto others. Although these two","the current
iteration with the input
tokens, which is set as"
"goals remain a tradeoff, Llumnix has a much larger space","the input for the next iteration. In this way, inference engines"
"to balance them with rescheduling. Another goal is prioriti-","also keep appending the calculated KV state of the current"
"zation (1-c) of certain requests by rescheduling co-located","iteration to the KV cache parameters, leaving the parameters"
"requests away for lower load and avoiding interference. Such","generated by previous iterations remain constant."
"rescheduling provides “decicated” resources to high-priority",""
"","The live migration mechanism of Llumnix utilizes the in-"
"requests dynamically, without the need for reserving machines",""
"","herent append-only characteristic of KV cache to pipeline the"
"statically. Finally, Llumnix also reschedules requests during",""
"","KV cache copying with the decoding computation. Because"
"auto-scaling, e.g., to drain out an instance to be terminated",""
"","the KV cache already generated won’t be modified in the"
"(1-d) or saturate a new instance more quickly.",""
"","following iterations, Llumnix can safely copy the KV cache"
"Realizing such highly dynamic rescheduling efficiently is","of previous tokens in parallel with the computation for new"
"challenging, considering the large request context states (i.e.,","tokens. In this way, Llumnix achieves near-zero and constant"
"the KV cache). Naïve solutions include recomputing or copy-","downtime to the rescheduled request. As shown in Figure 6,"
"ing the KV cache of the rescheduled requests, however with","when migration is initiated, the source instance starts to copy"
"high computation stalls and downtime, reaching over 50× of","the KV cache blocks of completed iterations, and continues"
"the decoding cost (§6.2). What’s more, the KV cache states","the computation at the same time (stage 0). When the copying"
"increase with sequence lengths, limiting the scheduling flexi-","for the previous KV cache blocks is done, there will be a few"
"bility under the trend of growing context lengths [50]. Such","more iterations (i.e., blocks in Figure 6) computed in stage 0."
"a high inference delay in generating next tokens greatly de-","Then, it switches to stage 1 to copy the KV cache generated"
"grades the user experiences of LLM serving and thus prohibits","by stage 0, while continuing the computation afterwards. The"
"request rescheduling. Llumnix addresses this challenge with","copying is generally much faster than the computation, thus"
"a live migration mechanism that pipelines and coordinates","the number of new blocks is typically small such that we can"
"the KV cache copying and the token generation computation,","copy them in a very short period. To the end, only one itera-"
"thereby bringing negligible downtime (§4.2).","tion of computation is conducted for the KV cache migration"
"To exploit the benefits of migration, Llumnix adopts a scal-","(i.e., stage-N). Therefore, Llumnix suspends the computation"
"able architecture that combines global and local scheduling to","for the request by draining it out of the current batch and"
"decentralize the scheduling decisions and the coordinated mi-","copies the remaining block, which introduces the downtime"
"gration actions, facilitating continuous rescheduling at scale","of this request. Once it is finished, the migration completes"
"(§4.3). Under this architecture, we further design an efficient","and the request resumes on the destination instance. Although"
"heuristic scheduling policy that centers around the virtual","the total copying duration of the whole sequence depends on"
"usage concept
to abstract
the requirements of the different","the sequence length, the downtime for the request is only the"
"scheduling goals in a unified manner (§4.4).","period of copying the KV cache generated by one iteration,"
