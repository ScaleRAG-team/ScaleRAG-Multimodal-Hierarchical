"ABSTRACT"
"While the scaling laws of large language models (LLMs) training have been exten-"
"sively studied, optimal
inference configurations of LLMs remain underexplored."
"We study inference scaling laws (aka test-time scaling laws) and compute-optimal"
"inference,
focusing on the trade-offs between model
sizes and generating ad-"
"ditional
tokens with different
inference strategies.
As a first
step towards un-"
"derstanding and designing compute-optimal
inference methods, we studied cost-"
"performance trade-offs for
inference strategies such as greedy search, majority"
"voting, best-of-n, weighted voting, and two different
tree search algorithms, us-"
"ing different model sizes and compute budgets. Our findings suggest
that scal-"
"ing inference compute with inference strategies can be more computationally ef-"
"ficient
than scaling model parameters. Additionally,
smaller models combined"
"with advanced inference algorithms offer Pareto-optimal
trade-offs in cost and"
"performance. For example,
the Llemma-7B model, when paired with our novel"
"tree search algorithm, consistently outperforms the Llemma-34B model across all"
"tested inference strategies on the MATH benchmark. We hope these insights con-"
"tribute to a deeper understanding of inference scaling laws (test-time scaling laws)"
"for LLMs."
