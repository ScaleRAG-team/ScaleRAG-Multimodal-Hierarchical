"Published as a conference paper at ICLR 2025"
"In our experiments, REBASE consistently outperforms sampling and MCTS methods across all set-"
"tings, models, and tasks.
Importantly, we find that REBASE with a smaller language model
typi-"
"cally achieves a Pareto-optimal
trade-off.
For
instance, we show that
the Llemma-7B model can"
"achieve competitive accuracy to a Llemma-34B model while using 2× less FLOPs when evaluat-"
"ing on MATH500 (Fig. 4) or GSM8K (Fig. 5). Moreover, Llemma-7B with REBASE outperforms"
"Llemma-34B with standard majority voting across all compute budgets. Our results show the value"
"of using smaller models with advanced inference-time algorithms, and the benefits of new algo-"
"rithms for achieving better returns on inference-time compute."
"Our contributions are summarized as follows:"
"• We explore new inference scaling laws and compute-optimal
inference by evaluating the"
"performance of various model sizes under a fixed inference strategy. We show that smaller"
"models can outperform larger ones under the same compute budget by increasing the num-"
"ber of samples."
"• We provide new theoretical analysis of the scaling behavior of voting methods, presenting"
"convergence bounds and rates. Our analysis shows performance limits and diminishing"
"returns from sampling, pointing to the need for more sophisticated inference algorithms."
"• We formulate a new compute-optimal
inference problem and propose a novel
tree search"
"algorithm, REBASE, which is compute-optimal compared to widely-used sampling and"
"MCTS methods. Our results show benefits of using smaller models with advanced infer-"
"ence algorithms, and new algorithms for achieving better cost-performance tradeoffs."
