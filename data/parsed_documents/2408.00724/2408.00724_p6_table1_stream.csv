"Figure 3: Illustration of one iteration of REward BAlanced SEarch (REBASE)."
"3.1.1
MONTE CARLO TREE SEARCH (MCTS)"
"Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where"
"strategic decision-making is required (Silver et al., 2016; 2017; Jones, 2021).
Recent work has"
"shown that adapting MCTS to the context of LLMs can enhance the text generation process (Zhang"
"et al., 2023; Zhou et al., 2024; Liu et al., 2024; Choi et al., 2023; Chen et al., 2024a; Tian et al.,"
"2024; Chen et al., 2024a).
In this context, MCTS is paired with a value model
to score and guide"
"the exploration steps. For additional background, we provide a review of MCTS in Appendix B."
"Recent work in MCTS or its variants mainly focus on improving the performance (e.g., accuracy) on"
"the studied tasks. However, generic comparisons of MCTS with conventional methods like best-of-n"
"and majority voting in terms of computational budget, measured in generated tokens or processing"
"time are scarce or indicate potentially unfavorable cost-performance tradeoffs. For example, MCTS"
"consumes substantially more resources, often requiring dozens of times more generated tokens than"
"simpler methods. Specifically, a significant portion of the paths in the search tree are used to estimate"
"and select nodes, and these paths do not necessarily become a part of the final candidate solution,"
"although MCTS ensures that
the sampled solutions comprise high-quality intermediate steps.
In"
"contrast, sampling methods generate multiple solutions in parallel and independently, and all
the"
"generated sequences are included in the candidate solutions. However,
the intermediate steps in"
"these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor"
"steps or exploiting promising ones."
"This highlights the need for a new tree search method that can achieve a comparable (or better) per-"
"formance as MCTS, and that is computationally less costly, with a cost similar to weighted majority"
"voting and best-of-n. This motivates our new method, Reward Balanced SEarch (REBASE)."
"3.1.2
REWARD BALANCED SEARCH (REBASE)"
"The REBASE tree search method, illustrated in Fig. 3, inherits the exploitation and pruning proper-"
"ties of tree search, while using a reward model alone to estimate quality of intermediate nodes. This"
"saves inference compute compared to methods such as MCTS, since it does not
involve estimate"
"node quality with explicit rollouts. In short, the underlying idea is to use a process reward model to"
"determine how much each node should be expanded at each depth. Namely, REBASE expands nodes"
"at a given depth according to their softmax-normalized reward scores, subject
to a total expansion"
"budget. We describe this procedure in more detail below."
"Notations. We view the fine-tuned LLM as a policy πθ which generates the solution step by step."
"the (k + 1)-th step is sampled from
Given a question x and the first k steps of a solution r1 · · · rk,"
"in which the root node is the
πθ(·|xr1 · · · rk). REBASE generates a solution tree during inference,"
"question x, and other nodes corresponds
to solution steps. When generating solution trees, we"
"generate children of rk by sampling from πθ(·|xr1 · · · rk). We use the corresponding solution step"
"to denote a node. The reward of a node rk is generated by the PRM: R(rk) := R(xr1 · · · rk)."
