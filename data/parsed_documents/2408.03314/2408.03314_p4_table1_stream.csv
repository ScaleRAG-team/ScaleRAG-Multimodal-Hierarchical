"by augmenting the given prompt with an additional set of tokens that the LLM conditions on to obtain"
"the modified distribution, or (2) at the output level: by sampling multiple candidates from the standard"
"LM and performing surgery on these candidates. In other words, we could either modify the proposal"
"distribution induced by the LLM itself such that it is an improvement over naïvely conditioning on the"
"prompt or we could use some post-hoc verifiers or scorers to perform output modifications. This process"
"is reminiscent of Markov chain Monte Carlo (MCMC) [2] sampling from a complex target distribution but"
"by combining a simple proposal distribution and a score function. Modifying the proposal distribution"
"directly by altering input tokens and using a verifier form two independent axes of our study."
"Modifying the proposal distribution. One way to improve the proposal distribution is to directly optimize"
"the model for a given reasoning task via RL-inspired finetuning methods such as STaR or ReSTEM [35, 50]."
"Note that these techniques do not utilize any additional input tokens but specifically finetune the model to"
"induce an improved proposal distribution.
Instead, techniques such as self-critique [4, 8, 23, 30] enable"
"the model
itself to improve its own proposal distribution at test time by instructing it to critique and"
"revise its own outputs in an iterative fashion. Since prompting off-the-shelf models is not effective at"
"enabling effective revisions at test time, we specifically finetune models to iteratively revise their answers"
"in complex reasoning-based settings. To do so, we utilize the approach of finetuning on on-policy data"
"with Best-of-N guided improvements to the model response [28]."
"In our abstraction of the proposal distribution and verifier, the verifier is used to"
"Optimizing the verifier."
"aggregate or select the best answer from the proposal distribution. The most canonical way to use such"
"a verifier is by applying best-of-N sampling, wherein we sample N complete solutions and then select"
"the best one according to a verifier [7]. However, this approach can be further improved by training"
"a process-based verifier [22], or a process reward model (PRM), which produces a prediction of the"
"correctness of each intermediate step in an solution, rather than just the final answer. We can then utilize"
"these per-step predictions to perform tree search over the space of solutions, enabling a potentially more"
"efficient and effective way to search against a verifier, compared to naïve best-of-N [6, 10, 48]."
"3. How to Scale Test-Time Computation Optimally"
"Given the unification of various methods, we would now like to understand how to most effectively utilize"
"test-time computation to improve LM performance on a given prompt. Concretely we wish to answer:"
