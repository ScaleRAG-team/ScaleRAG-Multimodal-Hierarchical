"answer which is somewhat correlated with the correct answer,
to improve learning. The remaining"
"incorrect answers, we sample randomly from the set of available answers. In the case where there are"
"fewer than 4 incorrect answers sampled, we truncate the uniform distribution’s max to match the number"
"of incorrect samples. We use this procedure to generate trajectories for all questions in the training data."
"We then finetune the base language model on the correct answer solutions in these generated trajectories."
"We use the AdamW optimizer with lr 1e-5, batch size 128, dropout 0.0, and Adam betas (0.9, 0.95)."
"We find that generally evaluating loss on an evaluation set consisting of trajectories generated as described"
"above, does not provide a good signal for early stopping. Rather, we find that checkpoints much after"
"the evaluation loss begins increasing are much more capable of revisions. This is likely because after"
"finetuning the revision model, the evaluation set represents off-policy data, which will naturally be out-of-"
"distribution compared to the trajectires that the model itself would generate on-policy. We therefore select"
"our revision model checkpoint slightly after the point where we observe overfitting on the validation set."
"I. Revision Model Selection Criteria"
"As described in Section 6.1,
in order to effective use our revision model we need to deploy a criteria for"
"selecting the best answer both within a revision trajectory and between multiple parallel trajectories. We"
"use two approaches: 1) ORM verifier; and 2) majority voting."
"For the ORM verifier, we train an ORM on the revision model’s outputs according to the procedure in"
"Appendix J. At inference, time we then use this verifier to select the best answer. Since we have two axes"
"across which to aggregate (within each revision trajectories and between multiple trajectories), we deploy"
"a hierarchical strategy, first selecting the best answer within each revision trajectory and then aggregating"
"these selected answers across trajectories. To select the best answer within each trajectory, we perform"
"best-of-N weighted aggregation and then choose the highest scoring solution with the maximum best-of-N"
"weighted answer. Then, to select the final answer across all revision chains, we perform another round"
"of best-of-N weighted selection using the best answer from each revision chain. The answer after this"
"second round of best-of-N weighted represents our final answer prediction."
"For majority voting we found hierarchical aggregation to create problems when the length of the trajectory"
"or the number of trajectories was too small. The problem being that without enough samples, majority"
"voting is unable to effectively select the best option. Therefore, for majority voting, we simply take all"
"answers, across all trajectories, at once and take their majority as the final-answer. We found this to"
"produce much smoother scaling behavior than the hierarchical approach."
"J. Revision Model Verifier Training"
"We found that the PRM we finetuned on the PaLM 2-S* base model outputs was not as effective when"
"applied to the PaLM 2-S* revision model’s outputs (see Figure 15(a)),
likely due to distribution shift with"
"the revision model. We therefore, trained a separate ORM verifier to use with our PaLM 2-S* revision"
"model. We could have trained a PRM as well, but opted for an ORM due to the high cost of generating"
"per-step PRM labels."
"We modified the standard ORM slightly for the revision setting, by finetuning the ORM with previous"
"revision in context, such that the verifier has access to the same context as the revision model, allowing"
"26"
