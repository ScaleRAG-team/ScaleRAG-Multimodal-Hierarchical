"2024-8-7"
"Scaling LLM Test-Time Compute Optimally can"
"be More Effective than Scaling Model Parameters"
"♦, 1
2
♣, 2
♣, 2"
"Charlie Snell
, Jaehoon Lee
, Kelvin Xu
and Aviral Kumar"
"♣Equal advising, 1UC Berkeley, 2Google DeepMind, ♦Work done during an internship at Google DeepMind"
"Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards"
"building generally self-improving agents that can operate on open-ended natural language. In this paper,"
"if an
we study the scaling of inference-time computation in LLMs, with a focus on answering the question:"
"LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its"
"performance on a challenging prompt? Answering this question has implications not only on the achievable"
"performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time"
"and pre-training compute. Despite its importance,
little research attempted to understand the scaling"
"behaviors of various test-time inference methods. Moreover, current work largely provides negative results"
"for a number of these strategies.
In this work, we analyze two primary mechanisms to scale test-time"
"computation: (1) searching against dense, process-based verifier reward models; and (2) updating the"
"model’s distribution over a response adaptively, given the prompt at test time. We find that in both cases, the"
"effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty"
"of the prompt. This observation motivates applying a “compute-optimal” scaling strategy, which acts to"
"most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we"
"can improve the efficiency of test-time compute scaling by more than 4× compared to a best-of-N baseline."
"Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains"
"somewhat non-trivial success rates, test-time compute can be used to outperform a 14× larger model."
"1.
Introduction"
"Humans tend to think for longer on difficult problems to reliably improve their decisions [9, 17, 18]."
"Can we instill a similar capability into today’s large language models (LLMs)? More specifically, given"
"a challenging input query,"
"can we enable language models
to most effectively make use of additional"
"computation at test time so as to improve the accuracy of their response? In theory, by applying additional"
"computation at test time, an LLM should be able to do better than what it was trained to do.
In addition,"
"such a capability at test-time also has the potential to unlock new avenues in agentic and reasoning"
"tasks [28, 34, 47]. For instance, if pre-trained model size can be traded off for additional computation"
"during inference, this would enable LLM deployment in use-cases where smaller on-device models could"
"be used in place of datacenter scale LLMs. Automating the generation of improved model outputs by"
"using additional inference-time computation also provides a path towards a general self-improvement"
"algorithm that can function with reduced human supervision."
"Prior work studying inference-time computation provides mixed results. On the one hand, some works"
"show that current LLMs can use test-time computation to improve their outputs [4, 8, 23, 30, 48], on"
"the other hand, other work shows that the effectiveness of these methods on more complex tasks such"
"as math reasoning remains highly limited [15, 37, 43], even though reasoning problems often require"
"drawing inferences about existing knowledge as opposed to new knowledge. These sorts of conflicting"
"findings motivate the need for a systematic analysis of different approaches for scaling test-time compute."
"Corresponding author(s): csnell22@berkeley.edu"
