"incorrect answers [28] (e.g.
improving the proposal distribution; Section 6) or verify the correctness of"
"individual steps in an answer using a process-based reward model (PRM) [22, 45] (Section 5). With"
"both approaches, we find that the efficacy of a particular test-time compute strategy depends critically"
"on both the nature of
the specific problem at hand and the base LLM used. For example, on easier"
"problems, for which the base LLM can already readily produce reasonable responses, allowing the model"
"to iteratively refine its initial answer by predicting a sequence of N revisions (i.e., modifying the proposal"
"distribution), may be a more effective use of test-time compute than sampling N independent responses in"
"parallel. On the other hand, with more difficult problems that may require searching over many different"
"high-level approaches to solving the problem, re-sampling new responses independently in parallel or"
"deploying tree-search against a process-based reward model is likely a more effective way to use test-time"
"computation. This finding illustrates the need to deploy an adaptive “compute-optimal“ strategy for"
"scaling test-time compute, wherein the specific approach for utilizing test-time compute is selected"
"depending on the prompt, so as to make the best use of additional computation. We also show that a"
"notion of question difficulty (Section 4) from the perspective of the base LLM can be used to predict the"
"efficacy of test-time computation, enabling us to practically instantiate this ‘compute-optimal’ strategy"
"given a prompt. By appropriately allocating test-time compute in this way, we are able to greatly improve"
"test-time compute scaling, surpassing the performance of a best-of-N baseline while only using about 4x"
"less computation with both revisions and search (Sections 5 and 6)."
"Using our improved test-time compute scaling strategy, we then aim to understand to what extent"
"test-time computation can effectively substitute for additional pretraining. We conduct a FLOPs-matched"
"comparison between a smaller model with additional test-time compute and pretraining a 14x larger"
"model. We find that on easy and intermediate questions, and even hard questions (depending on the"
"specific conditions on the pretraining and inference workload), additional test-time compute is often"
"preferable to scaling pretraining.
This finding suggests that rather than focusing purely on scaling"
"pretraining, in some settings it is be more effective to pretrain smaller models with less compute,"
"and then apply test-time compute to improve model outputs. That said, with the most challenging"
"questions, we observe very little benefits from scaling up test-time compute.
Instead, we find that"
"on these questions,
it is more effective to make progress by applying additional pretraining compute,"
"demonstrating that current approaches to scaling test-time compute may not be 1-to-1 exchangeable"
"with scaling pretraining. Overall, this suggests that even with a fairly naïve methodology, scaling up"
"test-time computation can already serve to be more preferable to scaling up pretraining, with only more"
"improvements to be attained as test-time strategies mature. Longer term, this hints at a future where"
"fewer FLOPs are spent during pretraining and more FLOPs are spent at inference."
"2. A Unified Perspective on Test-Time Computation: Proposer and Verifier"
"We first unify approaches for using test-time computation and then analyze some representative methods."
"First, we view the use of additional test-time compute through the lens of modifying the model’s predicted"
""
"distribution adaptively at test-time, conditioned on a given prompt."
"modify the distribution so as to generate better outputs than naïvely sampling from the LLM itself would."
"In general, there are two knobs to induce modifications to an LLM’s distribution: (1) at the input level:"
"since these capabilities are absent even in strong proprietary LLMs [15, 33]. However, we expect that future LLMs will be more"
"effective at verification and revision due to both increased scale and the inclusion of additional data targeted specifically towards"
"these capabilities [5, 24, 36]. Therefore in order to make progress towards understanding scaling of test-time computation,"
"we must use models finetuned for these capabilities. That said, we expect future models to be pretrained for such capabilities"
"directly, therefore avoiding the need for capability-specific finetuning."
