"are only given access to test prompts that we don’t know the answer to.
In order to be feasible in practice,"
"a compute-optimal scaling strategy conditioned on difficulty needs to first assess difficulty and then"
"utilize the right scaling strategy to solve this problem. Therefore, we approximate the problem’s difficulty"
"via a model-predicted notion of difficulty, which performs the same binning procedure over the the"
"averaged final answer score from a learned verifier (and not groundtruth answer correctness checks) on"
"the same set of 2048 samples per problem. We refer to this setting as model-predicted difficulty and"
"the setting which relies on the ground-truth correctness as oracle difficulty."
"While model-predicted difficulty removes the need for need knowing the ground truth label, estimating"
"difficulty in this way still incurs additional computation cost during inference. That said, this one-time"
"inference cost can be subsumed within the cost for actually running an inference-time strategy (e.g.,"
"when using a verifier, one could use the same inference computation for also running search). More"
"generally, this is akin to exploration-exploitation tradeoff in reinforcement learning: in actual deployment"
"conditions, we must balance the compute spent in assessing difficulty vs applying the most compute-"
"optimal approach. This is a crucial avenue for future work (see Section 8) and our experiments do not"
"account for this cost largely for simplicity, since our goal is to present some of the first results of what is"
"in fact possible by effectively allocating test-time compute."
"So as to avoid confounders with using the same test set for computing difficulty bins and for selecting"
"the compute-optimal strategy, we use two-fold cross validation on each difficulty bin in the test set. We"
"select the best-performing strategy according to performance on one fold and then measure performance"
"using that strategy on the other fold and vice versa, averaging the results of the two test folds."
"4. Experimental Setup"
"We first outline our experimental setup for conducting this analysis with multiple verifier design choices"
"and proposal distributions, followed by the analysis results in the subsequent sections."
""
"Datasets. We expect"
"“knowledge” needed to answer a question, and instead the primary challenge is about drawing (complex)"
"inferences from this knowledge. To this end, we focus on the MATH [13] benchmark, which consists of"
"high-school competition level math problems with a range of difficulty levels. For all experiments, we use"
"the dataset split consisting of 12k train and 500 test questions, used in Lightman et al. [22]."
"Models. We conduct our analysis using the PaLM 2-S* [3] (Codey) base model. We believe this model"
"is representative of the capabilities of many contemporary LLMs, and therefore think that our findings"
"likely transfer to similar models. Most importantly, this model attains a non-trivial performance on MATH"
"and yet has not saturated, so we expect this model to provide a good test-bed for us."
"5. Scaling Test-Time Compute via Verifiers"
"In this section we analyze how test-time compute can be scaled by optimizing a verifier, as effectively as"
"possible. To this end, we study different approaches for performing test-time search with process verifiers"
"(PRMs) and analyze the test-time compute scaling properties of these different approaches."
"5.1. Training Verifiers Amenable to Search"
"PRM training. Originally PRM training [22, 42] used human crowd-worker labels. While Lightman"
"et al. [22] released their PRM training data (i.e., the PRM800k dataset), we found this data to be largely"
