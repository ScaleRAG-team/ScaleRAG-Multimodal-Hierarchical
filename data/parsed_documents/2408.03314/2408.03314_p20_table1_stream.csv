"[43] K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by"
"self-critiquing their own plans?, 2023."
"[44] P. Villalobos and D. Atkinson. Trading off compute in training and inference, 2023. URL https:"
"//epochai.org/blog/trading-off-compute-in-training-and-inference. Accessed:"
"2024-07-03."
"[45] P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify"
"and reinforce llms step-by-step without human annotations, 2023."
"[46] R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, and N. D. Goodman. Hypothesis search: Inductive"
"reasoning with language models, 2024. URL https://arxiv.org/abs/2309.05660."
"[47]
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-"
"thought prompting elicits reasoning in large language models, 2023."
"[48] S. Yao, D. Yu, J. Zhao,
I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:"
"Deliberate problem solving with large language models, 2023."
"[49] Z. Yuan, H. Yuan, C. Li, G. Dong, K. Lu, C. Tan, C. Zhou, and J. Zhou. Scaling relationship on"
"learning mathematical reasoning with large language models, 2023."
"[50] E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning,"
"2022."
"[51] E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman. Quiet-star: Language"
"models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/"
"2403.09629."
