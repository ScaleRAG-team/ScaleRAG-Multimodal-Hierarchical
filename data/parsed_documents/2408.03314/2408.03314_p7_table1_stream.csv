"ineffective for us. We found that it was easy to exploit a PRM trained on this dataset via even naïve",""
"strategies such as best-of-N sampling. We hypothesize that this is likely a result of the distribution shift",""
"between the GPT-4 generated samples in their dataset and our PaLM 2 models. Rather than proceeding",""
"with the expensive process of collecting crowd-worker PRM labels for our PaLM 2 models, we instead",""
"apply the approach of Wang et al. [45] to supervise PRMs without human labels, using estimates of",""
"per-step correctness obtained from running Monte Carlo rollouts from each step in the solution. Our",""
"PRM’s per-step predictions therefore correspond to value estimates of reward-to-go for the base model’s",""
"","sampling policy, similar to recent work [31, 45]. We also compared to an ORM baseline (Appendix F)"
"but found that our PRM consistently outperforms the ORM. Hence, all of the search experiments in this",""
"section use a PRM model. Additional details on PRM training are shown in Appendix D.",""
"Answer aggregation. At test time, process-based verifiers can be used to score each individual step in a",""
"set of solutions sampled from the base model.","In order to select the best-of-N answers with the PRM, we"
"need a function that can aggregate across all the per-step scores for each answer to determine the best",""
"candidate for the correct answer. To do this, we first aggregate each individual answer’s per-step scores",""
"to obtain a final score for the full answer (step-wise aggregation). We then aggregate across answers to",""
"determine the best answer (inter-answer aggregation). Concretely, we handle step-wise and inter-answer",""
"aggregation as follows:",""
