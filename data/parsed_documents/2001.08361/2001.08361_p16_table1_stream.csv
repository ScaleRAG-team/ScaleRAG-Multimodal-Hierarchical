"103"
"0"
"10 7
10 5
10 7
10 5
10 3
10 1
10 3
10 1"
"Compute (PF-days), non-embedding
Compute (PF-days), excluding embeddings"
"Figure 14
Left: Each value of the compute budget Cmin has an associated optimal model size N . Optimal"
"increasing by 5x for each 10x increase in compute. The number
model size grows very rapidly with Cmin,"
"of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x."
"Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most"
"of the growth in data examples processed can be used for increased batch sizes."
"can be ﬁt very well with a power-law"
"(6.1)
N (Cmin) ∝ (Cmin)0.73."
"In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4)."
"In particular, since
By deﬁnition Cmin ≡ 6N BcritS, and so we can use N (Cmin) to extract further results."
"prior ﬁts show B ∝ L−4.8 and L ∝ C −0.05
, we can conclude that Bcrit ∝ C 0.24
min . This leads us to conclude"
"min"
"that the optimal number of steps will only grow very slowly with compute, as"
"(6.2)
Smin ∝ (Cmin)0.03,"
"matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results"
"may even be consistent with an exponent of zero."
"Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we"
"should predominantly increase the model size N , while simultaneously scaling up the batch size via B ∝"
"training uses relatively
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient"
"few optimization steps, additional work on speeding up early training dynamics may be warranted."
"6.2
Predictions from L(N, Smin)"
"The results
for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in"
"Section 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin
6N B and then ﬁnd the minimum"
"of the loss as a function of N , while ﬁxing the training compute. We carry out
this procedure in detail
in"
"Appendix B, where we also provide some additional predictions."
"For the loss as a function of training compute, we predict that"
