"393.2K
2.8"
"106"
"2.5"
"2.4"
"105
104
107
108
109
1010"
"Tokens in Dataset
Estimated Smin"
"Figure 4
Left: The early-stopped test
loss L(N, D) varies predictably with the dataset size D and model"
"size N according to Equation (1.5). Right: After an initial
transient period,
learning curves for all model"
"the number of steps when
sizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin,"
"training at large batch size (details in Section 5.1)."
"These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N , and over two"
"orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters"
"(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2"
"specify the degree of performance improvement
training set
[RWC+19].
The power
laws αN, αD, αmin"
"expected as we scale up N , D, or Cmin; for example, doubling the number of parameters yields a loss that"
"is smaller by a factor 2−αN = 0.95.
, and Dc depend on the
The precise numerical values of Nc, C min"
"vocabulary size and tokenization and hence do not have a fundamental meaning."
"The critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also"
"roughly obeys a power law in L:"
"B∗"
",
(1.4)
Bcrit (L) =
B∗ ∼ 2 · 108 tokens,
αB ∼ 0.21"
"L1/αB"
"Equation (1.1) and (1.2) together suggest
that as we increase the model size, we should increase the dataset"
"αN"
"αD ∼ N 0.74.
In fact, we ﬁnd that there is a single equation combining
size sublinearly according to D ∝ N"
"(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:"
"(cid:35)αD"
""
"(cid:19) αN
(cid:34)(cid:18) Nc
Dc"
"L(N, D) =
+
(1.5)"
"N
D"
"with ﬁts pictured on the left
in ﬁgure 4. We conjecture that
this functional form may also parameterize the"
"trained log-likelihood for other generative modeling tasks."
"When training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after"
"an initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)"
""
"(cid:19)αN
(cid:18)
(cid:19)αS
(cid:18) Nc
Sc"
"L(N, S) =
+
(1.6)"
"N
Smin(S)"
"where Sc ≈ 2.1 × 103 and αS ≈ 0.76, and Smin(S) is the minimum possible number of optimization steps"
"(parameter updates) estimated using Equation (5.4)."
"When training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the"
"prediction that the optimal model size N , optimal batch size B, optimal number of steps S, and dataset size"
"D should grow as"
"N ∝ C αmin
/αN ,
B ∝ C αmin
/αB ,
S ∝ C αmin
/αS ,
D = B · S
(1.7)"
"with"
"αmin
(1.8)
= 1/ (1/αS + 1/αB + 1/αN )
C"
"which closely matches the empirically optimal
results N ∝ C 0.73
min , B ∝ C 0.24
min , and S ∝ C 0.03
min . As the"
"computational budget C increases, it should be spent primarily on larger models, without dramatic increases"
"in training time or dataset size (see Figure 3). This also implies that as models grow larger,
they become"
"increasingly sample efﬁcient.
In practice, researchers typically train smaller models for longer than would"
