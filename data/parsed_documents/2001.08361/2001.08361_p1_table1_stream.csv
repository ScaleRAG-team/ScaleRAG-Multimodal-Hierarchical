"Abstract"
"We study empirical scaling laws for language model performance on the cross-entropy loss."
"The loss scales as a power-law with model size, dataset size, and the amount of compute"
"used for training, with some trends spanning more than seven orders of magnitude. Other"
"architectural details such as network width or depth have minimal effects within a wide"
"range. Simple equations govern the dependence of overﬁtting on model/dataset size and the"
"dependence of training speed on model size. These relationships allow us to determine the"
"optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-"
"efﬁcient, such that optimally compute-efﬁcient training involves training very large models"
"on a relatively modest amount of data and stopping signiﬁcantly before convergence."
