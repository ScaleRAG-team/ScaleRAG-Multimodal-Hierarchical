"Notation","",""
"We use the following notation:","",""
"","• L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in",""
"some cases we report the loss for speciﬁc tokens within the context.","",""
"• N – the number of model parameters, excluding all vocabulary and positional embeddings","",""
"","• C ≈ 6N BS – an estimate of the total non-embedding training compute, where B is the batch size,",""
"","and S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,",""
"where one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.","",""
"• D – the dataset size in tokens","",""
"• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1.","Training at","the"
"","critical batch size provides a roughly optimal compromise between time and compute efﬁciency.",""
"","• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of",""
"","the loss. This is the training compute that would be used if the model were trained at a batch size",""
"much less than the critical batch size.","",""
"","• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.",""
"","This is also the number of training steps that would be used if the model were trained at a batch size",""
"much greater than the critical batch size.","",""
"","• αX – power-law exponents for the scaling of the loss as L(X) ∝ 1/X αX where X can be any of",""
"N, D, C, S, B, C min.","",""
