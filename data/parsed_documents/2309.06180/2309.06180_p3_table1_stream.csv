"2.1
Transformer-Based Large Language Models","","The prompt phase takes the whole user prompt (ğ‘¥1, . . . , ğ‘¥ğ‘›)"
"","","as input and computes the probability of the first new to-"
"The task of language modeling is to model the probability","",""
"","","|
ken ğ‘ƒ (ğ‘¥ğ‘›+1
ğ‘¥1, . . . , ğ‘¥ğ‘›). During this process, also gener-"
"of a list of tokens (ğ‘¥1, . . . , ğ‘¥ğ‘›). Since language has a natural","",""
"","","ates the key vectors ğ‘˜1, . . . , ğ‘˜ğ‘› and value vectors ğ‘£1, . . . , ğ‘£ğ‘›."
"sequential ordering, it is common to factorize the joint prob-","",""
"","","Since prompt tokens ğ‘¥1, . . . , ğ‘¥ğ‘› are all known, the computa-"
"ability over the whole sequence as the product of conditional","",""
"","","tion of the prompt phase can be parallelized using matrix-"
"probabilities (a.k.a. autoregressive decomposition [3]):","",""
"","","matrix multiplication operations. Therefore, this phase can"
"","","efficiently use the parallelism inherent in GPUs."
"(1)
ğ‘ƒ (ğ‘¥) = ğ‘ƒ (ğ‘¥1) Â· ğ‘ƒ (ğ‘¥2
| ğ‘¥1) Â· Â· Â· ğ‘ƒ (ğ‘¥ğ‘› | ğ‘¥1, . . . , ğ‘¥ğ‘›âˆ’1).","",""
"","","The autoregressive generation phase generates the re-"
"Transformers [53] have become the de facto standard ar-","","maining new tokens sequentially. At iteration ğ‘¡, the model"
"chitecture for modeling the probability above at a large scale.","","takes one token ğ‘¥ğ‘›+ğ‘¡ as input and computes the probability"
"The most important component of a Transformer-based lan-","","ğ‘ƒ (ğ‘¥ğ‘›+ğ‘¡ +1
| ğ‘¥1, . . . , ğ‘¥ğ‘›+ğ‘¡ ) with the key vectors ğ‘˜1, . . . , ğ‘˜ğ‘›+ğ‘¡ and"
"guage model is its self-attention layers. For an input hidden","","value vectors ğ‘£1, . . . , ğ‘£ğ‘›+ğ‘¡ . Note that the key and value vectors"
"âˆˆ Rğ‘›Ã—ğ‘‘ , a self-attention layer","","at positions 1 to ğ‘› + ğ‘¡ âˆ’ 1 are cached at previous iterations,"
"state sequence (ğ‘¥1, . . . , ğ‘¥ğ‘›)","",""
"first applies linear transformations on each position ğ‘–
to get","","only the new key and value vector ğ‘˜ğ‘›+ğ‘¡ and ğ‘£ğ‘›+ğ‘¡ are com-"
"the query, key, and value vectors:","","puted at this iteration. This phase completes either when the"
"","","sequence reaches a maximum length (specified by users or"
"ğ‘ğ‘– = ğ‘Šğ‘ğ‘¥ğ‘–, ğ‘˜ğ‘– = ğ‘Šğ‘˜ğ‘¥ğ‘–,
ğ‘£ğ‘– = ğ‘Šğ‘£ğ‘¥ğ‘– .
(2)","","limited by LLMs) or when an end-of-sequence (<eos>) token"
"","","is emitted. The computation at different iterations cannot"
"Then, the self-attention layer computes the attention score","","be parallelized due to the data dependency and often uses"
"ğ‘ğ‘– ğ‘— by multiplying the query vector at one position with all","","matrix-vector multiplication, which is less efficient. As a re-"
"the key vectors before it and compute the output ğ‘œğ‘– as the","","sult, this phase severely underutilizes GPU computation and"
"weighted average over the value vectors:","","becomes memory-bound, being responsible for most portion"
"","","of the latency of a single request."
"âˆš","",""
"ğ‘‘)
exp(ğ‘âŠ¤
ğ‘– ğ‘˜ ğ‘— /","","2.3
Batching Techniques for LLMs"
"ğ‘–âˆ‘ï¸ ğ‘—
ğ‘ğ‘– ğ‘—ğ‘£ ğ‘— .
, ğ‘œğ‘– =
ğ‘ğ‘– ğ‘— =
âˆš
(3)","",""
"(cid:205)ğ‘–","",""
"ğ‘‘)","",""
"=1
ğ‘¡ =1 exp(ğ‘âŠ¤","","The compute utilization in serving LLMs can be improved"
"","","by batching multiple requests. Because the requests share"
"Besides the computation in Eq. 4, all other components","","the same model weights, the overhead of moving weights is"
"in the Transformer model, including the embedding layer,","","amortized across the requests in a batch, and can be over-"
"feed-forward layer, layer normalization [2], residual connec-","","whelmed by the computational overhead when the batch"
"tion [22], output logit computation, and the query, key, and","","size is sufficiently large. However, batching the requests"
"value transformation in Eq. 2, are all applied independently","","to an LLM service is non-trivial for two reasons. First, the"
"position-wise in a form of ğ‘¦ğ‘– = ğ‘“ (ğ‘¥ğ‘– ).","","requests may arrive at different times. A naive batching strat-"
"","","egy would either make earlier requests wait for later ones"
"2.2
LLM Service & Autoregressive Generation","",""
"","","or delay the incoming requests until earlier ones finish, lead-"
"Once trained, LLMs are often deployed as a conditional gen-","","ing to significant queueing delays. Second, the requests may"
"eration service (e.g., completion API [34] or chatbot [19, 35]).","","have vastly different input and output lengths (Fig. 11). A"
"A request to an LLM service provides a list of input prompt","","straightforward batching technique would pad the inputs"
"tokens (ğ‘¥1, . . . , ğ‘¥ğ‘›), and the LLM service generates a list of","","and outputs of the requests to equalize their lengths, wasting"
"output tokens (ğ‘¥ğ‘›+1, . . . , ğ‘¥ğ‘›+ğ‘‡ ) according to Eq. 1. We refer to","","GPU computation and memory."
"the concatenation of the prompt and output lists as sequence.","","To address this problem, fine-grained batching mecha-"
"Due to the decomposition in Eq. 1, the LLM can only sam-","","nisms, such as cellular batching [16] and iteration-level sched-"
"ple and generate new tokens one by one, and the generation","","uling [60], have been proposed. Unlike traditional methods"
"process of each new token depends on all the previous tokens","","that work at the request level, these techniques operate at"
"in that sequence, specifically their key and value vectors. In","","the iteration level. After each iteration, completed requests"
"this sequential generation process, the key and value vectors","","are removed from the batch, and new ones are added. There-"
"of existing tokens are often cached for generating future","","fore, a new request can be processed after waiting for a"
"tokens, known as KV cache. Note that the KV cache of one","","single iteration, not waiting for the entire batch to complete."
"token depends on all its previous tokens. This means that the","","Moreover, with special GPU kernels, these techniques elim-"
"KV cache of the same token appearing at different positions","","inate the need to pad the inputs and outputs. By reducing"
"in a sequence will be different.","","the queueing delay and the inefficiencies from padding, the"
"Given a request prompt, the generation computation in","","fine-grained batching mechanisms significantly increase the"
"the LLM service can be decomposed into two phases:","","throughput of LLM serving."
"","3",""
