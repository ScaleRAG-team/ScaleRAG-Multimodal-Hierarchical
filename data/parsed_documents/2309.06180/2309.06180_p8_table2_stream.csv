"Sequence A
Sequence B","",""
"Prompt
Prompt","","context: (1) Which blocks should it evict? (2) How to recover"
"Translate English to French:
Translate English to French:","","evicted blocks if needed again? Typically, eviction policies"
"“sea otter” => “loutre de mer”
“sea otter” => “loutre de mer”","",""
"Shared prefix","",""
"“peppermint” => “menthe poivrée”
“peppermint” => “menthe poivrée”","","use heuristics to predict which block will be accessed fur-"
"“plush girafe” => “girafe en peluche”
“plush girafe” => “girafe en peluche”","",""
"Task input
“cheese” =>
“I love you” =>","","thest in the future and evict that block. Since in our case we"
"Sequence A
Sequence B","","know that all blocks of a sequence are accessed together, we"
"LLM output
LLM output","",""
"Task output
“fromage”
“Je t’amie”","","implement an all-or-nothing eviction policy, i.e., either evict"
"","","all or none of the blocks of a sequence. Furthermore, multi-"
"Figure 10. Shared prompt example for machine translation.","","ple sequences within one request (e.g., beam candidates in"
"The examples are adopted from [5].","","one beam search request) are gang-scheduled as a sequence"
"","","group. The sequences within one sequence group are always"
"on the full prompt. Fig. 10 shows an example. Moreover, the","",""
"","","preempted or rescheduled together due to potential memory"
"shared prefix can be further tuned, via prompt engineering,","",""
"","","sharing across those sequences. To answer the second ques-"
"to improve the accuracy of the downstream tasks [26, 27].","",""
"","","tion of how to recover an evicted block, we consider two"
"For this type of application, many user prompts share a","",""
"","","techniques:"
"prefix, thus the LLM service provider can store the KV cache","",""
"","","Swapping. This is the classic technique used by most virtual"
"of the prefix in advance to reduce the redundant computa-","",""
"","","memory implementations which copy the evicted pages to a"
"tion spent on the prefix. In vLLM, this can be conveniently","",""
"","","swap space on the disk. In our case, we copy evicted blocks to"
"achieved by reserving a set of physical blocks for a set of","",""
"","","the CPU memory. As shown in Fig. 4, besides the GPU block"
"predefined shared prefixes by the LLM service provider, as","",""
"","","allocator, vLLM includes a CPU block allocator to manage"
"how OS handles shared library across processes. A user in-","",""
"","","the physical blocks swapped to CPU RAM. When vLLM"
"put prompt with the shared prefix can simply map its logi-","",""
"","","exhausts free physical blocks for new tokens, it selects a set"
"cal blocks to the cached physical blocks (with the last block","",""
"","","of sequences to evict and transfer their KV cache to the CPU."
"marked copy-on-write). The prompt phase computation only","",""
"","","Once it preempts a sequence and evicts its blocks, vLLM"
"needs to execute on the user’s task input.","",""
"","","stops accepting new requests until all preempted sequences"
"Mixed decoding methods. The decoding methods dis-","",""
"","","are completed. Once a request completes, its blocks are freed"
"cussed earlier exhibit diverse memory sharing and access-","",""
"","","from memory, and the blocks of a preempted sequence are"
"ing patterns. Nonetheless, vLLM facilitates the simultane-","",""
"","","brought back in to continue the processing of that sequence."
"ous processing of requests with different decoding prefer-","",""
"","","Note that with this design, the number of blocks swapped to"
"ences, which existing systems cannot efficiently do. This is","",""
"","","the CPU RAM never exceeds the number of total physical"
"because vLLM conceals the complex memory sharing be-","",""
"","","blocks in the GPU RAM, so the swap space on the CPU RAM"
"tween different sequences via a common mapping layer that","",""
"","","is bounded by the GPU memory allocated for the KV cache."
"translates logical blocks to physical blocks. The LLM and","",""
"","","Recomputation. In this case, we simply recompute the KV"
"its execution kernel only see a list of physical block IDs","",""
"","","cache when the preempted sequences are rescheduled. Note"
"for each sequence and do not need to handle sharing pat-","",""
"","","that recomputation latency can be significantly lower than"
"terns across sequences. Compared to existing systems, this","",""
"","","the original
latency, as the tokens generated at decoding"
"approach broadens the batching opportunities for requests","",""
"","","can be concatenated with the original user prompt as a new"
"with different sampling requirements, ultimately increasing","",""
"","","prompt—their KV cache at all positions can be generated in"
"the system’s overall throughput.","",""
"","","one prompt phase iteration."
"4.5
Scheduling and Preemption","","The performances of swapping and recomputation depend"
"","","on the bandwidth between CPU RAM and GPU memory and"
"When the request
traffic surpasses the system’s capacity,","",""
"","","the computation power of the GPU. We examine the speeds"
"vLLM must prioritize a subset of requests. In vLLM, we adopt","",""
"","","of swapping and recomputation in §7.3."
"the first-come-first-serve (FCFS) scheduling policy for all","",""
"requests, ensuring fairness and preventing starvation. When","",""
"","","4.6
Distributed Execution"
"vLLM needs to preempt requests, it ensures that the earliest","",""
"arrived requests are served first and the latest requests are","","Many LLMs have parameter sizes exceeding the capacity of a"
"preempted first.","","single GPU [5, 9]. Therefore, it is necessary to partition them"
"LLM services face a unique challenge: the input prompts","","across distributed GPUs and execute them in a model parallel"
"for an LLM can vary significantly in length, and the resulting","","fashion [28, 63]. This calls for a memory manager capable of"
"output lengths are not known a priori, contingent on both","","handling distributed memory. vLLM is effective in distributed"
"the input prompt and the model. As the number of requests","","settings by supporting the widely used Megatron-LM style"
"and their outputs grow, vLLM can run out of the GPU’s phys-","","tensor model parallelism strategy on Transformers [47]. This"
"ical blocks to store the newly generated KV cache. There","","strategy adheres to an SPMD (Single Program Multiple Data)"
"are two classic questions that vLLM needs to answer in this","","execution schedule, wherein the linear layers are partitioned"
"","8",""
