"","and external fragmentation – exist that prevent other requests from fitting into the memory. The token in each memory slot",""
"","represents its KV cache. Note the same tokens can have different KV cache when at different positions.",""
"3
Memory Challenges in LLM Serving","","§6.3) of their KV cache, and the sharing pattern evolves as"
"","","the decoding process advances."
"Although fine-grained batching reduces the waste of com-","",""
"","","Scheduling for unknown input & output lengths. The"
"puting and enables requests to be batched in a more flexible","",""
"","","requests to an LLM service exhibit variability in their input"
"way, the number of requests that can be batched together is","",""
"","","and output lengths. This requires the memory management"
"still constrained by GPU memory capacity, particularly the","",""
"","","system to accommodate a wide range of prompt lengths. In"
"space allocated to store the KV cache. In other words, the","",""
"","","addition, as the output length of a request grows at decoding,"
"serving system’s throughput is memory-bound. Overcom-","",""
"","","the memory required for its KV cache also expands and may"
"ing this memory-bound requires addressing the following","",""
"","","exhaust available memory for incoming requests or ongoing"
"challenges in the memory management:","",""
"","","generation for existing prompts. The system needs to make"
"Large KV cache. The KV Cache size grows quickly with the","","scheduling decisions, such as deleting or swapping out the"
"number of requests. As an example, for the 13B parameter","","KV cache of some requests from GPU memory."
"OPT model [62], the KV cache of a single token demands 800","",""
"KB of space, calculated as 2 (key and value vectors) × 5120","",""
"","","3.1
Memory Management in Existing Systems"
"(hidden state size) × 40 (number of layers) × 2 (bytes per","",""
"FP16). Since OPT can generate sequences up to 2048 tokens,","",""
"","","Since most operators in current deep learning frameworks"
"the memory required to store the KV cache of one request","",""
"","","[33, 39] require tensors to be stored in contiguous memory,"
"can be as much as 1.6 GB. Concurrent GPUs have memory","",""
"","","previous LLM serving systems [31, 60] also store the KV"
"capacities in the tens of GBs. Even if all available memory","",""
"","","cache of one request as a contiguous tensor across the differ-"
"was allocated to KV cache, only a few tens of requests could","",""
"","","ent positions. Due to the unpredictable output lengths from"
"be accommodated. Moreover, inefficient memory manage-","",""
"","","the LLM, they statically allocate a chunk of memory for a"
"ment can further decrease the batch size, as shown in Fig. 2.","",""
"","","request based on the request’s maximum possible sequence"
"Additionally, given the current trends, the GPU’s computa-","",""
"","","length,
irrespective of the actual
input or eventual output"
"tion speed grows faster than the memory capacity [17]. For","",""
"","","length of the request."
"example, from NVIDIA A100 to H100, The FLOPS increases","",""
"","","Fig. 3 illustrates two requests: request A with 2048 max-"
"by more than 2x, but the GPU memory stays at 80GB max-","",""
"","","imum possible sequence length and request B with a max-"
"imum. Therefore, we believe the memory will become an","",""
"","","imum of 512. The chunk pre-allocation scheme in existing"
"increasingly significant bottleneck.","",""
"","","systems has three primary sources of memory wastes: re-"
"Complex decoding algorithms. LLM services offer a range","","served slots for future tokens, internal fragmentation due to"
"of decoding algorithms for users to select from, each with","","over-provisioning for potential maximum sequence lengths,"
"varying implications for memory management complexity.","","and external fragmentation from the memory allocator like"
"For example, when users request multiple random samples","","the buddy allocator. The external fragmentation will never"
"from a single input prompt, a typical use case in program","","be used for generated tokens, which is known before serving"
"suggestion [18],
the KV cache of
the prompt part, which","","a request. Internal fragmentation also remains unused, but"
"accounts for 12% of the total KV cache memory in our ex-","","this is only realized after a request has finished sampling."
"periment (§6.3), can be shared to minimize memory usage.","","They are both pure memory waste. Although the reserved"
"On the other hand, the KV cache during the autoregressive","","memory is eventually used, reserving this space for the en-"
"generation phase should remain unshared due to the dif-","","tire request’s duration, especially when the reserved space"
"ferent sample results and their dependence on context and","","is large, occupies the space that could otherwise be used to"
"position. The extent of KV cache sharing depends on the","","process other requests. We visualize the average percentage"
"specific decoding algorithm employed. In more sophisticated","","of memory wastes in our experiments in Fig. 2, revealing"
"algorithms like beam search [49], different request beams","","that the actual effective memory in previous systems can be"
"can share larger portions (up to 55% memory saving, see","","as low as 20.4%."
"","4",""
