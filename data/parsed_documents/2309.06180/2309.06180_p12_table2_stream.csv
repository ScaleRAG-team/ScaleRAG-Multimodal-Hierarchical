"","","7.1
Kernel Microbenchmark"
"","Figure 17. Performance on chatbot workload.",""
"","","The dynamic block mapping in PagedAttention affects the"
"","","performance of the GPU operations involving the stored KV"
"","","cache, i.e., block read/writes and attention. Compared to the"
"","Fig. 10. For the model, we use LLaMA-13B [52], which is mul-",""
"","","existing systems, our GPU kernels (§5) involve extra over-"
"","tilingual. For the workload, we use the WMT16 [4] English-",""
"","","heads of accessing the block table, executing extra branches,"
"","to-German translation dataset and synthesize two prefixes",""
"","","and handling variable sequence lengths. As shown in Fig. 18a,"
"","that include an instruction and a few translation examples.",""
"","","this leads to 20–26% higher attention kernel
latency, com-"
"","The first prefix includes a single example (i.e., one-shot)",""
"","","pared to the highly-optimized FasterTransformer implemen-"
"while the other prefix includes 5 examples (i.e., few-shot). As","",""
"","","tation. We believe the overhead is small as it only affects"
"","shown in Fig. 16 (a), vLLM achieves 1.67× higher through-",""
"","","the attention operator but not
the other operators in the"
"","put than Orca (Oracle) when the one-shot prefix is shared.",""
"","","model, such as Linear. Despite the overhead, PagedAttention"
"","Furthermore, when more examples are shared (Fig. 16 (b)),",""
"","","makes vLLM significantly outperform FasterTransformer in"
"","vLLM achieves 3.58× higher throughput than Orca (Oracle).",""
"","","end-to-end performance (§6)."
"6.5","Chatbot",""
