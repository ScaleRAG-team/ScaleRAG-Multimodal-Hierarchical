"20","",""
"0.0
0","",""
"","",""
"1
2
4
8
16
32
64
128
256
1
2
4
8
16
32
64
128
256","","indirection in paging by fusing the GPU kernels for memory"
"Block size
Block size","",""
"","","access operations with those for other operations such as"
"(a) Microbenchmark
(b) End-to-end performance","",""
"","","attention."
"Figure 19. (a) Overhead of recomputation and swapping for","",""
"different block sizes. (b) Performance when serving OPT-13B","","9
Related Work"
"with the ShareGPT traces at the same request rate.","",""
"","","General model serving systems. Model serving has been"
"7.3
Comparing Recomputation and Swapping","","an active area of research in recent years, with numerous"
"","","systems proposed to tackle diverse aspects of deep learning"
"vLLM supports both recomputation and swapping as its re-","",""
"","","model deployment. Clipper [11], TensorFlow Serving [33],"
"covery mechanisms. To understand the tradeoffs between","",""
"","","Nexus [45],
InferLine [10], and Clockwork [20] are some"
"the two methods, we evaluate their end-to-end performance","",""
"","","earlier general model serving systems. They study batch-"
"and microbenchmark their overheads, as presented in Fig. 19.","",""
"","","ing, caching, placement, and scheduling for serving single"
"Our results reveal that swapping incurs excessive overhead","",""
"","","or multiple models. More recently, DVABatch [12] intro-"
"with small block sizes. This is because small block sizes often","",""
"","","duces multi-entry multi-exit batching. REEF [21] and Shep-"
"result in numerous small data transfers between CPU and","",""
"","","herd [61] propose preemption for serving. AlpaServe [28]"
"GPU, which limits the effective PCIe bandwidth. In contrast,","",""
"","","utilizes model parallelism for statistical multiplexing. How-"
"the overhead of recomputation remains constant across dif-","",""
"","","ever, these general systems fail to take into account the auto-"
"ferent block sizes, as recomputation does not utilize the KV","",""
"","","regressive property and token state of LLM inference, result-"
"blocks. Thus, recomputation is more efficient when the block","",""
"","","ing in missed opportunities for optimization."
"size is small, while swapping is more efficient when the block","",""
"","","Specialized serving systems for transformers. Due to"
"size is large, though recomputation overhead is never higher","",""
"","","the significance of the transformer architecture, numerous"
"than 20% of swapping’s latency. For medium block sizes from","",""
"","","specialized serving systems for it have been developed. These"
"16 to 64, the two methods exhibit comparable end-to-end","",""
"","","systems utilize GPU kernel optimizations [1, 29, 31, 56], ad-"
"performance.","",""
"","","vanced batching mechanisms [14, 60], model parallelism [1,"
"8
Discussion","",""
"","","41, 60], and parameter sharing [64]
for efficient serving."
"","","Among them, Orca [60] is most relevant to our approach."
"Applying the virtual memory and paging technique to","",""
"other GPU workloads. The idea of virtual memory and","","Comparison to Orca. The iteration-level scheduling in"
"paging is effective for managing the KV cache in LLM serving","","Orca [60] and PagedAttention in vLLM are complementary"
"because the workload requires dynamic memory allocation","","techniques: While both systems aim to increase the GPU"
"(since the output length is not known a priori) and its perfor-","","utilization and hence the throughput of LLM serving, Orca"
"mance is bound by the GPU memory capacity. However, this","","achieves it by scheduling and interleaving the requests so"
"does not generally hold for every GPU workload. For exam-","","that more requests can be processed in parallel, while vLLM"
"ple, in DNN training, the tensor shapes are typically static,","","is doing so by increasing memory utilization so that
the"
"and thus memory allocation can be optimized ahead of time.","","working sets of more requests fit into memory. By reducing"
"For another example,
in serving DNNs that are not LLMs,","","memory fragmentation and enabling sharing, vLLM runs"
"an increase in memory efficiency may not result in any per-","","more requests in a batch in parallel and achieves a 2-4×"
"formance improvement since the performance is primarily","","speedup compared to Orca. Indeed, the fine-grained sched-"
"compute-bound. In such scenarios, introducing the vLLM’s","","uling and interleaving of the requests like in Orca makes"
"techniques may rather degrade the performance due to the","","memory management more challenging, making the tech-"
"extra overhead of memory indirection and non-contiguous","","niques proposed in vLLM even more crucial."
"block memory. However, we would be excited to see vLLM’s","",""
"","","Memory optimizations. The widening gap between the"
"techniques being applied to other workloads with similar","",""
"","","compute capability and memory capacity of accelerators has"
"properties to LLM serving.","",""
"","","caused memory to become a bottleneck for both training"
"LLM-specific optimizations in applying virtual mem-","","and inference. Swapping [23, 42, 55], recomputation [7, 24]"
"ory and paging. vLLM re-interprets and augments the idea","","and their combination [40] have been utilized to reduce the"
"of virtual memory and paging by leveraging the application-","","peak memory of training. Notably, FlexGen [46] studies how"
"specific semantics. One example is vLLM’s all-or-nothing","","to swap weights and token states for LLM inference with"
"","13",""
